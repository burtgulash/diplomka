\documentclass[11pt,letterpaper,oneside,openright]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage[left=4cm,right=2.8cm,bottom=2.5cm,footskip=1.5cm]{geometry}
\usepackage{color}
\usepackage[scaled=0.83]{beramono}

\usepackage[sort,numbers]{natbib}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}

\usepackage{tabulary}
\usepackage{subcaption}

\usepackage{fancyvrb}
\usepackage{pygme}

\usepackage{float}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{caption}
\captionsetup[figure]{name=Obr.}
\captionsetup[table]{name=Tab.}

\pagestyle{plain}
\linespread{1.15}
\setlength{\tabcolsep}{1em}

\definecolor{Darkgreen}{rgb}{0,0.4,0}
\hypersetup{%
    pdfborder={0 0 0},
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue,
}

% \usepackage{sectsty}
% \sectionfont{\fontsize{21}{21}\selectfont}
% \subsectionfont{\fontsize{16}{16}\selectfont}
% \subsectionfont{\fontsize{14}{14}\selectfont}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{30pt}%
    {\parindent 0pt \raggedright \normalfont
        \interlinepenalty=-1
        \Huge \bfseries \thechapter\hspace{.75em}#1\par\nobreak
        \vskip 40pt
    }}
\makeatother

\newcommand{\bftt}[1]{\texttt{\textbf{#1}}}
\newcommand{\boldred}[1]{\textbf{\color{red} #1}}
\newcommand{\horizlina}%
{
    \mbox{}\vspace{1em}
    \hrule
    \mbox{}
}



\begin{document}
\frontmatter
{\hypersetup{hidelinks}
    \tableofcontents
}

\mainmatter
\chapter{Úvod}
Textové vyhledávání se stalo populární v 90. letech s nástupem Webu a s ním
masivního nárůstu informací, které jsou dostupné prakticky každému po pár
kliknutích myši. Toho bylo dosaženo vzestupem velkých webových vyhledávačů
(Excite, AltaVista, Yahoo, Seznam, Google), které byly schopné exponenciálně
rostoucí růst informací zkrotit~\cite{search_history}. Alespoň do jisté míry,
protože tyto vyhledávače umožňují přítup pouze k té části webu, ke které se lze
dostat následováním hypertextových odkazů. Většina informací není skrze pouhé
odkazy dostupná (deep web), protože je např. skrytá v interních databázích, k
nimž se lze dostat pouze interním vyhledávacím systémem.

Popularita webových vyhledávačů zapříčinila zájem o vyhledávací systémy i u
jednotlivců a firem. Stejné techniky začaly být dostupné v open source
systémech, z nichž nejznámějšími jsou Solr a ElasticSearch vycházející ze
stejné softwarové knihovny pro vyhledávací systémy - Apache Lucene. Někteří
tvrdí, že knihovna Lucene přinesla komoditizaci a demokratizaci na poli
specializovaných vyhledávačů~\cite{dion_almaer,javaworld}.

Vyhledávání by se tedy mohlo jevit jako vyřešený problém. V této práci bude ale
představeno několik méně známých technik, které pokud by se více
popularizovaly, mohly by změnit tenhle pohled a zapříčinit vznik nových
vyhledávacích systémů.

Pokud by tyto techniky byly natolik užívatelsky výhodné, pak by bylo podivné,
že by je dodnes nikdo neimplementoval. Některé výzkumné týmy a komerční řešení
se jimi přímo zabývají, ale v dnešní době platí v softwaru očekávání, že pokud
není takový systém distribuován zdarma s otevřeným kódem (open source), nezíská
si popularitu ani mezi nezávislými jedinci, ani mezi firmami. Snad pouze pokud
byly osobně osloveny obchodním oddělením dané firmy, a tedy tvoří menšinu.
Dalším důvodem je, že pokud se nějaký systém snaží být nejlepším pro všechny
možné případy, pak je šance, že nebude nejlepším pro žádný z nich. To otvírá
šanci pro nová řešení.

\section{Motivace}
Prozkoumávat oblast alternativních vyhledávacích technik jsem začal po
frustraci s vytvořením jednoduchého vyhledávače pro položky jako názvy produktů
eshopu nebo pro vyhledávání v databázi jmen. Vytvoření takového systému je dnes
považováno za vyřešený problém, protože existuje spousta open source databází a
jiných produktů, které se dají přizpůsobit téměř každé situaci.

\subsection{Fragmentace systémů}
Bez použití specializovaného vyhledávacího systému je nejčastějším řešením pro
vyhledávání v databázi použití technik, které poskytují databázové systémy.
Časté řešení je rozdělení textových polí na více částí a vyhledávání separátně
v každé z nich pomocí operátoru \bftt{like}, který umožňuje omezené přibližné
vyhledávání. U databáze lidských jmen se často rozděluje text jména na části
jméno, příjmení a někty titul a prostřední jméno. Problémy pak nastávají v méně
častých případech, kdy osoby např. nemají příjmení, jejich jméno je složeno z
více slov, než je počet kolonek ve formuláři, apod~\cite{name_falsehoods}.
Často by přitom stačilo modelovat jméno pouze jako jedno textové pole.

Nejčastějším řešením, kdy je požadované pokročilé vyhledávání, které databázové
systémy neposkytují, je duplikace dat z primární databáze (např. MySQL,
Postgres) do sekundárního systému pro vyhledávání (Solr, ElasticSearch,
Sphinx). Přitom by bylo jednodušší, kdyby databázové systémy přímo podporovaly
solidní vyhledávací systém formou externí komponenty. Např. databázový systém
Postgres některé vyhledávací doplňky poskytuje, ale očividně ne v takové formě,
aby se lidem nevyplatilo fragmentovat data do více systémů. 

% BIg Noise - Silver
% Gargage In - Garbage Out

\subsection{Jeden systém pro všechny případy}
V oblasti databázových systémů bylo v posledních několika desetiletích cílem
vytvořit systémy, který by byly přizpůsobitelné pro každou situaci. Podle M.
Stonebrakera je tomuto úsilí konec, protože se ukazuje, že specializovaná
architektura může být řádově výkonnější než tradiční všeobecná
architektura~\cite{Stonebraker:2005:OSF:1053724.1054024}. Příkladem jsou datové
sklady, ve kterých je charakteristická asymetrie mezi vkládáním a čtením dat.
Zatímco všeobecný systém musí být schopný vykonat jakékoliv rozložení zátěže,
ale pokud se tomuto požadavku povolí uzda a nastaví se nutnost vkládat data
dávkově, pak lze změnit uspořádání dat do velmi kompaktní formy, která lépe
zvládá dotazy používané v analytických aplikacích.

Jiným příkladem je použití databázového systému, který primárně uchovává data v
paměti RAM a sekundárně na disku. Tradiční architektura má tohle pořadí
obrácené, protože v 70. letech, kdy vznikala, bylo typické, že kapacita paměti
byla silně omezená. Prudký pokles cen paměti, disků a nástup jiných úložných
formátů (flash) zapříčinil, že toto schéma už není nejvhodnější pro současné
aplikace, protože většina z nich nepracuje s daty, které by se z většiny
nevešly do RAM.

Pokles cen a vyšší výkon hardwaru se týká i oblasti vyhledávačů, což přináší
nové příležitosti pro techniky, které byly doposud opomíjené z obdobných důvodů
zavrhované.

\subsection{Big Data a Small Data}
Jestliže se díky levnější paměti a silnějším procesorům stává výpočetní výkon
dostupnějším, existuje více směrů, jak lze tento jev aplikovat. Prvním z nich
je využít novou výpočetní kapacitu pro zpracování většího množství dat. Druhým
způsobem je výkon jednoduše vyplýtvat. Třetím je použít novou kapacitu pro
komplikovanější problémy.

První z těchto směrů se stal v poslední době velmi populární z několika důvodů.
Jednak se zvýšila dostupnost výpočetní kapacity díky firmám pronajímajícím
volnou kapacitu ve svých datových centrech (cloud). Vzestup tzv. NoSQL databází
a distribuovaného výpočetního modelu MapReduce přinesl běžným uživatelům
možnost používat distribuované algoritmy a datové struktury pro rozsáhlé datové
sady přesahující kapacity jednoho počítače (Big Data), bez větší námahy.  Podle
Stonebrakera jsou tato řešení nevhodná a jejich uživatelé postupně znovuobjeví
stejné problémy, kterým se databázoví výzkumníci věnují posledních několik
desetiletí.

Druhý směr je viditelný např. ve vzestupu dynamicky typovaných programovacích
jazyků, které silně odstiňují programátora od složitostí hardwaru za cenu
řádově pomalejšího výkonu. Tím, že je výpočetní výkon natolik levný, nejedná se
tolik o plýtvání, pokud se efektivně využije dražší čas programátora. 

Třetí směr je Opačným jevem k fenoménu Big Data, který by se odvozeně z názvu
jmenoval Small Data, znamená soustředit se na relativně malé objemy dat, ale s
použitím vlastností, které byly při minulých výpočetních kapacitách vyhodnoceny
jako příliš náročné pro praktické použití. Pojem Small Data není tolik
rozšířený jako Big Data a byl několikrát použit v různých významech. V jednom
případě se ale shoduje s významem, v jakém je v této práci
zamýšlen~\cite{small_data}. Autor navrhuje použití nadbytečného výkonu např.
pro interaktivitu v publikovaných vědeckých pracích. 

\subsection{Přibližné vyhledávání}
Motivaci této práce charakterizuje pojem small data, protože hlavní myšlenkou
je, že spousta problémů firem nepotřebuje produkty pro manipulaci s terabyty
dat, ale často jsou to pouze datové objemy, které dnešní počítače zvládnou
hravě bez opuštění rychlé paměti RAM. Například přibližné vyhledávání v textu
je jednou z oblastí, které není věnováno příliš pozornosti, ačkoliv je jeho
využití nesmírné.

Znatelný rozdíl je v požadavcích pro vyhledávání ve full-textu, tedy rozsáhlých
dokumentech (webové stránky, pdf soubory) a v relativně krátkých textových
řetězcích, jakými jsou např. jména lidí, názvy produktů, geografické lokace,
krátkých reklamních sděleních, apod. Rozdíl je především ten, že full-text je
tvořen celými větami jednoho jazyka, zatímco názvy a pojmenování jsou tvořeny
často jen několika málo slovy, které netvoří větu, a často nejsou tvořená slovy
jednoho jazyka. 

U názvů produktů často nelze rozlišit jazyk, protože jsou slova unikátní a tím,
že obsahují číslice, nelze o nich ani hovořit jako o slovech. Např. \bftt{MSI
GeForce GTX 1060 GAMING X 6G, 6GB GDDR5} je název grafické karty, \bftt{Bosch
TDA502412E} a \bftt{Tefal Autoclean ANTI CALC FV9640} jsou příklady názvů pro
žehličku a \bftt{55" LG OLED55E6V} a \bftt{55" HISENSE H55M3300} jsou názvy
televizorů. 

V databázích jmen se často vyskytují jména osob různých národností a i přes
znalost národnosti nelze správně určit jazyk jména. Např. Jakými jazyky by byla
vyhodnocena jména \bftt{Keira Knightley}, \bftt{Keira Knightleyová} nebo
\bftt{Jessica Nováková}? Cílem je pouze poukázat, že vyhledávač jmen by měl být
robustní a nespoléhat pouze na jednoduchou analýzu textu podle detekovaného
jazyka. Jazyková detekce je často možná pouze v dlouhém textu, protože se opírá
o statistické metody (např. frekvenční analýza písmen, párů písmen, slov),
která u krátkého textu selže.

Aplikace přibližného vyhledávání by mohla být pro hypotetický vyhledávač v
počítačově generovaných transkripcích mluveného textu nebo textu vygenerovaného
strojem z obrázků nebo videa. U obou případů můžeme předpokládat určitou
chybovost stroje a náročný proces, který by musel opravit tyto chyby před
vložením do databáze. Robustnost přibližného vyhledávače by se s chybami
vypořádala až během vyhledávání. Možná právě absence efektivního přibližného
vyhledávacího systému je důvodem, proč ještě neexistuje populární vyhledávač v
textech zaznamenaných v audio nebo video souborech.

%  Full-text obsahuje celé věty a tedy obsahuje tzv. stop slova
% (a, já, jsem, tu, k, nebo, ...), která pomáhají plynulosti jazyka a navazování
% myšlenek. Názvy stop slova neobsahují.

% Spousta interních databází vyžaduje efektivní a robustní vyhledávání, které 

\section{Cíl práce}
Cílem práce je navrhnout praktický vyhledávací systém s vlastnostmi
odpovídajícími požadavkům a očekáváním dnešní doby. Některé z popisovaných
technik jsou opomíjené, protože jejich implementace není jednoduchá, nebo
dlouho nebyla výpočetně výhodná. Pokrok ve výpočetním výkonu, nových
počítačových architekturách, vyšších kapacitách pamětí a obecně pokrok v
hardwaru pro ně ale poskytuje novou příležitost.

Důraz bude kladen především na přibližné vyhledávání, které je považované za
velmi výpočetně nevýhodné, pokud by se implementovalo do současných populárních
vyhledávacích systémů. Existují pro něj ale techniky, pro které by
doimplementace do stávajících systémů znamenala drastický zásah do celkové
architektury. Jinou technikou je ohodnocení podle blízkosti (proximity) slov ve
výsledcích.

Systém, který bude navrhnut v této práci se nesnaží být nejlepším řešením pro
všechny případy a explicitně se zaměřuje na specifičtější případy, ve kterých
by přibližné vyhledávání přineslo velkou hodnotu. Kompromisem je, že se
navrhovaný systém nesnaží jít stejnou cestou velkých dat (Big Data), ale přesně
opačně by mělo představované řešení nalézt uplatnění pro menší objemy dat
(Small Data).

Práce je koncipována tak, aby přiblížila některé opomíjené příležitosti ve
vyhledávání, přiblížila dosavadní výzkum v těchto oblastech a na prototypové
aplikaci demonstrovala praktickou proveditelnost navrhovaného řešení. Prototyp
bude porovnán s některými populárními open source vyhledávacími systémy, které
budou konfigurovány tak, aby splňovaly podobné požadavky.


\section{Porovnání}
Protože je dalších vyhledávacích systémů mnoho, bude vytvořeno pouze několik
konfigurací pro ElasticSearch a Postgres, které v současnosti reprezentují
nejpoužívanější open source systémy s porovnatelnými schopnostmi.

\subsection{Relevance}
Vyhodnocení toho, který systém je lepší nebo horší, je z velké míry subjektivní
záležitost. Existují způsoby, jak hodnotit míru relevance vyhledávače pomocí
metrik přesnost (precision) a výtěžnost (recall), pokud se výsledky vyhledávání
uvažují pouze jako množiny. Pro srovnání výsledků vyhledávačů nás bude více
zajímat pořadí, ve kterém se relevantní výsledky vyskytnou a robustnost
vyhledávače při změnách dotazu. Pro uživatele je frustrující, pokud má
vyhledávač vlastnost, že při malé změně dotazu, např.  změnou koncovky slova,
mu systém vrátí naprosto odlišné výsledky, rozdílný počet, nebo při této změně
dokonce nic nevrátí. U webového vyhledávání to tolik nevadí, protože i přes
změnu dotazu vyhledávač vrátí velkou množinu výsledků.  Uživatel nepozná,
jestli se výsledky ztratily, nebo byly pouze seřazeny níže. U menších dat dojde
k tomu, že se záznamy z výsledků ztratí, čehož si uživatel ihned všimne.

Porovnání proběhne na několika datových sadách o několika stovkách tisíc
záznamech, které budou reprezentovat datové sady pro případy, ve kterých by se
navrhovaný vyhledávač měl nejlépe uplatnit. Porovnáním jsou ukázky interakce a
zhodnocení několika uživatelů, kterým byl systém prezentován v porovnání s
ostatními implementovanými vyhledávači.

\subsection{Výkon}
U instantního vyhledávání platí pravidlo, že dotazy vykonané pod hranici
100\,ms jsou lidmi vnímány jako instantní~\cite{sto_ms}. Do této doby se musí
vejít jednak samotné vykonání algoritmu, a jednak doba pro přenos dat, která
nebude zanedbatelná, pokud bude vyhledávací aplikace poskytována jako služba ze
vzdáleného počítače.

Protože bude návrh nového vyhledávače prezentován pouze jako prototyp v
dynamicky typovaném Pythonu, kvůli čemuž nelze provést objektivní srovnání
výkonu. Rychlost běhu programu v Pythonu se oproti nativní implementaci se liší
řádově, a proto použiji hrubé pravidlo jedné vteřiny, tedy pokud očekáváme
výkon prototypu desetkrát pomalejší, pak by se měla doba běhu vejít do
desetinásobku kritické hranice pro lidské vnímání.

Vyhledávač by rovněž neměl být příliš paměťově náročný. Na jednu stranu tím, že
se soustřeďuje na malé objemy dat a paměť se neustále zlevňuje, by neměl být
problém využít volné kapacity pro zvýšení výkonu. Na druhou stranu pokud nebude
vyhledávač nenasytný, pak se do paměti vejde více dat a nebude nutné zasahovat
na disk, což je u nízkolatenčního vyhledávání velkou překážkou. Několik
náhodných zásahů na disk by znamenalo instantní porušení hranice 100\,ms, proto
by měl být systém schopný pojmout všechna data do paměti.



% Díky vzestupu počítačů a především internetu došlo k dramatickému nárustu
% objemu textových informací ve formě snadno přístupné lidem i pro počítačové
% zpracování. Textová data jsou specifická v tom, že jejími tvůrci jsou lidé a ne
% počítačové systémy nebo měřící čidla. Při vyhledávání v takovýchto datech máme
% jako lidé silná očekávání na relevanci výsledků vyhledávání, nicméně kvůli
% charakteristice textových dat je vyhledávání v nich obtížné. Tím, že je úkol
% obtížný, existuje více specifických zaměření, než jedno obecné a použitelné pro
% všechny případy.
%
% S nástupem World Wide Webu v 90. letech došlo k vzestupu Webových vyhledávačů,
% které dnes slouží jako vstupní brána do světa informací.
% ...
%
% V této práci se snažím najít alternativní způsoby v tomto širokém oboru, které
% nejsou velmi známé, a zaměřím se na oblast, která je dle mého názoru jednou z
% nejžádanějších v praktických aplikacích. V současné době se pro ni ale
% používají techniky pro příliš obecné a zdrojově náročné prohledávání Webu a
% nedosahují takových výsledků, jakých by mohly, nebo za příliš vysokou cenu
% komplikovaných řešení.
%
% Velkým důvodem proč převažují techniky pro Webové vyhledávání je psychologický
% - většina problémů, se kterými se potýkají firmy i nekomerční projekty, zdaleka
% nepracuje s takovými objemy dat, se kterým se musí potýkat rozsáhlé webové
% vyhledávače nebo světové sociální sítě. V praxi jsou tyto problémy často
% řešitelné s použitím pouze jednoho počítače. Nicméně lidé přehnaně věří, že
% jejich projekty budou jednou dosahovat obdobných závratných velikostí, a proto
% volí řešení nevhodné pro jejich problémy.
%
% V ideálním případě by měla být podpora pro efektivní textové vyhledávání
% zabudována přímo do databázových systémů, ale v současné době není jejich
% podpora ideální, a proto aplikace s podporou textového vyhledávání používají
% kromě databáze ještě paralelní systém specializovaný pro text. Důvod je ten, že
% je obtížné vytvořit vyhledávač v takové formě, aby vyhovoval obecným
% požadavkům, na které se databáze používají. Techniky představené v této práci
% by měly být vhodnější pro generické účely, než ty, které používají Webové
% vyhledávače a ze kterých se čerpá inspirace pro textové indexy vestavěné do
% databází.
%
% ...
%
% V první části uvedu problém vyhledávání v textu, aby čtenář pochopil z jakého
% základu tato práce vychází. V druhé části na současný stav v praktickém světě
% textového vyhledávání a jak jsme se do něj dostali. Pak se zaměřím na algoritmy
% a datové struktury textového vyhledávání a alternativní techniky, které jsou v
% některých případech exotické a nepříliš vhodné na obecné použití, ale jiné
% které by dle mého názoru zasloužily větší pozornost, protože jejich zaměření
% odpovídá současným potřebám více, než na které je v dnešní době kladen největší
% důraz. V další části rozeberu vybranou oblast textového vyhledávání více do
% hloubky a popíšu problémy, které se v ní vyskytují. Zároveň rozeberu jejich
% řešení různými výzkumnými týmy a můj návrh na řešení. V sekci Analýza
% kvantitativně poukážu na konkrétní problémy popsané v předchozí sekci a
% problémy implementace takového systému. Na závěr představím možná budoucí
% řešení a další kroky k vylepšení, aby bylo možné vyhledávací systém nasadit v
% praxi.
%
% \chapter{Motivace}
%
% \subsection{Databáze a textové vyhledávání}
% Textové vyhledávání se často uvádí odděleně od databázových systémů, přestože
% by v ideálním případě mělo být součástí databází. Důvodem je silně různorodá
% povaha textových dat s nejednoznačným způsobem zacházení. Pokud v databázích
% pracujeme například s čísly, pak je situace celkem snadná, protože čísla jsou
% snadno a jednoznačně porovnatelná, ať už to jsou floating nebo celá čísla. Díky
% seřaditelnosti pak můžeme použít algoritmus binární vyhledávání, nebo
% dynamickou obdobu v podobě binárních vyhledávacích stromů.
%
% Jiné datové typy jako třeba intervaly jsou složitější, ale přesto existují
% pevně definovatelné způsoby pro jejich seřazení a tedy snadné vyhledávání.
% Intervalovými daty mohou být jednorozměrná časová rozmezí, dvourozměrná
% geografická data, nebo vícerozměrná data často používaná v počítačové grafice.
% Existuje pro ně velké množství relativně efektivních algoritmů a aktivní
% výzkum. Časová data jsou na první pohled lehce seřaditelná, jenže kvůli lidským
% "obohacením", jako jsou časové zóny, letní čas, nebo více dimenzí času (čas
% platnosti, čas záznamu) se jejich zacházení v databázích komplikuje. Textová
% data generovaná lidskou řečí jsou ještě komplikovanější a často se řeší mimo
% databázové systémy. Ve výsledku používá spousta uživatelských aplikací s
% databázovou podporou ve skutečnosti dva systémy - kromě klasické databáze ještě
% speciální systém pro relevantní a efektivní textové vyhledávání.
%
% Problém s textem, který nás zajímá, je jeho nejednoznačná seřaditelnost.
% Představme si databázi lidských jmen a aplikaci určenou pro vyhledávání v nich.
% Počítá se s tím, že pokud vyhledáváme konkrétní osobu, nevíme přesně jak je
% její jméno v databázi uloženo. Kdybychom věděli, že se Jaromír Kobliha v
% databázi vyskytuje v konkrétním tvaru Jaromír Kobliha nebo Kobliha, Jaromír
% nebo Dr. Jarda Kobliha, pak jednoduše zadáme dotaz na přesnou shodu a máme
% vyřešeno. Jenže naše očekávání jsou jiná. Člověk by všechny tyto tvary jména
% vyhodnotil ekvivalentně, a tím pádem je naším úkolem vytvořit podobně chytrý
% systém anebo alespoň iluzi takového systému.
%
% Jména mohou být jednoduše rozdělena na více sloupců - křestní jméno, prostřední
% jméno, příjmení, titul(y), tak jak to známe, pokud vyplňujeme kdejaké
% formuláře. To umožní systému provést oddělený dotaz v každém tomto sloupci a
% sloučit výsledky. Nebo jména seřadit nejprve podle příjmení, pak podle jména a
% titulů naposled (Kobliha, Jaromír, Dr.), protože předpokládáme, že existuje
% méně příjmení než křestních jmen a ještě méně titulů. Tohle je ovšem jenom
% heuristika, kterou nelze aplikovat např. ve Vietnamu (Nguyen a Tran tvoří 50\%
% všech příjmení) nebo v Jižní Korey (Kim, Lee, Park a Choi tvoří 50\% všech
% příjmení). Jiné kultury nemají ani koncept příjmení, proto je takový systém
% obecně nedostatečný.
%
% Pro větší záznamy - celé dokumenty o několika stovkách až tisících slov - nelze
% ani uvažovat seřaditelnost podobně naivním způsobem. Přestože je vyhledávání v
% textu obtížné, existují možnosti, jak povahu lidského textu využít a vytvořit
% algoritmy umožňující efektivní vyhledávání.
%
% Statisticky můžeme pozorovat unikátní povahu lidského jazyka v několika statistických pozorováních.
% % Frekvenční analýza - codebreaking
% % 1, 2,3 - více jedničěk distribuce
% % zipf distribuce slov
% % redundance - deflate a jiný textový komprese
%
% [1] \url{https://en.wikipedia.org/wiki/Information_retrieval#History}
% [1] \url{https://www.theguardian.com/commentisfree/2015/apr/18/google-eu-monopoly-inquiry-too-late-to-stop}


% \section{Změny v architekturách počítačů a počítačových systémů}
% Stonebraker o databázích
% sloupcové a in-memory db
% něco o nosql
% hierarchie paměti
% ssd
% nvram

% konvergence db a fulltextu. Podobnost se sloupcovými db. Not yet because of
% reasons below. Columnar dbs for analytical slow access. tens of thousands
% columns needed for each word. If some words have short inverted list, then
% waste,because blocks have minimum size.

% \subsection{Nové potřeby ve vyhledávání}
% \subsection{Vertikální vyhledávání}
% Dle hlavního vedoucího výzkumu v Googlu, Petera Norviga, je úspěch Googlu
% založen ne na lepších algoritmech, než by měli ostatní, ale jednoduše tím, že
% má více dat.
% % přístup googlu - chytré scrapování. Přitom data existují ve zpracované formě.
% % Nedostatek vertikálního a site search? Špatné nebo obtížné open source
% % řešení?
% \subsection{Linked data}
% \subsection{Mobilní zařízení}


\chapter{Textové vyhledávání}
V této části práce budou představena teoretická východiska - tedy techniky,
kolem kterých práce staví. Budou představena pokročilá uživatelská rozhraní pro
vyhledávací systémy, algoritmy pro vyhledávání v textu a rozšiřující algoritmy
pro techniky, na které se práce zaměřuje.

\section{Definice problému}

\section{Uživatelské rozhraní}
Základním uživatelským rozhraní pro textové vyhledávání je obyčejný řádek, do
kterého uživatel napíše svůj dotaz a po potvrzení mu vyhledávač obratem pošle
sadu dokumentů, které ohodnotí jako nejrelevantnější vzhledem k jeho dotazu.
Tento koncept se silně osvědčil díky svojí jednoduchosti. Některé vyhledávače
rozšiřují tento jednoduchý koncept například tím, že s uživatelem reagují už
během jeho vkládání dotazu a nečekají na potvrzení. Tahle vlastnost se začala
rozšiřovat v první dekádě tisíciletí, kdy začaly webové stránky více používat
techniku AJAX - tedy komunikace webové stránky se serverem bez toho, aniž by
uživatel explicitně vyslal požadavek.

Vyhledávače interpretují dotaz nejčastěji tak, že text rozdělí na slova a
naleznou všechny dokumenty, které obsahují všechna zároveň. Kdybychom si chtěli
dotaz představit v explicitní formě, pak by dotaz \bftt{forrest gump} vypadal
jako \bftt{forrest AND gump}, kde operátor \bftt{AND} označuje konjunkci.

Pak záleží na vyhledávačích, jaké další funkce uživateli poskytnou. Mezi
populární patří operátory \bftt{NOT} a frázové vyhledávání. \bftt{NOT} se
vztahuje ke konkrétnímu slovu a má efekt, že tohle slovo nesmí být přítomno ve
výsledcích. Frázové vyhledávání slouží k odfiltrování výsledků, které nejsou
těsně vedle sebe. Ačkoliv jsou tyto dva operátory takřka standardem, přesto
jsou v různých vyhledávačích implementovány různým způsobem, např. symbolem
\bftt{-} nebo slovem \bftt{NOT}, a frázové vyhledávání se interpretuje různě v
závislosti na vyhledávači. Navíc samotné vyhledávače obměňují svoji rozšířenou
funkcionalitu, a proto i operátory, které dříve fungovaly a spousta uživatelů
si na ně zvykla, buďto už nefungují, nebo nesplňují dřívější očekávání.

Ve výsledku jsou rozšiřující funkce vyhledávačů vlastností pro stálé a
pokročilé uživatele. Pokud tedy implementujeme nový vyhledávač, který nemá
letitou stálou základnu uživatelů, musíme implementovat pokročilé vlastnosti
bez použití pokročilých operátorů. Tedy vyhledávač dostane pouze základní
textovou informaci, kterou pak může inteligentně interpretovat.

\subsubsection{Inkrementální vyhledávání}
Jednou technikou, kterou může vyhledávací systém implementovat bez nutnosti
učit uživatele svým pokročilým funkcím, je vyhledávání ještě před tím, než
uživatel dokončí svůj dotaz. Této technice se říká mnoha způsoby - kromě
inkrementálního vyhledávání - instantní vyhledávání, typeahead search, search
as you type, a další. Pro uživatele je tento způsob interakce pohodlný jednak v
tom, že nemusí dokončovat svůj dotaz, a jednak že dokonce nemusí znát svůj
dotaz úplně, a přesto mu vyhledávač nalezne odpověď. Inkrementální vyhledávání
zkracuje interakční smyčku s uživatelem. Ten namísto opakovaného vkládání a
potvrzování nového dotazu, pokud mu nevyhovují výsledku, pouze mění svůj dotaz
a vyhledávač reaguje instantně.

V plné formě by měl inkrementální vyhledáváč totožné rozhraní pro výsledky,
které uživateli prezentuje ještě před dokončením a zároveň pro ty, které
uživateli vrátí po potvrzení dotazu. Nejen uživatelské rozhraní by bylo stejné,
ale i množina výsledků a jejich pořadí, které v obou případech vrátí. Ve slabší
formě má dvě rozhraní. Kromě běžného rozhraní pro prezentaci výsledků má ještě
tzv. našeptávač, který reaguje instantně s každým nově zadaným písmenem dotazu.
Díky oddělení obou rozhraní nemusí našeptávač vracet tytéž výsledky, jaké by
vyhledávač vrátil po potvrzení - proto je to pouze našeptávač.

Plná forma inkrementálního vyhledávání (prefixové vyhledávání) je striktně
náročnější na výpočetní výkon, protože vykonává plný dotaz pro každé písmeno
dotazu. Některé systémy to řeší ukládáním průběžných výsledků nedokončených
dotazů do mezipaměti s krátkou žívotností (cite tastier, completesearch?).
Populární je slabší forma inkrementálního vyhledávání využívající dvou
uživatelských rozhraní, která vykoná pouze výpočetně méně náročnou verzi dotazu
při každém novém písmenu.  Jejím příkladem by mohlo být vyhledávání pouze v
titulcích nebo názvech dokumentů při našeptávání a plné vyhledání v celé datové
sadě po potvrzení dotazu.  Jiným způsobem je využití záznamu všech dotazů
(query log). Toho využívají rozsáhlé webové vyhledávače, díky velkého počtu
svých uživatelů a jimi vykonaných dotazů. Statistické informace jsou zpětně
zabudovány do našeptávače, který pak doplňuje nové dotazy jejich
nejpopulárnějším dokončením.

Prefixově interpretovaný dotaz může mít dvě základní varianty. V té první se
každé zadané slovo dotazu uvažuje jako prefix nedokončeného slova. Např. dotaz
\bftt{motor pil} bychom interpretovali prefixově v notaci \bftt{motor* pil*} a
očekávali od něj výsledek \bftt{motorová pila}. V druhé variantě interpretujeme
prefixově pouze poslední slovo, tedy \bftt{motor pil*}. Odůvodněním je, že
očekáváme, že uživatel vkládá nová písmena na konec dotazu a ne, že je vkládá
v náhodném pořadí.

\subsection{Přibližné vyhledávání}
V některých případech je žádoucí, když vyhledávač pomáhá vyhledávat navzdory
morfologii jazyka. Při prohledávání celých textových dokumentů je pro užívatele
frustrující situace, kdy hledá výraz - nejčastěji v prvním mluvnickém pádu -
ale výskyt fráze je v dokumentu v jiném pádu nebo čísle. Přibližným
vyhledáváním se také rozumí, pokud dokáže systém napovědět, nebo rovnou
vyhledat výsledky, přestože dotaz obsahuje překlepy (\bftt{metamatyka} ->
\bftt{matematika}), zdvojená písmena nebo naopak (\bftt{forest gump} ->
\bftt{forrest gump}), nesprávné rozdělení slov (\bftt{i phone} ->
\bftt{iphone}), inteligentní nahrazování některých slov jejich synonymy, nebo
expanze zkratek na plný výraz (\bftt{U.S. -> United States}).

Některé techniky je obtížné implementovat, protože závisí na podrobné analýze
dat. Např. přibližná shoda slov na základě morfologie závisí na správné detekci
jazyka dokumentu, nebo dokonce jednotlivých slov, protože v dnešním
globalizovaném světě není netypické míchání cizích slov do textu jiného jazyka.
Nahrazování slov synonymy a expanze zkratek mohou být závislé na oblasti, ze
které dokumenty pocházejí, a vyžadovat definování nahrazovacích pravidel
experty z oblasti. Oproti tomu kontrolování překlepů je relativně snadno
implementovatelné, protože je závislé pouze na znalosti kompletního slovníku
datové kolekce.


\subsection{Filtrované vyhledávání}
Textové vyhledávání může být kombinováno s různými filtry pro vlastnosti, které
výsledky obsahují. Dokonce ještě přes započetím vkládání dotazu může být
uživateli prezentována omezená množina celé kolekce, seřazená např. podle
popularity v poslední době. Vedle této množiny mohou být představeny různé
filtry nebo všechny (nebo alespoň nejpopulárnější) kategorie, kterými lze
datovou sadu rozdělit. Uživatel pak dokonce nemusí ani vepisovat dotaz a pouze
se k výsledkům dopracuje postupnou interakcí s nabízenými filtry, které
postupně reflektují zmenšující se množinu. Tedy např. z nabídky filtrů zmizí
všechny kategorie, které nemají v omezené množině zastoupení. Pole pro
vyhledávání pak lze chápat jako pouze další filtr.

\section{Povaha textových dat}
% zipf, heaps

\section{Algoritmy pro textové vyhledávání}
Implementace a výběr algoritmů bude záviset na vlastnostech, které po
vyhledávači požadujeme a na povaze dat, ve kterých bude vyhledávat. Budeme
předpokládat, že datová sada, ve které se bude vyhledávat, je dostupná celá
nebo její většina, ještě před tím, než dojde k prvnímu dotazu. V takovém
případě nás budou zajímat tzv. offline a statické algoritmy.  Opakem by byla
situace, kdy text přichází jako proud jednotlivých záznamů, a v tomto případě
hovoříme o tzv. online problému. Pokud bychom neměli možnost uchovat všechny
tyto přicházející záznamy v nějaké datové struktuře, pak bychom museli použít
tzv. streamovací algoritmy. Opačně bychom datovou strukturu budovali postupně.
Takový algoritmus se v kontrastu ke statickému nazývá dynamický. Oproti
statickému musí být přizpůsobený na postupné úpravy datové struktury a zároveň
vykonávat dotazy přícházející od uživatelů.

Dynamické algoritmy jsou obtížné na implementaci, protože musí neustále
synchronizovat přístup do datových struktur zvlášť pro zápis a pro čtení.
Naopak statické algoritmy a datové struktury lze maximálně přizpůsobit datům,
např. provedením těsné komprese dat, která by v případě dynamické datové
struktury ztěžovala vkládání nových záznamů. Algoritmy pro textové vyhledávání
mají často statický charakter, protože se velká část dat nemění. Nové záznamy
nebo úpravy stávajících záznamů lze proto uchovat ve vejdlejší a menší
dynamické datové struktuře a při dosažení určité velikosti se sloučí s větší
statickou v jednorázové dávkové operaci. Sloučení nemusí vyústit pouze v jednu
velkou datovou strukturu, ale může se slučovat postupně. Takové datové
struktuře říkáme vícegenerační, protože je tvořena několika generacemi, ve
kterých musíme paralelně provést vstupní dotaz a následně sloučit výsledky.

\subsection{Index}
Textová data, ve kterých se vyhledává, můžeme ponechat v původní textové
reprezentaci, nebo ji zakódovat tak, aby se urychlilo vyhledávání. Pokud
uvažujeme text jako posloupnost slov, pak je jednoduchým indexovacím schématem
převést každé slovo v textu na nějaké celé číslo a uchovat si k němu pozici, na
které se v původním textu nachází. Ntici (identifikátor dokumentu,
identifikátor slova, pozice v dokumentu) bude unikátně označovat jeden výskyt.
Překlad ze slov na konkrétní číslo (identifikátor) bude uložen v datové
struktuře, kterou označíme jako slovník (lexikon).

Výhodou uchování datové sady v indexovaném formátu bude její menší velikost,
čehož dosáhneme navíc díky kompresi, která využívá nerovnoměrné povahy
textových dat. Často se vyskytujícím slovům mohou být přiřazena malá čísla a
naopak unikátnějším slovům větší čísla. Menší velikost datové struktury
znamená, že se jí více vejde do paměti, což umožňuje vyhnout se pomalým dotazům
na externí ůložiště.

Vyhledání slova v takové datové struktuře znamená převod vstupních slov na
korespondující identifikátory pomocí slovníku a následný průchod celou datovou
kolekcí. Při průchodu si zaznamenáme všechny dokumenty, které obsahují zároveň
všechna dotazovaná slova a jejich pozice v dokumentu.

Indexu, který si ponechává informaci o pozicích výskytu, říkáme poziční index.
Ten je ekvivalentní s původními daty a proto i při zamíchání ntic výskytu
můžeme data zpětně zrekonstruovat. Pozice ukládat nemusíme, a v takovém případě
bude index odpovídat množině slov pro každý dokument.

% TODO codes.

\subsection{Invertovaný index} \label{sec:inverted_index}
Protože je typickým dotazem velmi krátký text v porovnání s dlouhým dokumentem,
průchod obyčejným indexem od začátku do konce vrátí pouze malé množství
nalezených dokumentů poměrně k počtu všech dokumentů. Invertovaný index
(inverted index) je datová struktura, která umožňuje rychle nalézt pouze ty
dokumenty, které obsahují konkrétní slovo.
% TODO průměrná délka dotazu google, yahoo, atd.

Obyčejný index lze převést na invertovaný index tak, že k sobě sloučíme všechny
výskyty - tedy páry (id\_dokumentu, id\_slova, pozice) - podle identifikátoru
slova. Vyhledání dokumentů v takto upraveném indexu proběhne jednoduše tak, že
pro dotazované slovo vyhledáme jeho konkrétní sloučenou skupinu výskytů. Tyto
skupiny (invertované seznamy) jsou prakticky množinou dokumentů, ve kterých se
slovo vyskytuje. Vyhledání více slov znamená pouze dohledání korespondujících
invertovaných seznamů a jejich sloučení provedením zvolené množinové operace.
Protože je možné uchovat invertované seznamy seřazené podle identifikátoru
dokumentu, je sloučení velmi rychlou lineární operací nad více invertovanými
seznamy.

Indexu se říká invertovaný, protože se efektivně v ntici výskytu zamění pořadí
dokumentu a slova a obyčejný index se seřadí dle tohoto nového pořadí. Pro k
invertovanému indexu budeme nazývat obyčejný index jako dopředný index (forward
index). Invertovaný index by měl být znám všem, kdo čtou knihy, protože
rejstřík na posledních stránkách není ničím jiným než právě invertovaným
indexem, kde jedním záznamem je stránka knihy.

Invertovaný index má ještě lepší kompresní vlastnosti než dopředný index,
protože může pro používat oproti instantním efektivnější blokové kompresní
metody, které drasticky sníží velikost invertovaných seznamů odpovídajících
často se opakujícím slovům. Má však horší kompresní vlastnosti pro záznamy o
pozicích výskytu, protože se v tomhle schématu nevyskytují striktně za sebou.


\subsection{Prefixové vyhledávání} \label{sec:prefix_search}
Instantní vyhledávání můžeme implementovat více způsoby. Nejjednodušším je
zaindexováním všech možných prefixů a slova samotného. Tedy pro slovo
\bftt{podlaha} se zaindexují výrazy \bftt{p}, \bftt{po}, \bftt{pod},
\bftt{podl}, \bftt{podla}, \bftt{podlah}, \bftt{podlaha}. Při vyhledávání není
nutné dotaz nijak upravovat, protože při existující shodě v dokumentech bude
odpovídající výraz v takovém indexu přítomný. Nevýhodou je násobná velikost
výsledného indexu, protože se efektivně indexuje kvadratické množství slov.
Často se velké množství omezuje minimální a maximální délkou prefixů¸ které se
zaindexují.  Očividně pak vyhledávač nemůže vyhledávat při krátkých dotazech a
nebude přesný pro delší prefixy.

Jiným způsobem je použít klasický invertovaný index beze změny a vypočítat
prefixy až při dotazování.  Docílíme toho přepsáním dotazu do formy konjunkce
více disjunkcí. Např. dotaz \bftt{motor* pil*} by byl přepsán do tvaru
\[\bftt{(motorová OR motorových OR ...) AND (pila OR pil OR pilách OR ...)}\]
podle všech odpovídajících slov ve slovníku.

Problém nastane u krátkých prefixů, protože odpovídají velkému počtu slov.
Dotaz s disjunkcí tisíců invertovaných seznamů bude pravděpodobně méně výkonný
než prostý průchod celou datovou kolekcí.

Řešením bude poloinvertovaný index popsaný v sekci ...
% TODO reference na sekci

\section{Algoritmy pro přibližné vyhledávání}
V této práci se budeme zabývat pouze technikami, které uvažují slova pouze jako
posloupnost znaků. Tedy vynecháme hledání synonym nebo expanzi zkratek, které
silně závisí na kontextu a na obsahu dat.

\subsection{Stematizace}
Velkým problémem textových vyhledávačů je tvarosloví jazyka (morfologie - různé
tvary slov, např. dobrý, dobrou, dobrému, dobrých).  Například při dotazování
vyhledávače na výraz \bftt{motorová pila} nás jistě budou zajímat i výsledky,
které obsahují text \bftt{motorové pile} nebo \bftt{motorových pilách}. Nelze
po uživateli požadovat, aby vyjmenoval všechny tvary slov, které zadává.

V češtině a v dalších indoevropských jazycích dochází k nejvíce morfologickým
změnám na koncích slov. Klasickým řešením je před indexováním dat provést
jazykovou analýzu a převést slova na kmenový tvar (stematizace). Ve výsledku se
místo slova \bftt{motorových} zaindexujeme slovo \bftt{motor} a poté při
vyhledávání se provede identická konverze dotazovaných slov. Vyhledávač by tedy
původní výraz \bftt{motorová pila} nejprve převedl na \bftt{motor pil} a
vykonal dotaz s tímto modifikovaným výrazem. Co se považuje za kmen slova je
určeno vybraným algoritmem. Kromě stematizace existuje ještě technika
lemmatizace, která místo kmenu slova hledá jeho základní (slovníkový) tvar.

V jazykové analýze existuje mnoho problémů. Především je závislá na konkrétním
jazyce. Může se stát, že některé datové kolekce obsahují záznamy ve více
jazycích. Pokud je text dostatečně dlouhý nebo obsahuje znaky specifické jen
pro daný jazyk, lze jazyk záznamu detekovat frekvenční analýzou před
morfologickým rozborem. V jiném případě může datová sada obsahovat záznamy v
jednom jazyce, ale jednotlivá slova mohou být v jiném. Pak jednoduchá
frekvenční analýza nebude dostačovat a museli bychom zvolit složitější techniky
pro jazykový rozbor, které jsou ale mimo rozsah této práce.

Problém s lemmatizací je také nejednoznačnost základního tvaru. Například slovo
\bftt{tancích} může mít základní tvar \bftt{tank} nebo \bftt{tanec} podle
významu slova, který by člověk určil z kontextu věty.

\subsection{Editační vzdálenost}
Obecně můžeme podobnost dvou textů můžeme měřit pomocí tzv. editační
vzdálenosti (edit distance). Základní editační vzdálenost měří minimální počet
písmen, které musí být odebrány nebo přidány, aby z prvního řetězce vzniknul
druhý. Např. mezi slovy \bftt{karel} a \bftt{kremrole} je editační vzdálenost
5, protože bychom první slovo transformovali na druhé odebráním znaku \bftt{a}
a přidáním znaků \bftt{m},~\bftt{r},~\bftt{o}~a~\bftt{l} na patřičná místa.
Základní editační vzdálenost můžeme alternativně vypočítat jako \[\bftt{edit}(x,
y) = |x| + |y| - 2\,\bftt{lcs}(x, y)\], kde $\bftt{lcs}$ (longest common
subsequence) označuje nejdelší společný podřetězec textů $x$ a $y$ a $|x|$
znamená délku textu $x$.  Tato metrika se proto také označuje jako vzdálenost
největších společných podřetězců.

\paragraph{Levenshteinova vzdálenost} je rozšířením~\cite{Levenshtein66}, které
kromě odebrání a přidání uvažuje navíc záměnu písmen (substituce) za jednu
operaci s váhou 1.  Prakticky se liší pouze tím, že záměnu písmen penalizuje
vahou 1, zatímco základní editační vzdálenost bere záměnu jako jedno přidání a
jedno odebrání, tedy s penalizací 2. Pokud se běžně hovoří o editační
vzdálenosti, je jí myšlena právě ta Levenshteinova, proto budeme označovat
pojmem editační vzdálenost právě ji.

\paragraph{Damerau-Levenshteinova vzdálenost} je druhým
rozšířením~\cite{Damerau:1964:TCD:363958.363994}, které navíc uvažuje záměnu
dvou sousedících písmen (transpozice) jako operaci s vahou 1.  Bez této
podmínky by byla vzdálenost transpozice vypočtena jako jedno odebrání a jedno
přidání celkem s váhou 2.

\paragraph{Prefixovou editační vzdálenost} definujeme abychom mohli zkombinovat
inkrementální vyhledávání s tolerancí překlepů. Vypočteme ji jako nejmenší
editační vzdálenost prvního řetězce k prefixům druhého \[\bftt{p\_edit}(x, y) =
\min_{p \in prefix(y)} \bftt{edit}(x, p)\], kde $prefix(y)$ označuje množinu
všech prefixů řetězce $y$~\cite{Bast:2013:EFS:2457465.2457470}. Př.
$\bftt{p\_edit}(ker, karel) = 1$, protože $\bftt{edit}(ker, kar) = 1$. Oproti
předchozím metrikám není symetrická v $x$ a $y$. 

% TODO damerau paper?

\subsection{Výpočet editační vzdálenosti}
Pro obyčejnou, Levenshteinovu i Damerau-Levenshteinovu vzdálenost se používá
Wagner-Fisherův tabulkový algoritmus s kvadratickou náročností
$\Theta(|x||y|)$, kde $|x|$ a $|y|$ jsou délky dvou porovnávaných řetězců.
Algoritmus byl nezávisle objeven více lidmi (např. Vintsyuk, Needleman -
Wunsch, a další)~\cite{Navarro:2001:GTA:375360.375365}.

Obdobný tabulkový algoritmus se používá pro výpočet nejdelšího řetězce se
stejnou kvadratickou náročností, proto nepomůže výpočet přes alternativní
formu. Náročnost lze snížit, pokud nastavíme strop pro maximální možnou
editační vzdálenost pomocí Ukkonenova algoritmu nebo Ukkonenova prořezávání.

\subsection{Hranice pro editační vzdálenost}
Ohodnocení všech položek slovníku editační vzdáleností ku dotazovanému slovu je
zbytečně obecný problém. Nám bude stačit nalezení slov do určité vzdálenosti,
jehož výpočet je efektivnější a hlavně při správně zvolené datové struktuře
nemusí vypočítávat vzdálenost ke všem slovům.

Omezení se provádí adaptivně podle délky slova. Vysoká tolerance pro krátká
slova by znamenala spoustu shod, které by uživatel nevyhodnotil jako podobné.
Např. vzdálenost mezi krátkými slovy \bftt{na} a \bftt{ty} je 2, přitom spolu
nemají slova pranic společného. Naopak u delších slov je větší šance, že dojde
k překlepům, proto práh můžeme zvýšit. Dokonce pro velmi dlouhá slova bude
platit, že ve slovníku bude pouze omezené množství dlouhých a zároveň podobných
slov, proto si můžeme dovolit limit nastavit poměrně vysoko. 

Jako řešení můžeme definovat relativní editační vzdálenost jako
\[\bftt{edit\_rel}(x, y) = \frac{\max (|x|, |y|) - edit(x, y)}{\min (|x|,
|y|)}\], která zohledňuje délku obou slov. Nebo jednoduše nastavit
konfigurovatelnou hranici podle délky vyhledávaného slova. Např. pro slovo
delší než 10 znaků se použije limit 4, pro slova delší než 7 limit 3, apod.

% TODO ukkonen popsat



\subsection{N-gramová podobnost}
Pro přibližné vyhledávání můžeme indexovat jiné díly původního textu než slova
- n-gramy (nebo také q-gramy). N-gram je posloupnost několika po sobě jdoucích
znaků. Např. n-gramy slova \bftt{podlaha} o délce 3 (trigramy) jsou \bftt{pod},
\bftt{odl}, \bftt{dla}, \bftt{lah}, \bftt{aha}. V tomto případě se jejich znaky
překrývají, a proto hovoříme o překrývajících se trigramech.

Podobnost dvou textů pomocí n-gramů můžeme efektivně zjistit pomocí Jaccardovy
podobnosti množin. Tu zjistíme jako poměr velikosti průniku ku velikosti
sjednocení obou množin \[\bftt{jacc}(X, Y) = |X \cap Y|\ /\ |X \cup Y|\]. Pokud
porovnávané texty reprezentujeme jako dvě množiny jejich n-gramů, vypočteme
jejich podobnost jako podobnost těchto množin.

Tento způsob funguje pro krátké řetězce a nízkou toleranci odlišnosti. Podobná
slova budou sdílet všechny n-gramy vzdálené $n$ pozic od místa, kde mezi nimi
došlo ke změně. Např. slova \bftt{kremrole} a \bftt{kremrule} se
levenshteinovou vzdáleností 1 budou mít Jaccardovu vzdálenost trigramů pouze
$0.5$. Aby metoda fungovala, musí být text dlouhý alespoň dvakrát delší než
velikost n-gramu, pokud ke změně dojde uprostřed textu. Proto tenhle způsob
selhává u kratších slov.

Kromě Jaccardovy podobnosti můžeme použít i další metriky pro podobnost množin
jako euklidovskou, kosínovou nebo Jaro-Winklerovu vzdálenost, pokud převedeme
množiny na vektory, kde jedna dimenze označuje přítomnost jednoho znaku nebo
n-gramu textu.

% Vztah mezi Jaccardovou metrikou a základní editační vzdáleností je
% $\bftt{jacc}(x, y) = 1 - |\bftt{lcs}(x, y)|\ /\ (|\bftt{lcs}(x, y)| +
% \bftt{edit}(x, y))$, pokud jsou prvky množiny jednotlivá písmena textu.

\subsection{Reprezentace slovníku}
Pokud zvolíme fuzzy technikou stematizaci, pak lze implementaci slovníku pro
invertovaný index provést jednoduchou hashovací tabulkou, která si u
záznamenaných kmenů slov poznamená všechny původní tvary. V praxi se odvozené
tvary často zahazují a nerozlišuje se ve výsledcích vyhledávání rozdíl mezi
přesným zásahem a odvozeným tvarem. Slovník pro index s podporou n-gramového
indexu rovněž postačí obyčejná hashovací tabulka.

\subsection{Prefixový slovník}
Pro nalezení všech slov v prefixovém indexu, které odpovídají jednomu prefixu
můžeme použít reprezentaci slovníku seřazeným seznamem slov. V něm nalezneme
první a poslední slovo, které odpovídají prefixu. Všechna slova mezi nimi jsou
výsledkem hledání.  Tuto reprezentaci můžeme efektivně komprimovat technikou
Front-Coding, která rozdělí seřazený seznam na bloky o konstantní velikosti
(např. 8) a pro každý blok uloží nejdelší prefix, který sdílí všechna slova
bloku. Technika funguje pro slova přirozeného jazyka, protože díky tendenci
výskytu morfologie ke konci slov se ve slovníku bude objevovat značné množství
silně komprimovatelných úseků jako hody, hodnota, hodnoty, hodný, hodná,
hodnému, atd. Implementace obyčejnou hashovací tabulkou je pro prefixy
nevhodná, protože si neuchovává informaci o lexikografickém pořadí slov.

\subsection{Fuzzy slovník}
U n-gramového přibližného vyhledávání se přibližnost vyhodnocuje až po získání
množiny všech n-gramů v dokumentu. Naopak editační vzdálenost se vyhodnotí už ve
slovníku ještě před přístupem do invertovaných seznamů. Existuje více datových
struktur, které umožňují efektivní vyhledání všech slov do určité vzdálenosti
od hledaného - tedy bez vyčerpávajícího průchodu celým slovníkum.

\subsubsection{Metrické stromy}
Protože editační vzdálenost splňuje trojúhelníkovou nerovnost
\[\bftt{edit}(x,y) \leq \bftt{edit}(x,z) + \bftt{edit}(z,y)\]. lze pro hledání
v prostoru řetězců použít tzv. metrické stromy, které díky ní efektivně
prořezávají prostor s objekty. Burkhard-Kellerův strom
(BK-strom)~\cite{Burkhard:1973:ABF:362003.362025} je metrický strom, který se
používá pro diskrétní metriky, jakou je právě editační vzdálenost.

Představme si, že strom ve výchozím stavu obsahuje pouze jediné slovo (kořen).
Vložení druhého proběhne tak, že se mezi těmito slovy vypočte jejich vzdálenost
\bftt{d} a nové slovo se vloží jako potomek se štítkem \bftt{d}. Další slova se
vloží stejným způsobem, pouze s rozdílem, že když dojde ke kolizi - tedy
potomek se štítkem \bftt{d} už existuje - vloží se nové slovo rekurzivně jako
potomek tohoto slova. Slovník sestavíme obdobným vložením všech slov z datové sady.

Vyhledání začíná porovnáním dotazovaného slova s kořenem stromu. Pokud je
vzdálenost mezi těmito slovy \bftt{d} menší než zvolený limit \bftt{k}, bude
vráceno jako jeden ze zásahů. Vyhledávání pokračuje stejným způsobem rekurzivně
pro všechny potomky tohoto slova, které nesou štítek v rozsahu \bftt{d - k} až
\bftt{d + k}. Důvod pro tento rozsah vyplývá z trojúhelníkové nerovnosti.

Metody využívající rozdělování metrického prostoru jsou pro Levenshteinovu
vzdálenost méně časově i podle paměťové náročnosti výkonné než jiné algoritmy,
které budou představeny
dále~\cite{Celikik:2009:FES:1529282.1529669,Boytsov:2011:IMA:1963190.1963191}.

\subsubsection{Hashovací metoda}
Dvě podobná slova sdílejí netriviální nejdelší společný podřetězec (lcs). Slova
zaindexujeme tak, že do hashovací tabulky vložíme všechny jejich odvozené lcs
podřetězce. Ty pro každé slovo získáme odstraněním všech podmnožin písmen do
požadované velikosti. Např. pokud slovník omezíme pro vyhledávání do
vzdálenosti 2, pak do tabulky vložíme jako klíč všechna odvozená slova, která
vzniknou odstraněním všech $n$ písmen a všech ${n \choose 2}$ párů písmen. Jaho
hodnotu klíče v tabulce použijeme původní slovo. Např. slovo \bftt{ruka}
zaindexujeme odvozenými podřetězci \bftt{uka}, \bftt{rka}, \bftt{rua},
\bftt{ruk}, \bftt{ka}, \bftt{ua}, \bftt{uk}, \bftt{ra}, \bftt{rk}, \bftt{ru}.
Při hledání podobných slov provedeme stejnou operaci s dotazovaným slovem a
zkusíme najít shodu v tabulce pro každý jeho odvozený podřetězec. Každý
nalezený zásah prověříme vypočtením vzdálenosti klasickým tabulkým algoritmem,
protože tahle metoda může přestřelit. Např. $\bftt{edit}(ruka, karu) = 4$,
přestože by došlo k zásahu na podřetězcích \bftt{ru} a \bftt{ka}.

Tato metoda je jednou z nejrychlejších, ale za cenu obrovské náročnosti na
paměť, jelikož je pro maximální strop editační vzdálenosti \bftt{k} každé slovo
zaindexováno $\sum_{i \leq k} {n \choose i}$~krát.

Původní algoritmus  využívající tuto techniku pro překlepy do vzdálenosti 1
(Mor - Fraenkel) byl popsán v~\cite{Mor:1982:HCM:358728.358752}. Zobecnění pro
větší vzdálenosti bylo popsáno nezávisle na sobě jako FastSS~\cite{FastSS},
Boytsov~\cite{Boytsov:2011:IMA:1963190.1963191} a Symmetric
Delete~\cite{Faroo_symmetric_delete}. Několik variant algoritmu je popsáno
v~\cite{FastSS} a~\cite{Bast:2013:EFS:2457465.2457470} popisuje modifikovanou
kompaktnější verzi s názvem DeleteMax, která řeší problém s velikostí.

\subsubsection{Trie}
Populární datovou strukturou, do které lze relativně kompaktně uložit sadu slov
a která podporuje rychlé vyhledávání je trie, také známá jako radixový nebo
prefixový strom. Jedno slovo do trie uložíme tak, že ho rozdělíme na písmena a
každé z nich vložíme jako jeden uzel cesty ve stromu. Např. slovo \bftt{ruka} a
\bftt{ruda} se uloží jako \bftt{/r/u/k/a} a \bftt{/r/u/d/a}, pokud použijeme
notaci pro zápis cest v hierarchii souborového systému. V tomto případě budou v
\uv{adresáři} \bftt{u} dva podadresáře \bftt{k} a \bftt{d}. Trii lze
komprimovat tím, že se sloučíme všechny cesty, které mají pouze jednoho
potomka. Stejný příklad by byl v komprimované trii reprezentován cestami
\bftt{/ru/ka} a \bftt{/ru/da}, kde adresář \bftt{/ru} obsahuje dva podadresáře
\bftt{ka} a \bftt{da}. Úspory paměti je docíleno sdílením prefixů.

Výpočet editační vzdálenosti mezi dvěma slovy proběhne standardně tabulkovou
metodou. U vyhledání všech podobných slov ale proběhne pouze jednou pro řádky
tabulky u všech slov, které odpovídají sdílenému prefixu mezi těmito slovy.
Např. pokud v trii z příkladu hledáme slovo \bftt{luka}, pak můžeme vyhodnotit
editační vzdálenost mezi prefixy \bftt{lu} a \bftt{ru} pro obě slova
\bftt{ruka} a \bftt{ruda} a dokonat zbytek výpočtu pro obě slova zvlášť.

Při průchodu trií jsou jednotlivé větve postupně prořezávány, pokud hodnota
editační vzdálenosti v tabulce překročí pro danou větev stanovenou hranici.
Nejefektivnější je algoritmus v případě, pokud nevyhledáváme slova od prvního
písmene slova. Pokud začíná vyhledání od prvního písemene, musí se otestovat
všechny větve odpovídající prvním písmenům, protože i na nich může dojít k
překlepu. Prohledání stromu od druhého písmene je o poznání efektivnější,
protože už se uvažuje pouze podstrom, který odpovídá stejnému prvnímu písmenu,
které má dotazované slovo. Tento jev může být využit více způsoby. 

Prvním je datová struktura pojmenovaná FB-trie (Forward -
Backward)~\cite{Boytsov:2011:IMA:1963190.1963191}, která vytvoří dvě trie.
První je stejná jako ta výše popsaná a druhá pouze s rozdílem, že zaindexuje
slova v obráceném pořadí. Díky tomu, že prohledávání v trii je výhodné až od
několikátého písmene, využije se druhé trie, která nalezne zbylá slova po
odřezání větví prvního písmene v první trii. Jiný používaný přístup je
penalizovat neshodu na prvních pozicích vyšší vahou.

Druhým způsobem je nalezení slov editační vzdáleností s prefixově dynamickým
prořezáváním. Při omezení limitu pro editační vzdálenost podle délky slova
budeme pro slova delší než 4 znaky prohledávat celou trii do nějaké konstantní
vzdálenosti, např. 2. Pro slova delší než 7 znaků s limitem např. 3. Tenhle
způsob povoluje odlišnosti na začátku i na koncích slov se stejnou penalizací.
Využijeme toho, že v trii procházíme postupně prefixy dotazovaného slova, pro
které stanovíme stejné prořezávací limity, jako kdybychom postupně prohledávali
prefixy jako celá slova. Tedy při délce prefixu 3 budou prořezány všechny větve
s prefixovou editační vzdáleností vyšší než 1, při délce prefixu 5 vyšší než 2,
apod.

Tenhle způsob je omezený pro překlepy na začátcích slov, ale díky vyššímu
prořezávání můžeme stanovit vyšší limity pro delší slova. Efektem bude vyšší
tolerance překlepů na koncích slov. Změny slov, které se vyskytují v nejvyšší
míře na koncích slov v důsledku morfologie, můžeme uvažovat jako překlepy.
Tenhle způsob bude pro tyto případy výpočetně efektivní a můžeme ho použít jako
náhradu stematizace, která je nezávislá na jazyce. Podmínkou je pouze, že
uvažujeme jazyk, který má většinu morfologie na koncích slov, jakým je zejména
čeština.


\subsubsection{Jiné metody}
Kromě několika populárních metod zde popsaných poskytuje
\cite{Boytsov:2011:IMA:1963190.1963191} taxonomii algoritmů zabývající se
obecným problémem přibližného hledání v lexikonu.
\cite{Bast:2013:EFS:2457465.2457470} některé tyto metody rozšiřuje a zabývá se
algoritmy pro prefixové hledání v lexikonu.


% ngram - krátký slova
% permuterm

\subsection{Reprezentace invertovaných seznamů}
Nejjednodušším způsobem, jak uložit invertované seznamy je vytvořit jeden
soubor pro každé vyskytující se slovo. Po zaindexování celé kolekce je poměrně
snadné index upravovat, protože stačí pozměnit jen ty invertované seznamy,
které odpovídají změnám v datech. Současné operační a souborové systémy ale
nejsou přizpůsobené pro miliony malých souborů a navíc by došlo k plýtvání
místem, protože i soubory obsahující pouze jeden znak si uchovávají větší
strukturu s metadaty. Řešením je invertované seznamy uchovat sekvenčně v jednom
velkém souboru. Tenhle způsob je kompromis mezi efektivitou při vyhledávání a
možností modifikovat index. Jeden velký soubor lze prakticky modifikovat pouze
jako celek. Za každým blokem invertovaného seznamu je mozné nechat nějaké volné
místo, a předpokládat, že úpravy budou přicházet rovnoměrně, ale i přesto bude
občas nutné provést změnu celého souboru.

Index může být rozdělen na dva podindexy. Hlavní statický a neefektivní pro
jednotlivé úpravy a vedlejší dynamický, který přijímá změny v reálném čase a
postupně je dávkově slučuje s hlavním. Hlavní index je přizpůsobem pro vysoký
výkon a kompaktnost a vedlejší je přizpůsobem pro zápis. Efektivní způsob, jak
zabránit nákladné rekonstrukci hlavního indexu je použít množství menších
indexů ve vzrůstající velikosti a změny z dynamického indexu postupně s těmito
slučovat od nejmenšího po největší~\citep[kap.~4]{Manning:2008:IIR:1394399}.
Tenhle přístup používá knihovna Apache Lucene~\cite{lsm_lucene}.

\subsubsection{Komprese}
Invertované seznamy jsou pouze seznamy identifikátorů dokumentů - tedy celých
čísel, které bývají uloženy v seřazeném pořadí. To umožňuje efektivní kompresi
tím, že uložíme pouze rozdíly mezi identifikátory (delta komprese), které budou
vždy nezáporné, protože je posloupnost vzrůstající. Tyto rozdíly budou oproti
původním identifikátorům pouze poměrně malá čísla, které můžeme zmenšit
kompresními kódy pro celá čísla.

Komprese invertovaných seznamů má několik opodstatnění. Zmenšením velikosti
indexu docílíme, že větší množství dat může být obslouženo bez jejich hledání
na pomalém persistentním úložišti, ať už je to pevný disk nebo ssd. Pokud už k
zásahu do externího úložiště dojde, pak komprimovaný seznam bude přenesen do
RAM rychleji i přes čas strávený dekódováním. Vysvětlením je úzké hrdlo při
přenosu dat (I/O bottleneck) oproti rychlosti dnešních procesorů. Algoritmy pro
dekompresi mohou být velmi rychlé a tento jev zaznamenáme i na rychlejších ssd
úložištích a dokonce i pokud uvažujeme celou operaci výhradně v paměti
počítače. Dekódování může být díky několika úrovním vyrovnávací paměti
procesoru rychlejší než přístup k nekomprimovaným datům v RAM. 
% TODO cite ssd, memory hierarchy model, atd?

\paragraph{Instantní kódy} uvažují každé číslo samostatně a snaží se
zkomprimovat menší čísla na úkor větších. Příkladem jsou bitově orientované
kódy Elias $\gamma$ a $\delta$ a Golomb/Rice, nebo bytově orientované schéma
VByte. Bitové kódy jsou kompaktnější, ale bytové rychlejší pro dekódování na
současných procesorových architekturách.

\paragraph{Blokové kódy} naopak počítají při kompresi s celými bloky, které se
snaží maximálně zkomprimovat jako celek. Výhodou je vyšší komprese, protože se
využívá charakter distribuce čísel. Nevýhodou je pomalejší přístup, pokud
chceme získat pouze jeden prvek, pak musí být prakticky dekódován celý blok.
Proto se volí kompromis a velikost bloku je omezená pouze na několik stovek až
tisíců prvků. Při sekvenčním přístupu u slučování invertovaných seznamů se ale
využije velká část prvků bloku, proto jsou bloková schémata pro invertované
indexy efektivní. Příkladem jsou např. kompresní rodiny Frame of Reference (bit
packing, FOR, PFor, PForDelta, ...) a v poslední době se rožšiřující velmi
efektivní kódování Elias-Fano~\cite{DBLP:journals/corr/abs-1206-4300}.

\subsubsection{Poloinvertovaný index}
Velkou výpočetní zátěží u prefixových a fuzzy indexů je, pokud musí provést
disjunktivní sloučení (sloučení - OR) většího množství invertovaných seznamů.
Většinou v literatuře potkáme konjunktivní sloučení (průnik - AND), které je
naopak tím efektivnější, čím více invertovaných seznamů v něm učinkuje.
Analogicky ke konjunkci množin - čím více náhodných množin, tím menší je
pravděpodobnost, že bude jejich prvek ve všech najednou. U disjunkce platí, že
čím více množin, tím bude sloučená množina zpravidla větší.

Sloučení invertovaných seznamů je problém, pokud jejich množství narůstá.
Zejména to platí pro krátké prefixy, protože ve slovníku k nim budou
korespondovat tisíce slov. Problém velkého množství slov u krátkých prefixů je
i u hranových n-gramů, jenže u nich jsou invertované seznamy předsloučeny během
indexování. Právě zde přichází myšlenka hybridního indexu, který během
indexování spolu předsloučí ty invertované seznamy, které mají velkou šanci, že
by byly sloučeny během vykonání dotazu (materializace invertovaných seznamů).
Tahle technika je obzvlášť vhodná pro čistě prefixové indexy bez fuzzy
rozšíření. Velkou pravděpodobnost sloučení totiž mají slova, která jsou
abecedně blízko sebe.

Pokud je index rozšířený o pozicovou informaci, pak sloučením všech
invertovaných záznamů získáme zpět původní datovou sadu, pouze zakódovanou
(dopředný index, forward index). Index se nazývá hybridní, protože je hybridem
mezi invertovaným a dopředným indexem.  Hybridní index je velmi podobný
dopřednému indexu v jednom z prvních popisů architektury Googlu. To, co se
myslí skupinou slov v hybridním indexu, odpovídá barelu (barrel) v jejich
indexu. S rozdílem, že oba pojmy slouží k jinému účelu. Jejich dopředný index
byla pouze mezifáze před vytvořením invertovaného indexu, zatímco u hybridního
indexu se tahle datová struktura struktura používá přímo k vyhledávání.

\subsubsection{HYB} je typ hybridního invertovaného indexu, který během
indexování předsloučí invertované seznamy slov v abecedním rozsahu.
Materializované seznamy mohou mít rozsahy např. \bftt{[A-EN], [EN-NUL],
[NUL-QU], [QU-Z]} - první invertovaný seznam by obsahoval slova
začínající na \bftt{A} až \bftt{EN}. Disjunktivní dotaz v téhle
struktuře je už buďto předsloučený, nebo během vykonání dotazu za běhu
sloučí pouze malé množství sousedících materializovaných seznamů. Ke
každému záznamu ve sloučeném seznamu musí být poznamenáno, ke kterému
slovu patří. Dotazy, které nevyužijí celý rozsah seznamu se díky této
dodatečné informaci vyfiltrují od slov, které zúženému rozsahu
neodpovídají. cite completesearch.

Tím, že jsou některé seznamy sloučeny do většího, budou dotazy, které by těchto
seznamů využily, penalizovány. Naopak komplexní disjunktivní dotazy budou
efektivnější než bez materializace. V klasickém invertovaném indexu je kvůli
zipfově distribuci velký nepoměr náročnosti při zpracování krátkých a dlouhých
seznamů. Hybridní index tenhle rozdíl srovnává za cenu, že původně efektivní
dotazy budou běžet zhruba stejně dlouho, jako ty původně náročné.

Bylo by možné materializovat všechny prefixy, které se ve slovníku budou
nacházet a prefixové dotazy by díky odpadnutí disjunkcí byly velmi efektivní,
ale u fuzzy dotazů nelze dopředu předpovědět, které invertované seznamy v nich
budou figurovat, protože závisí na dotazu. Tohle schéma by navíc bylo velmi
paměťově náročné. HYB oproti tomu, až na nutnost uložení identifikátoru slova
ke každému výskytu dokumentu, zabírá v paměti stejně místa jako obyčejný
invertovaný index

\subsubsection{Bitmapy}
Protože představuje jeden invertovaný seznam množinu všech dokumentů, ve
kterých se vyskytuje jedno slovo, existuje jiná technika pro uložení této
množiny - bitmapa. Bitmapové kódování množiny použije jeden kladný bit na
pozici dané identifikátorem dokumentu, pokud se slovo v seznamu vyskytuje, a
záporný, pokud nikoliv. Bitmapy poskytují velmi rychlé algoritmy přesně pro ty
množinové operace, se kterými invertovaný index operuje - tedy konjunktivní
(AND) a disjunktivní (OR) sloučení. Pro jejich vykonání existují extrémně
rychlé bitově-paralelní operace zabudované přímo v instrukčních sadách
procesorů.

Tato reprezentace množiny poskytuje vysokou kompresi pro množiny s vysokou
hustotou. Velikost reprezentace je přesně $\bftt{n}\ /\ 8$ bytů, kde \bftt{n}
je počet dokumentů. Bohužel kvůli zipfově distribuci slov je většina
invertovaných seznamů velmi řídká, a proto nejsou bitmapy pro invertované
indexy používány. Navíc řeší pouze množinový problém, jestli dokument obsahuje
dané slovo, ale neřeší dodatečné informace o multiciplitě a pozici výskytů.

Bitmapy ukládají pouze množinovou informaci o existenci slova v dokumentu, ale
takto jednoduše v nich nelze uložit pozicovou informaci. To z nich dělá
nevhodného kandidáta pro potřeby vyhledávače, který ohodnocuje výsledky podle
blízkosti slov a zvýrazňuje výsledky na základě právě pozicové informace.

U hybridního indexu se sloučenými seznamy je hustota množiny vyšší, ale přesto
z experimentálního měření není dostatečná, aby zdůvodnila použití bitmap. Pro
komprimované bitmapy by se snížila účinnost jednoduchých bitových operací a
vzájemně by si mohla překážet s relativně vysokou hustotou u hybridního
seznamu, protože kompresní schémata pro bitmapy (např. WAH, EWAH, Concise nebo
Roaring~\cite{DBLP:journals/corr/LemireKK16}) jsou stavěné pro řídké množiny.

% popiš kompresi bitmapy tabulky s hustotou

\subsubsection{Jiné přístupy}
Invertované indexy jsou použitelné zejména pro lidský jazyk, který odpovídá
vlastnostem zipfova a heapsa zákona. Text je modifikován odstraněním
neabecedních znaků a rozdělením na slova. V důsledku odstranění některých částí
textu není možné text z indexu zpětně rekonstruovat. 

Jiné přístupy, které jsou vhodné i pro text neodpovídající lidské řeči (např.
biologické sekvence) jsou založené na suffixových stromech, suffixových polích
nebo waveletových stromech.


Kromě invertovaného indexu existují i alternativní datové struktury umožňující
rychlé vykonání dotazu a v některých případech i kompaktnější index. V poslední
době se rozšířila popularita tzv. stručných (succint) datových struktur, které
uchovávají data velmi kompaktně za použití bitových polí a bitových algoritmů.
Takovou datovou strukturou je např. waveletový
strom~\cite{Grossi:2003:HET:644108.644250} který našel uplatnění i jako
reprezentace invertovaného
indexu~\cite{Ferragina:2007:CRS:1240233.1240243,FERRAGINA2009849}. Tento
přístup je poměrně nový a neprobádaný a má proti klasickému invertovanému řadu
výhod (kompaktnost, možnost více seřazení~\cite{Navarro2010}), ale ten stále
zůstává dle rychlosti vykonání dotazu nepokořen.  Důvodem je zejména jeho
uspořádání v paměti umožňující téměř perfektní sekvenční přístup, který
současné hardwarové architektury silně zvýhodňují. AutoTree~\cite{Weber2007} je
obdoba waveletového stromu s aplikací pro prefixové dotazy.

% TODO

\section{Algoritmy pro sloučení invertových seznamů}
První část vykonání dotazu, prohledání slovníku, a poslední část, ohodnocení a
seřazení výsledků, by měly být relativně rychlé. Slovník odpovídá pro lidské
jazyky heapsově zákonu, a proto je oproti velikosti všech dat relativně malý.
Stejně tak počet výsledků by v průměrném případě měl být malý. Získání a
sloučení invertovaných seznamů by proto mělo dominovat celkovému času doby
vykonání dotazu. Ty totiž tvoří většinu uložených dat indexu.

Konstantním výzkumem na poli vyhledávačů jsou algoritmy pro jejich efektivní
slučování zejména v konjunktivní formě (průnik, intersection). V této práci nás
bude zajímat i disjunktivní forma sloučení (sjednocení, union, merge), která má
velkou roli v prefixových a fuzzy variantách indexu. 

Invertovanými seznamy se rozumí vzrůstající posloupnosti identifikátorů
dokumentů. Ty mohou být obohaceny o pozicovou informaci a ohodnocení, ale pro
jednoduchost zed uvažujeme pouze dokumentové identifikátory.

\subsection{Strategie sloučení}
Existují dvě hlavní strategie, jak seznamy sloučit. První z nich sloučení slovo
po slovu (term-at-a-time, \bftt{taat}). Ta nejprve získá invertované seznamy každého
slova a postupně buduje výsledný seznam sloučením mezivýsledku a dalšího
seznamu v pořadí. Např. v konjunktivním sloučení nejprve zpracuje první dva
seznamy do jejich průniku a tenhle mezivýsledek postupně slučuje se zbylými. V
konjunktivním sloučení se vyplatí seřadit seznamy podle velikosti od nejmenšího
po nejdelší, protože průnik množin je nejefektivnější u sloučení s co nejmenší
množinou.

Druhou strategií prochází všechny slučované seznamy naráz jeden dokumentový
identifikátor po druhém (document-at-a-time, \bftt{daat}). I zde platí, že konjunkce
je nejefektivnější po předseřazení seznamů podle velikosti. Oproti prvnímu má
tenhle způsob výhodu, že nemusí materializovat jednotlivé disjunkce, ale tvoří
je za běhu.

\subsubsection{Konjunkce}
Nejjednodušším způsobem vykonání průniku je lineární průchod seznamy a postupným
zaznamenáváním prvku, který se vyskytuje ve všech seznamech - algoritmus
\bftt{zipper}. V situaci, kdy slučujeme dva seznamy - jeden velmi krátký oproti
druhému - je efektivní použít nějaké přeskakovací schéma v delším seznamu,
protože většina dokumentů delšího seznamu nemusí být brána v úvahu. Algoritmy s
přeskakováním jsou např. binární nebo exponenciální \bftt{galloping}
vyhledávání ve zbytku delšího seznamu, nebo použití přeskakovacích tabulek
(\bftt{skip list}, \bftt{skipper}, \bftt{lookup}).
\cite{Sanders:2007:III:2791188.2791195} poskytuje experimentální měření
efektivity těchto algoritmů i při použití delta komprese.

\subsubsection{Disjunkce}
Disjunktivní sloučení je stejný problém jako sloučení podseznamů v řadícím
algoritmu mergesort. Ten se řeší postupným vybíráním minimálního prvku z $k$
seznamů pomocí prioritní fronty (k-way merge) s náročností $\mathcal{O}(n \log
k)$ při použití binární haldy. Díky tomu je disjunktivní sloučení formou taat i
daat algoritmicky totožné, ale daat má výhodu možnosti překakování. Při dotazu
typu \uv{konjunkce disjunkcí} totiž můžeme vynechat velké množství operací
disjunkce, protože v postupující konjunkci průběžně zjišťujeme, které dokumenty
mohou být přeskočeny.

Existuje několik možností pro zefektivnění. Prvním je povšimnutí si, že některé
seznamy budou vyčerpány dříve než ostatní, a proto je můžeme z binární haldy
odstranit předčasně. Druhá modifikace je nepoužití binární haldy pro prioritní
frontu, ale obyčejného seznamu, pokud používáme hybridní index s malým počtem
seznamů ke sloučení. Konstantní faktory vytahování a vkládání prvku binární
haldy mohou být jednoduchým seznamem sníženy. Oproti slučování v mergesortu je
ve slučování konjunkce disjunkcí rozdíl v neustálém přeskakování prvků. Pro
disjunkci znamená jedna přeskakující operace vytažení všech prvků z prioritní
fronty, provedení jejich posunutí a vložení zpět do fronty. Disjunkci můžeme
více přizpůsobit přeskakování oproti posunutím po jednom dokumentu tak, že
seznamy seřadíme od největšího po nejmenší a přeskočení ukončíme v momentu, kdy
alespoň jeden seznam vrátí prvek, na který přeskakujeme. Pouze si uchováme
informaci, že později musíme dorovnat zbytek seznamů k témuž dokumentu. Seznamy
seřadíme od největšího, protože očekáváme větší šanci výskytu dokumentu v
delším seznamu, čímž zvýšíme šanci předčasného ukončení operace. Tyto metody
jsou experimentálně vyzkoušeny v \S\,\ref{sec:rychlost_vyhledavani}.



%TODO 
% wand, mwand

\section{Ohodnocovací funkce}
Poslední fáze vykonání dotazu je ohodnocení všech nalezených výsledků, jejich
seřazení a zvýraznění výskytů v textu (highlighting, snippet generation). V
téhle fázi je důležité se rozmyslet, jestli je nutné zpracovávat všechny
výsledky, nebo pouze \bftt{k} nejlépe ohodnocených (top-k). Zpracování všech
znamená seřazení celé výsledné množiny v čase $\mathcal{O}(n \log n)$, oproti
částečnému zpracování, kdy můžeme použít pouze několik iterací heapsortu v
$\mathcal{O}(n + k \log n)$ nebo algoritmus pro částečné seřazení quickselect v
očekávané složitosti $\mathcal{O}(n + k \log k)$.

\subsection{tf.idf}
U dlouhého textu můžeme použít statistiky počtu slov k určení jeho relevance k
nějakému slovu. Na tomhle principu stojí většina ohodnocovacích metod, které
pracují s dlouhým textem.  \[\bftt{tf.idf}_{t, d} = (1 + \log \bftt{tf}_{t, d})
\times \log_{10} N / \bftt{df}_{t}\]

\bftt{tf} (term frequency) je počet výskytů slova v daném dokumentu. \bftt{df}
(document frequency) je počet výskytů slova v celé textové kolekci. \bftt{idf}
(inverse document frequency) je převrácená hodnota \bftt{df}.

\subsection{Předdefinované pořadí}

\subsection{Grafová analýza}
% page rank, centrality measures

\subsection{Blízkost}
Statistiky jako počet slov v textu ztrácí v krátkém textu význam. Film
nebude více zelený, pokud se v jeho názvu bude vícekrát vyskytovat slovo
\bftt{zelený} (cite algolia). V krátkém textu by byly nadměrně hodnoceny
delší texty, které slovo obsahují vícekrát, protože většina textů v
kolekci bude slovo obsahovat jenom jednou.

\subsection{Problémy ohodnocovacích funkcí}
Už od počátků velkých webových vyhledávačů v 90. letech se čistě textové
statistické metody založené na tf.idf zneužívaly, protože každý mohl
přizpůsobit text na své webové stránce, aby se lépe přizpůsobila
jednoduchým statistikám.

Stačí stránku zahltit co největším počtem různých slov (Keyword
stuffing), aby byla zvýšená pravděpodobnost, že ji vyhledávač uzná
relevantní při náhodném dotazu. Pro ještě větší úspěšnost lze důležitá
slova na stránce duplikovat pro zvýšení parametru \bftt{tf}.

% TODO Fuzzy a idf! článek od elasticsearch
% SEO - exploit of google's algorithms. Více vyhledávačů ztěžuje SEO

\subsubsection{Ohodnocení výsledků dotazu}
U dokumentů běžně uvažujeme náhodné pořadí, ale lze ho doupravit nějakým
preferenčním pořadím (podle času, podle ceny, počtu zhlédnutí, apod.). Přesto
by měla mít textová shoda největší význam, jinak by mohl mít uživatel pocit, že
si buď vyhledávač vymýšlí, má nízkou přesnost, nebo se mu někdo snaží něco
vnutit (např. upřednostněné produkty v eshopu).

Výpočetně by bylo výhodnější, pokud by se uvažovalo pouze předdefinované
pořadí, protože by bylo možné ho předpočítat během indexování a invertovaný
index seřadit podle něj. Případně ho uložit ve více pořadích za cenu více
potřebného úložného prostoru.

Předdefinované pořadí navíc ulehčuje předčasné ukončení vykonání dotazu (early
termination), protože nejdůležitější dokumenty budou nalezeny hned na začátku
invertovaných seznamů.

\subsubsection{Zvolení vah}
Celkové ohodnocení shody se vypočte jako součet všech dílčích faktorů
ohodnocených různými vahami. Důležitost textové shody by měla v první řadě být
určena vzdáleností výsledku od dotazu podle editační vzdálenosti. V druhé řadě
by to měla být minimální vzdálenost mezi slovy ve výsledku.  Tohle pořadí pak
může doupravit uživatelem zvolené pořadí, ale mělo by mít menší váhu než
textová shoda.

\subsubsection{Tie-breaking}
Jiný způsob výpočtu důležitosti používá implementace vyhledávače algolia
(tie-breaking). V něm se určí pořadí jednotlivých faktorů podle
důležitosti a nižší faktory se uvažují pouze pokud dojde ke shodě ve
vyšších faktorech. Např.  pokud je vzdálenost několika prvních výsledků
dotazu od dotazu 0 - tedy přesná shoda, pak teprve se přistoupí k
ohodnocení podle předdefinovaného pořadí, blízkosti, nebo jiného
faktoru. (cite algolia blog post).

\subsubsection{Blízkost}
Pro výpočet blízkosti slov dotazu ve výsledku jsem zvolil variaci na algoritmus
(plane sweep proximity match).



\chapter{Implementace}
%\subsection{Datové struktury}
%Po uvážení běžně používaných a alternativních datových struktur pro index jsem
%zvolil klasický invertovaný index s hybridní materializací. Pro slovník jsem
%použil prefixovou trii, díky její podpoře pro efektivní prefixové i fuzzy
%vyhledání slov. Invertovaný index je hybridní, protože se díky jeho
%předsloučeným invertovaným seznamům hodí právě na prefixové a fuzzy dotazy,
%které jsou klíčovou vlastností mého návrhu.
%
%Vzdálenostní funkcí pro fuzzy vyhledávání ve slovníku jsem použil vzdálenostně
%adaptovanou Levenshteinovu metriku kvůli lepšímu výkonu za cenu o něcoméně
%přesných výsledků při překlepech na začátcích slov. Ve výsledku to není
%problém, protože většina odlišností je až na konci slov kvůli povaze
%morfologie.
%
%Uvážil jsem i alternativní datové struktury pro index. Waveletové stromy jsou
%vhodné pro velmi kompaktní uložení indexu a navíc mohou podporovat více pořadí
%bez nutnosti ukládat index vícekrát. (cite dualsorted index). Protože se
%soustředím na menší objemy dat, není kompaktnost až tak důležitá. Za druhé jsem
%nenašel způsob, jak v něm uložit hybridní index, který má pro mé účely velkou
%výhodu.

V této kapitole budou popsány konkrétní detaily implementací porovnaných
systémů. V první sekci bude rozebrána implementace prototypu navrhovaného
systému a v pozdějších částech stručně popsány konfigurace v systémech
ElasticSearch a Postgres.

\section{Navrhovaný systém}
Rozložení skupin invertovaným seznamů u rozložení HYB
obecně nerespektuje abecední hranice. Důvodem je ten, že sloučené skupiny
invertovaných seznamů by měly být ideálně stejně dlouhé. Není to ale
jednoduché, protože se různá písmena objevují různě často. Např. obecně bude
existovat více slov začínající na \bftt{T} než těch, které začínají na
\bftt{x}. Kdyby se měla každá skupina shodovat alespoň v prvním písmenu, bylo
by to v rozporu s požadavkem na stejně dlouhé skupiny. Teoretickým řešením by
mohlo být předefinování abecedního pořadí dle frekvenční analýzy. Nejpočetnější
skupiny by mohly být rozděleny do více podskupin (např. \bftt{[TA-TD]},
\bftt{[TD-TM]}, \bftt{[TM-TZ]}), pro středně početné by existovala právě jedna
skupina a méně početné by byly sloučeny dohromady (např. \bftt{[U-X]},
\bftt{[X-Z]}). Kdyby se nepoužilo frekvenční pořadí, mohla by se mezi méně
početnými skupinami vyskytovat vysoce početná skupina zabraňující sloučení těch
okolních.

Tím, že používám vzdálenostně adaptovanou editační vzdálenost a většina slov
nalezených ve fuzzy dotazu bude mít podobný prefix, docházelo by k tomu, že
velká část seznamu bude nevyužita, protože se liší v prvním znaku. Rozmezí slov
u jedné skupiny může být např. \bftt{[J-M]} a dotaz by začínal na \bftt{M}. Slova v
seznamu začínající na \bftt{J}, \bftt{K} nebo \bftt{L} by v tomhle případě byla
nevyužita. Má úprava rozložení skupin oproti abecednímu rozložení u struktury
HYB se snaží zajistit, aby byly abecední hranice respektovány co nejvíce.
Výsledkem by měly být skupiny, jejichž hraniční slova sdílejí co nejdelší
prefixy.

\subsubsection{Algoritmus pro rozdělení skupin respektující abecední hranice}
Zařazení slova do slučující skupiny lze provést při prohledání trie do hloubky.
Začínáme na kořenovém uzlu s tím, že uvažujeme jedinou skupinu, do které na
začátku všechna slova patří. Pokud při průchodu součet délek invertovaných
seznamů poduzlů jednoho uzlu překročí stanovenou hranici, pak je dosavadní
přiřazená skupina uzlu nahrazena skupinou novou. Rekurzivní aplikací pro
všechny uzly se původně jedna skupina rozpadne na více skupin. Jediným
parametrem je hranice určující rozpadnutí skupiny, která může být staticky
daná, nebo být odvozená od dat. Zatím jsem ideální způsob, jak ji stanovit.

\subsection{Indexace}
Metody pro indexování u klasických invertovaných indexů se snaží vyhnout se
více průchody vstupní datovou sadou. Čtení vstupu z disku, parsování a
převádění na identifikátory je při velkých datových objemech časově náročné.
Rozložení do skupin u hybridního indexu - HYB i mé rozložení - vyžaduje
dvoufázové indexování, protože rozdělení do skupin potřebuje vědět statistiky,
ze kterých se určí, jak mají být sloučené skupiny velké.

V~\cite{Bast:2011:FCH:1993036.1993040} autoři popisují techniku, jak použít jen
několik vzorků, aby s vysokou pravděpodobností určili velikost skupin. (cite a
nastin alg.)

Hybridní index se od klasického liší v počtu invertovaných seznamů. Klasický má
jeden seznam, byť potenciálně velmi krátký, pro každé vyskytující se slovo v
kolekci. Počet slov se odvíjí od heapsova zákona - tedy roste přibližně s
odmocninou velikosti kolekce - a pro velké kolekce je následně velký počet slov
důvodem indexačních technik pracujících s externí pamětí. Kvůli
charakteristikám pevných disků (pomalé posunutí čtecí hlavice při nesekvenčním
přístupu) a problému mít otevřených tisíce souborů najednou spočívají klasické
indexační algoritmy v tom, že zapisují částečné indexy sekvenčně na konec
jednoho souboru, a poté (nebo ještě za běhu) je postupně sloučí do většího.

% TODO describe block merge sort based

Hybridní index ale bude mít pouze omezené množství invertovaných seznamů, které
lze navíc korigovat. Autoři (cite completesearch) doporučují celkový počet
seznamů v řádu stovek - okolo 256. Díky tomu by nemělo být takovým problémem
vytvořit pro každý invertovaný seznam jeden soubor a vyhnout se slučovacímu
kroku.

\subsection{Problém korelovaných slov}
Jednou příčinou pomalých dotazů ve fuzzy vyhledávači je výskyt více podobných
slov v dotazu. Jednoduchým případem pro ilustraci je, pokud dotaz obsahuje více
totožných slov. Jejich invertované seznamy, případně disjunkce více
invertovaných seznamů, budou totožné. Dotaz ale proběhne beze změn, a tedy
provede jejich sloučení. Je snadno vidět, že sloučení dvou totožných seznamů
bude odpovídat dvojnásobku času průchodu pouze jedním seznamem.

O dvou podobných invertovaných seznamech můžeme hovořit jako o vysoce
korelovaných. Pokud slučujeme dva nízce korelované seznamy, pak jejich sloučení
bude relativně malá množina. Vysoce korelované dotazy trpí tím, že pozitivní
zmenšující efekt konjunkce u nich neplatí a navíc zvyšuje výpočetní náročnost
lineárně s každým takovým slovem.

Jednoduchým případem pro vyřešení je, pokud lze v dotazu identifikovat totožná
slova a jednoduše použít jen jedno z nich, protože průnik více stejných množin
neovlivní výsledek. Pouze se musí zajistit, že výsledky skutečně obsahují více
výskytů téhož slova, a to lze provést až v pozdější fázi po průchodem
invertovanými seznamy.

Těžším případem je, pokud se v dotazu vyskytují slova, která k sobě mají podle
editační vzdálenosti blízko. Jejich invertované seznamy, případně disjunkce
více seznamů, budou sdílet značnou část slov. Kvůli tomu budou jejich
invertované seznamy vysoce korelované a budou způsobovat degradaci dotazu.
Nicméně tohle už nelze řešit pouhým přepsáním dotazu odstraněním duplikovaných
slov.

Dalším problémem korelovaných slov je najít každému slovu v dotazu odpovídající
slovo ve výsledku bez překrytí, tj. pro každé slovo v dotazu, přestože může být
duplikované, musí být nalezen výskyt ve výsledném dokumentu, který ale nesmí
připadat jinému slovu z dotazu. Např. naivní technika deduplikování totožných
slov by způsobila, že dotaz \bftt{the the} by nalezl všechna slova, která
obsahují pouze jeden výskyt slova \bftt{the}. Nás ale budou zajímat výsledky,
které obsahují alespoň dva výskyty. Proto jsme v dotazu druhé \bftt{the} přece
použili.

Řešení není úplně snadné, protože algoritmicky odpovídá nalezení maximálního
párování v bipartitním grafu. Pro svou implementaci používám pouze jednoduchou
heuristiku, která zajistí, že neodfiltrovaný výsledek obsahuje alespoň tolik
výskytů, kolik má dotaz celkově slov. To zabrání případům, kdy více podobných
slov v dotazu zasáhne jediné vyskytující se slovo ve výsledku. Např. dotaz
\bftt{2015 - 2016} by nesprávně vrátil dokument \bftt{Grand Tour 2015}, protože
obě slova dotazu zasáhnou jediný výskyt \bftt{2015}. Tím, že nastolíme
požadavek, že výsledek musí mít alespoň tolik výsledků (zde 1), kolik je slov v
dotazu (zde 2), bude těmto primitivním případům zabráněno.



\section{Konfigurace v ElasticSearch}
Pro systém ElasticSearch byly nakonfigurovány dva systémy.

\subsection{Základní řešení}
Jako základní řešení sloužící pro porovnání jsem nakonfiguroval Elasticsearch
tak, jak by se měl doporučeně nastavit pro full-textové vyhledávání ve větších
textových dokumentech. Text je stematizován zabudovaným stematizérem pro
češtinu. Cílem bude ukázat, že tohle nastavení bude mít potíže s vícejazykovou
datovou sadou, jmény a s některými případy morfologie v češtině.

\subsection{Trigramový vyhledávač}
Pro ukázku fuzzy vyhledávání pomocí n-gramů jsem vytvořil další konfiguraci pro
Elasticsearch, která používá n-gramy o minimální a maximální velikosti 3.

Pokud se v konfiguraci použije n-gramový filtr, pak není podporováno n-gramové
zvýrazňování výsledků. Musí se použít n-gramový tokenizér, který ale zahazuje
všechny n-gramy, které jsou kratší než minimální délka n-gramu. Řešením je
vytvořit dodatečný index se zahozenými krátkými slovy a při dotazování provést
disjunkci n-gramového a tohoto indexu. Pro zpřesnění výsledků, pokud dojde k
přesné shodě slova v dotazu a v dokumentu, je do disjunkce přidán ještě třetí
index s původními neořezanými slovy.

Konfigurace dotazu je navržena tak, aby došlo k zásahu, pokud dokument obsahuje
alespoň $70\%$ trigramů dotazu (odpovídá Jaccardově vzdálenosti $0.3$). Vyšší
hodnoty začaly rychle ztrácet výsledky a nižší hodnoty vracely příliš
nesouvisejících záznamů.

Dotaz se snaží shodu najít napříč třemi indexy. \bftt{shorts} jsou ngramy
kratší než $3$, protože je ElasticSearch z nějakého důvodu při indexování
zahazuje. Tohle chování nelze přenastavita doporučeným řešením je právě
vytvořit dodatečný index pro zahozené ngramy. Pro zvýšení přesnosti byl
vytvořen ještě třetí index, který obsahuje slova v původním tvaru. Dotaz je
proveden zároveň ve všech třech indexech metodou \bftt{most\_fields}.

Zvýrazňovač pro n-gramy funguje, pouze pokud se použije \textit{Fast Vector
Highlighter}. Přestože je zvýrazňování nastaveno doporučeným způsobem pro tento
typ dotazu, dochází k chybám, když se n-gramy ve výsledku překrývají. Je možné,
že je chyba pouze ve verzi Elasticsearch, kterou používám. Protože jsem nikde
nenašel, jak tohle vyřešit, vytvořil jsem v Pythonu dodatečný zvýrazňovač, aby
zde prezentované výsledky byly srovnatelné s ostatními systémy.

\section{Konfigurace v Postgres}


\chapter{Analýza}
V této části budou nejprve shrnuty vlastnosti alternativních implementovaných
vyhledávacích systémů a datové soubory použité pro experimentální analýzu.
První část analýzy je porovnání vlastností implementovaného systému s
alternativními. Druhá část obsahuje ukázku a shrnutí nonfunkčních vlastností
(rychlost, paměťová náročnost) systémů.

%\section{Implementované systémy}
%Pro demonstraci uvedených technik jsem vytvořil tři vyhledávací systémy. První
%z nich je má implementace hybridního indexu a zbylé dva jsou různé konfigurace
%pro Elasticsearch sloužící k porovnání. Všechny porovnávané systémy odstraňují
%diakritiku a převádějí velká písmena na malá při indexování i dotazování.
%Všechny konfigurace jsou v příloze \ref{appendix:search_config}.

\subsection{Vlastní implementace}
Můj systém vychází z výše popsaného návrhu vyhledávacího systému. Implementace
je pouze prototyp napsaný v dynamicky typovaném Pythonu, a tedy nemůže z
výkonostního důvodu sloužit k porovnání rychlosti a jiných výkonnostních
veličin. Protože ale používám relativně malé datové sady, neměla by být
výkonnost překážkou. Prototyp slouží zejména jako demonstrace implementovaných
technik. Tou hlavní je výše popsaný modifikovaný hybridní invertovaný index se
slovníkem implementovaným jako trie.  Ohodnocovací funkce je čistě textově
založená - nemá o datech žádné vlastní předpoklady nebo přednostní pořadí. V
první řadě upřednostňuje shodu dotazu s výsledkem podle editační vzdálenosti -
pro každé slovo zvlášť. Dále pozitivně hodnotí ty výsledky, které obsahují
shodu s minimální poziční vzdáleností.  Shody výsledků se stejným ohodnocením
jsou pak rozbity několika minoritními hodnotami s nízkou váhou. Shoda, která je
více na začátku záznamu má přednost a shoda, která obsahuje více shodujících se
slov má přednost - obdobně jako u modelu tf.idf, pouze zde má velmi malou váhu
a neuvažuje relativní důležitost slova ve shodě (idf).

Dokonce při použití nového interpreteru pro Python s podporou JIT
kompilace PyPy vyhledávač dosahuje až srovnatelných rychlostí jako
Elasticsearch.


\section{Použité datové soubory}
Cílem je porovnat několik vyhledávačů na datových sadách s relativně krátkými
dokumenty, které mají navíc různé záludnosti jako více jazyků, nebo slova
obtížně zpracovatelná jazykovou analýzou (jména nebo odborné pojmy).

\subsubsection{ČSFD - filmy}
První datovou sadou (\bftt{csfd\_filmy}) jsou názvy filmů z Československé
filmové databáze (\url{http://csfd.cz}). Přestože je možné rozlišit jazyk
filmu, protože databáze tuto metainformaci obsahuje, sloučil jsem všechny názvy
filmů do jedné sady. V datové sadě se tedy film vyskytuje vícekrát, pokud má
více názvů.

V datovém souboru je celkem \textbf{398\,430} filmů a k nim jsou přidány
alternativní názvy v jiných jazycích, kterých je dohromady dalších
\textbf{141\,722}.

\subsubsection{ČSFD - filmoví tvůrci}
Druhá datová sada (\bftt{csfd\_tvůrci}) jsou všichni tvůrci (herci, režiséři,
scénáristé, apod.) pocházejících rovněž z databáze ČSFD. Jsou uvažováni jen ti
tvůrci, kteří se podle dat podíleli na filmech z databáze \bftt{csfd\_filmy}.
Nejsou uvažováni tvůrci, kteří nejsou přiřazeni k žádnému filmu.

\subsubsection{Česká wikipedia - nadpisy článků}
Data z wikipedie lze stáhnout z \url{https://dumps.wikimedia.org/cswiki/}. Pro
nadpisy byl použit soubor \textbf{cswiki-20160111-all-titles.gz}, který
obsahuje \textbf{907\,094} titulků článků (\bftt{cswiki\_titulky}).


\mbox{}
\begin{tt}
\begin{table}[H]
\centering
\begin{tabulary}{\textwidth}{LRRRR}
\textbf{data} & \textbf{velikost} & \textbf{\# záznamů} & \textbf{\# slov} & \textbf{\# unikátních slov} \\
\hline
csfd\_filmy     & 11.2\,MB & 540\,152 & 1\,958\,147 & 231\,452 \\
csfd\_tvůrci    & 3.6\,MB  & 240\,659 & 523\,544    & 124\,780 \\
cswiki\_titulky & 17.1\,MB & 907\,094 & 2\,987\,750 & 339\,091 \\
\hline
\end{tabulary}
\caption{Statistiky datových sad}
\label{tab:data_stats}
\end{table}
\end{tt}

\section{Porovnání výsledků vyhledávání}
Všechny analyzované vyhledávače podle očekávání vrátí výsledek, pokud se v
jednotlivých slovech přesně shoduje s dotazem. Některé, zejména
\bftt{trigram\_ES}, vrací za cenu vyšší přesnosti více výsledků, aby se zvýšila
šance, že žádný relevantní výsledek nebude vynechán.  V následujících
výsledkových tabulkách je zobrazeno několik prvních výsledků vyhledávání pro
každý vyhledávač s uvedeným počtem celkových nalezených výsledků. Vedle každého
výsledku je uvedeno skóre, který daný vyhledávač přiřadil. Vyšší skóre zde
neznamená větší shodu, ale naopak označuje míru penalizace od perfektní shody.
Tedy menší skóre znamená přesnější shodu. Skóre nelze napříč vyhledávači
porovnávat (s výjimkou \bftt{edit} a \bftt{edit\_dyn}). Slouží jen pro
relativní porovnání shod v rámci jednoho vyhledávače.

\mbox{}\input{hits/forrest_gump.tex}\mbox{}

První příklad (tab.~\ref{tab:result:forrest_gump}) je dotaz, ve kterém hledáme
zahraniční film \bftt{Forrest Gump}, jehož jméno známe a umíme ho správně
napsat.

\section{Přesnost}
\subsection{Filmy}
Několik následujících příkladů demonstruje záměrně zkomolené nebo jinak
nepřesné dotazy, od kterých ale beztak očekáváme přesný zásah.
Vyhledávače by neměly vracet příliš mnoho výsledků, protože tyto dotazy
budou relativně dlouhé a specifické, které by v omezených datových
sadách měly obsahovat jen několik shodujících se záznamů.

\mbox{}\input{hits/smrt_krasneho_srnce.tex}\mbox{}

Tab.~\ref{tab:result:smrt_krasneho_srnce} ukazuje schopnost vyhledávačů
vypořádat se s dotazem v jiném pádu a číslu, než je zamýšlený film. K
povšimnutí stojí rozdíl mezi vyhledávačem \bftt{edit} a jeho dynamickou
variantou \bftt{edit\_dyn}. Dynamická editovací vzdálenost má větší toleranci
pro odlišnosti na koncích slov, proto se dokázala vypořádat s editovací
vzdáleností \bftt{3} u rozdílu mezi \bftt{krásného} a \bftt{krásných}.
\bftt{edit} používá vzdálenost \bftt{3} až od délky slova \bftt{9}, jinak by
došlo k obrovskému nárůstu podobných slov.

\bftt{cs\_ES} nenašel žádnou shodu, přestože by jeho český stematizátor měl
tenhle případ zvládnout. Tím, že se jazyková analýza dotazu
(\bftt{smrt~krasnh~srnk}) neshoduje s analýzou výsledku
(\bftt{smrt~krasnych~srnk}), nedojde ke shodě. Vinou zde bude pouze
nedostatečný základní stematizér v ElasticSearch.

\mbox{}\input{hits/sindleruv_seznam.tex}\mbox{}

Protože dynamická editovací vzdálenost upřednostňuje odlišnosti na
koncích slov oproti těm na začátcích, nenajde \bftt{edit\_dyn} shodu
filmu \bftt{Schindlerův seznam} při zkomoleném dotazu \bftt{Šindlerův
seznam}, který odpovídá fonetické reprezentaci - \bftt{Schi} je
nahrazeno českým \bftt{Ši} (tab.~\ref{tab:result:sindleruv_seznam}).

\mbox{}\input{hits/veznice_showsank.tex}\mbox{}

Na dotaz jiný zkomolený dotaz \bftt{Věznice Showsank}
(tab.~\ref{tab:result:veznice_showsank}, který by měl nalézt
\bftt{Vykoupení z věznice Shawshank} kladně reagují pouze \bftt{edit}
vyhledávače, protože cizí a specifický název \bftt{Shawshank} neprojde
jazykovou analýzou a trigramový systém selže, protože zasáhne pouze jeden
trigram \bftt{ank}.

Zvýrazňovač výsledků nebere v úvahu odstraněná písmena. Místo toho použije
délku dotazovaného slova nebo nejbližší mezeru, pokud je slovo delší. Kvůli
tomu není poslední písmeno slova \bftt{Shawshank} zvýrazněno, přičemž by mělo
být.

\mbox{}\input{hits/vezeni_showsank.tex}\mbox{}

Při zkomolení druhého slova už reaguje pouze \bftt{edit\_dyn}
(tab.~\ref{tab:result:vezeni_showsank}), protože ke zkomolení došlo na
konci slova.

\mbox{}\input{hits/prelet_kukacka.tex}\mbox{}

\begin{table}[H]
\begin{tt}
\horizlina

\bftt{edit\_dyn} [2 nalezeno]\vspace{5pt}

\begin{tabulary}{1.1\textwidth}{LL}
11.64 & \boldred{Přelet} nad \boldred{kukaččí}m hnízdem \\
11.87 & Byl jednou jeden film: \boldred{Přelet} nad \boldred{kukaččí}m hnízdem \\
\end{tabulary}
\horizlina

\end{tt}
\caption{Výsledek dotazu \bftt{Přeletěla kukajda}}
\label{tab:result:preletela_kukajda}
\end{table}

U dotazu \bftt{přelet~kukačka} v tab.~\ref{tab:result:prelet_kukacka} opět
selhal stematizér češtiny. Dotaz se převedl na \bftt{prelt~kukack}, zatímco
záznam \bftt{Přelet~nad~kukaččím~hnízdem} byl při indexování převeden na
neshodující se \bftt{prelt~nad~kukaccim~hnizd}. \bftt{edit\_dyn} našel shodu i
pro velmi zkomolený dotaz (tab.~\ref{tab:result:preletela_kukajda}).

\mbox{}\input{hits/gottfather.tex}\mbox{}

Pokud chceme najít film s anglickým názvem \bftt{The Godfather} a nemůžeme si
vzpomenout, jak se to vlastně píše (př. \bftt{Gottfather}), vyhledávače
využívající editovací vzdálenost mohou napovědět. Trigramový vyhledávač
\bftt{trigram\_ES} nenašel shodu, kterou jsme hledali, ale několik textově
správných výsledků také našel.  (tab.~\ref{tab:result:gottfather}).

\mbox{}\input{hits/god_father.tex}\mbox{}

Obecně je u invertovaných indexů problém s oddělenými slovy. \bftt{edit},
\bftt{edit\_dyn} a \bftt{cs\_ES} indexují celá slova, zatímco
\bftt{trigram\_ES} indexuje trigramy. Příklad v
tab.~\ref{tab:result:god_father} je ukázkou, jak se vyhledávače zachovají, když
je cílovým dokumentem opět film \bftt{The Godfather}, ale dotaz je napsán jako
dvě oddělená slova \bftt{god} a \bftt{father}, ze kterých je kompozitní slovo
\bftt{godfather} složeno.

\bftt{trigram\_ES} byl schopen nalézt 1005 dokumentů s vysokou přesností k
našemu očekávání. Ostatní rovněž nalezly několik dokumenťů, ale až na pár
výjimek, které v textu obsahují kompozitní slovo oddělené pomlčkou, se k cíli
ani nepřibližíly.

\subsection{Jména}
Pro databázi jmen byla zvýšena tolerance překlepů v editační vzdálenosti pro
kratší slova, protože jména jsou obvykle krátká proti názvům filmů. Menší
datová sada vykompenzuje nadbytečný výpočetní výkon, který se použije pro vyšší
editační vzdálenost.

\mbox{}\input{hits/paul_mccartney.tex}\mbox{}

Od všech vyhledávačů opět požadujeme alespoň přesnou shodu
(tab.~\ref{tab:result:paul_mccartney}).

\mbox{}\input{hits/paul_mccountry.tex}\mbox{}

Pro ukázku fuzzy hledání v databázi jmen je dotazem zkomolené jméno
\bftt{Paul McCountry}, které by mělo nalézt \bftt{Paul McCartney}
(tab.~\ref{tab:result:paul_mccountry}).

\mbox{}\input{hits/sean_konery.tex}\mbox{}

Dynamická editační vzdálenost u \bftt{edit\_dyn} se obtížně vypořádává s
krátkými jmény jako \bftt{Sean} u \bftt{Sean Connery} a s fonetickým přepisem
přepisem příjmení jako \bftt{Konery} (tab.~\ref{tab:result:sean_konery}). Běžná
editační vzdálenost neupřednostňující změny na koncovkách se s fonetickým
případem vypořádá opět za cenu toho, že najde velké množství nesouvisejících
záznamů.

Žádný vyhledávač se neuplatnil u plně fonetického přepisu u dotazu \bftt{Šón
Konery}. Musela by být silně zvýšena tolerance editační vzdálenosti pro krátká
slova (\bftt{son} a \bftt{sean} se liší vzdáleností 2). Nevýhodou by bylo velké
množství nevyhovujících záznamů.

\subsection{Názvy produktů}

\mbox{}\input{hits/intel_8086.tex}\mbox{}

Fuzzy vlastnost vyhledávače umožňuje explorativní vyhledávání podobných pojmů
těm, které by uživatel ani nemohl najít, narozdíl od běžných slov, v jiném
slovníku. Dostal by se k nim třeba přes seznam obdobných pojmů, které se
uvádějí v encyklopediích, nebo jaké nakupujícím představují eshopy (např.
\uv{sekce ostatní také koupili}). Příklad v tab.~\ref{tab:result:intel_8086}
ukazuje tuto situaci na produktu s technickým názvem, který má od stejné firmy
několik produktů s podobným nebo odvozeným názvem. Příklad v
tab.~\ref{tab:result:mitsubishi_ki} ukazuje, jak může fuzzy vyhledání u
\bftt{edit} a \bftt{edit\_dyn} pomoci, když je v databázi název firmy produktu
ve více jazycích, přestože se data tváří, že jsou pouze v jednom jazyce.
\bftt{trig\_ES} případ nezvládl, protože nelze jednoduše nakonfigurovat dotaz,
který by vyžadoval alespoň částečný zásah od každého slova v dotazu.

\mbox{}\input{hits/mitsubishi_ki.tex}\mbox{}




\section{Proximita slov}
Rozdíl mezi klasickým upřednostňováním počtu shodujících se slov v
dokumentu a ohodnocením na základě maximální blízkosti slov demonstrují
dva příklady v
tab.~\ref{tab:result:the_live}~a~\ref{tab:result:the_live_tour}. Zatímco
ten první preferuje delší dokumenty, protože mají větší šanci obsahovat
shodující se slova, ten druhý je k tomuhle imunní. Pro něj je ideální
shoda taková, kde jsou slova v dokumentu bezprostředně vedle sebe a ve
stejném pořadí jako v dotazu.

Pro tento příklad byla záměrně použita slova, která se ve filmových datech
objevují frekventovaně díky tomu, že se mezi filmy řadí i záznamy hudebních
koncertů. U běžného dotazu nelze pozorovat, že by proximita měla zásadní dopad
na kvalitu výsledků právě kvůli krátkosti dokumentů. Smysl by pravděpodobně
měla při použití na delším textu, kde by se ale projevila její relativně vyšší
výpočetní náročnost.

%\mbox{}\input{hits/co_nam.tex}\mbox{}

\mbox{}\input{hits/the_live.tex}\mbox{}

\mbox{}\input{hits/the_live_tour.tex}\mbox{}



\clearpage
\section{Rychlost vyhledávání} \label{sec:rychlost_vyhledavani}

%\mbox{}\input{figures/timings.tex}\mbox{}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost dotazu \bftt{[ms]}}
    \label{fig:timings}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost dotazu \bftt{log[y] [ms]}}
    \label{fig:timings_log}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Pro porovnání výkonu bylo náhodně zvoleno 24 dotazů pro vyhledání v databázi
\bftt{csfd\_filmy}.  . Obrázky \ref{fig:timings} a \ref{fig:timings_log}
porovnávají rychlost běhu dotazu mezi systémy \bftt{edit\_dyn} interpretovaného
v CPythonu, \bftt{edit\_dyn} interpretovaného v PyPy a dvou konfigurací pro
ElasticSearch \bftt{trig\_ES} a \bftt{cs\_ES}.  Časy jsou průměrem 100 běhů
dotazu pro každý ze systémů.

Podle očekávání je implementace \bftt{edit\_dyn} výkonově nejslabší kvůli
dynamicky typovanému Pythonu. Znatelný je rozdíl mezi jednotlivými interpretery
Pythonu. Zejména pro korelované dotazy \bftt{the of}, \bftt{the they} a
\bftt{the than then they thel them thun}, jimž odpovídá v
obr.~\ref{fig:timings_log} špička dosahující až 4000ms. Tyto dotazy byly
záměrně zkonstruovány z nefrekventovanějších slov \bftt{the} a \bftt{of}, aby
ukázaly na nejhorší možný případ. Oproti CPythonu dokáže PyPy rozpoznat
opakující se kód průchodu invertovanými seznamy a efektivně ho vykonat, jak je
patrné z relativního rozdílu těchto dotazů oproti ostatním.

\bftt{trig\_ES} je podle očekávání pomalejší než základní \bftt{cs\_ES}, ale do
takové míry, kdy to pro malou datovou kolekci nevadí.


\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_py_sorted.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro CPython \bftt{log[x]}}
    \label{fig:timings_py_sorted}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_pypy_sorted.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro PyPy \bftt{log[x]}}
    \label{fig:timings_pypy_sorted}
    \end{figure}
    \end{minipage}}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_py_sorted_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro CPython \bftt{log[x,y]}}
    \label{fig:timings_py_sorted_log}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_pypy_sorted_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro PyPy \bftt{log[x,y]}}
    \label{fig:timings_pypy_sorted_log}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Poměr všech tří fází vykonání dotazu (1. vyhledání podobných slov ve slovníku
\bftt{trie}, 2. sloučení všech odpovídajících invertovaných seznamů
\bftt{slouceni}, 3. ohodnocení nalezených dokumentů \bftt{ohodnoceni}) na
obrázcích \ref{fig:timings_py_sorted}, \ref{fig:timings_pypy_sorted},
\ref{fig:timings_py_sorted_log} a~\ref{fig:timings_pypy_sorted_log} je odlišný
pro různé dotazy. \bftt{ohodnoceni} je přímo závislé na počtu nalezených
výsledků, protože se pro každý z nich aplikuje stejná ohodnocující funkce a
následné seřazení.

Ostatní fáze nejsou na počtu výsledku závislé. \bftt{slouceni} není tolik
závislé díky předsloučeným invertovaným seznamům hybridního uspořádání. Zvýšení
náročnosti je možné pozorovat pro dotazy s frekventovanými slovy \bftt{the} a
\bftt{of}, protože jejich invertované seznamy a disjunkce seznamů s podobnými
slovy jsou delší než u průměrného dotazu. Fáze vyhledání všech slov -
\bftt{trie} - se odvíjí pouze od charakteristiky dat. Tedy pokud existuje více
podobných slov, pak je vykonání delší. Přesto nedosahuje velikých výkyvů.

Za zmínku stojí změna v pořadí fází \bftt{trie} a \bftt{slouceni} mezi
interpretery CPython a PyPy. PyPy si lehce poradí se sloučením invertovaných
seznamů a dominující fází se stane ne tak snadno predikovatelné prohledání ve
slovníku.


%\subsubsection{Porovnání implementací disjunkce}
%\noindent\makebox[\textwidth][c]{%
%    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
%    \begin{figure}[H]
%    \includegraphics[width=\textwidth]{figures/timings_union_cpython.eps}
%    \captionsetup{justification=centering}
%    \caption{Rychlost sloučení pro CPython}
%    \label{fig:timings_union_cpython}
%    \end{figure}
%    \end{minipage}
%    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
%    \begin{figure}[H]
%    \includegraphics[width=\textwidth]{figures/timings_union_pypy3.eps}
%    \captionsetup{justification=centering}
%    \caption{Rychlost sloučení pro PyPy}
%    \label{fig:timings_union_pypy}
%    \end{figure}
%    \end{minipage}}
%\mbox{}\\

Pro srovnání byly naimplementovány čtyři různé druhy vykonání disjunkce
invertovaných seznamů, které následně figurují v konjunktivním sloučení.

\bftt{heap} je klasické sloučení za pomocí binární haldy, které se hodí, pokud
je invertovaných seznamů mnoho. Kvůli hybridizaci invertovaných seznamů jich
tolik není, a proto by měly být minimálně stejně účinné implementace, kde
prioritní frontu tvoří pouhý lineární seznam - \bftt{list}. \bftt{list-prune}
je obdobou, pouze s tím, že z prioritní fronty odstraní invertované seznamy,
které se dostaly do konce. \bftt{skip} je experimentální implementace
optimalizující přeskakování v invertovaných seznamech oproti postupnému
posunování po jednom dokumentu.

% V obr.~\ref{fig:timings_union_cpython}~a~\ref{fig:timings_union_pypy} jsou
% porovnání těchto implementací pro CPython a Pypy na dvaceti náhodně
% vygenerovaných dotazech. Obrázky jsou seřazeny vzestupně podle implementace
% \bftt{skip}. Žádná ze čtyř implementací nemá dle výsledků v obecném případě
% zásadní vliv na dobu vykonání dotazu. Každá je v některém případě víceméně tou
% nejméně vhodnou a naopak.


\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_dense.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* y*]}}
    \label{fig:timings_prefix_union_x_y}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* yy*]}}
    \label{fig:timings_prefix_union_x_yy}
    \end{figure}
    \end{minipage}}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse2.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* yyyy*]}}
    \label{fig:timings_prefix_union_x_yyyy}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse3.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[xxxx* yyyy*]}}
    \label{fig:timings_prefix_union_xxxx_yyyy}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Obrázky~\ref{fig:timings_prefix_union_x_y},~\ref{fig:timings_prefix_union_x_yy},~\ref{fig:timings_prefix_union_x_yyyy}
a \ref{fig:timings_prefix_union_xxxx_yyyy} demonstrují rychlost vykonání
náhodných prefixových dotazů. U prefixových dotazů najde slovník velké množství
odpovídajících slov ke každému dotazovanému slovu. Dotazu typu \bftt{x* yyyy*}
v obr.~\ref{fig:timings_prefix_union_x_yyyy} znamená dotaz o dvou prefixech.
\bftt{x} a \bftt{yyyy} znázorňují šablony vygenerovaných slov. Tedy náhodně
vygenerovaný dotaz může vypadat například jako \bftt{s* bene*} nebo \bftt{m*
tolu*}. U dotazu \bftt{x* y*} očekáváme silný vliv disjunkce na dobu vykonání
sloučení, protože prefix pouze o jednom znaku odpovídá velkému množství slov. V
kontrastu dotaz typu \bftt{xxxx* yyyy*}  bude obsahovat menší množství seznamů
v disjunkci. Dotazy typu \bftt{x* yy*} a \bftt{x* yyyy*} slouží k porovnání
schopnost implementací přeskakovat dokumenty v kombinaci řídké (\bftt{yyyy*}) a
husté (\bftt{x*}) disjunkce. Podle očekávání by měla dominovat implementace
\bftt{skip}, ale není tomu tak. Nejvhodnější implementací se ukázala
jednoduchost obyčejného seznamu v případech \bftt{list} a \bftt{list-prune}.

\mbox{}\\
\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/hybrid_union_s_tebou.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[s* tebou*]}}
    \label{fig:hybrid_union_s_tebou}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/hybrid_union_s_t.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[s* t*]}}
    \label{fig:hybrid_union_s_t}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Klíčovým srovnáním této práce je závislost rychlosti sloučení invertovaných
seznamů na velikosti seskupení invertovaných seznamů (TODO ref algorithmus).
Pro porovnání byla datová sada \bftt{csfd\_filmy} zaindexována pro více
parametrů seskupení a pro každý takový index byla změřena doba sloučení pro
prefixové dotazy \bftt{s* tebou*} (reprezentující typ dotazu \bftt{x* yyyyy*},
obr.~\ref{fig:hybrid_union_s_tebou}), a dotaz \bftt{s* t*} (reprezentující
\bftt{x* y*}, obr.~\ref{fig:hybrid_union_s_t}). Parametrem seskupení se rozumí
minimální velikost seskupeného invertovaného seznamu. Tedy klasický invertovaný
index bez hybridního sloučení odpovídá hodnotě 1. Naopak úplný dopředný index
má maximálně možně dlouhý invertovaný seznam o velikosti počtu všech slov v
datové sadě (v tomto případě je v databázi \bftt{csfd\_filmy} 1\,958\,147
výskytů slov). Lidsky řečeno, čím více vlevo v
obr.~\ref{fig:hybrid_union_s_tebou}~a~\ref{fig:hybrid_union_s_t}, tím je index
\uv{více invertovaný}. Naopak, čím více vpravo, tím index více připomíná
původní datovou kolekci. Křivky jsou v hodnotách 500\,000 a výše téměř totožné,
protože pro oba dotazy už jsou jejich invertované seznamy předsloučené a při
průchodu už dochází pouze k jejich filtrování.

Zajímavým pozorováním je absolutní nevhodnost klasického invertovaného seznamu
pro krátké prefixové dotazy. V takovém případě dochází k masivnímu sloučení
mnoha krátkých invertovaných seznamů. Podle očekávání zde uplatňuje
implementace \bftt{heap}, jejíž složitost nalezení následujícího dokumentu je
$\log n$ oproti ostatním, které procházejí disjunktivní seznam lineárně.



\subsection{Hledání ve slovníku}
\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/trie_edit_cpython.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost trie (v~CPythonu)~\bftt{log[y]}}
    \label{fig:trie_edit_cpython}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/trie_edit_pypy3.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost trie (v~PyPy)~\bftt{log[y]}}
    \label{fig:trie_edit_pypy}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Jedním z důvodů, proč je dynamicky prořezávaná editovací vzdálenost výhodná je
její časová náročnost, protože se, narozdíl od běžné, vyhýbá drahému průchodu
podstromy u počátečních znaků.
Obr.~\ref{fig:trie_edit_pypy}~a~\ref{fig:trie_edit_cpython} znázorňují
závislost tří variant editovací vzdálenosti při průchodu prefixovým stromem a
nalezením všech podobných slov. \bftt{edit} je klasická Levenshteinova
vzdálenost, \bftt{edit-dyn} je dynamicky prořezávaná varianta a
\bftt{edit-dyn-prefix} je předchozí varianta, která vyhledává prefixově.
Vertikální čáry znázorňují zvýšení práhu editovací vzdálenosti. Ta začíná na
nule - tedy nejkratší slova se hledají přesně, a na délkách 2, 4, 6, 8 se
zvedne o jeden bod (slova o délce 3 se hledají s práhem 1, o délce 5 s práhem
2, ...). Dlouhé vykonání u krátkých slov u \bftt{edit-dyn-prefix} je způsobeno
vyčerpávajícím průchodem všemi slovy, které odpovídají prefixu. Dynamické
prořezávání je výhodné, protože umožňuje bez větší ztráty vyšší toleranci
editační vzdálenosti s rostoucí délkou slova.

Narozdíl od sloučení invertovaných seznamů není u vyhledání ve slovníku patrná
výhoda PyPy oproti CPythonu. Dokonce je u \bftt{edit-dyn} a
\bftt{edit-dyn-prefix} CPython výkonnější, což naznačuje, že JIT kompilace PyPy
v tomto případě nepřinesla žádný lepší výkon. Implementace slovníku v jazyce s
překladem do nativního kódu by měla být znatelně rychlejší.



\subsection{Velikost indexu}
ElasticSearch nabízí bod API se statistikami, které nám řeknou i velikost místa
zabraného indexem. Alternativně lze velikost indexu zjistit sečtením velikostí
všech uložených souborů. Obě metody vracejí zhruba stejné výsledky ($\pm 4\%$).

Indexy pro vyhledávače \bftt{edit} a \bftt{edit\_dyn} jsou totožné. Liší se
pouze formou hledání podobných slov ve slovníku.

\begin{Verbatim}
# Součet souborů na disku
du -shc //elasticsearch/nodes/*/indices/<index>/*/index/*

# Využití statistik skrz API
http '<uri>/<index>/_stats' | jq .indices.<index>.primaries.store.size_in_bytes
\end{Verbatim}

\begin{tt}
\begin{table}[H]
\centering
\begin{tabulary}{\textwidth}{LLRR}
\textbf{databáze} & \textbf{implementace} & \textbf{velikost indexu} \\
\hline
\multirow{3}{*}{\texttt{csfd\_filmy}} & trigram\_ES & 86\,MB \\
                                      & cs\_ES      & 37\,MB \\
                                      & edit        & 42\,MB \\
\hline
\multirow{3}{*}{\texttt{csfd\_tvůrci}} & trigram\_ES & 31\,MB \\
                                       & cs\_ES & 14\,MB \\
                                       & edit & 15\,MB \\
\hline
\multirow{3}{*}{\texttt{cswiki\_titulky}} & trigram\_ES & 135\,MB \\
                                          & cs\_ES & 63\,MB \\
                                          & edit & 63\,MB \\
\hline
\end{tabulary}
\caption{Velikosti indexů}
\label{tab:index_size}
\end{table}
\end{tt}

\chapter{Další postup a otevřené problémy}
\section{Integrace do existujících systémů}
Existující otevřené projekty, které se vyhledáváním zabývají, postrádají zde
popsané techniky. Tato práce by pro ně mohla poskytnout vodítko, jak techniky
implementovat, pokud by to pro ně neznamenalo překopání celé architektury.

\subsection{Invertované seznamy v externím systému}

\section{Statické a dynamické indexy}
% static: no locks and latches, compact data structures, cache efficient
% 2 level data structure + bulk updates
% columnar storage
% document or term based
% share nothing
\section{Příliš krátký vstupní řetězec}
\section{Plánování dotazů}
\section{Ukládání inkrementálních mezivýsledků}
\section{Zvýrazňovač pro editační vzdálenost}
\section{Škálovatelnost}
V této práci je mým cílem poukázat, že pro malá objemy dat je vhodné použít
adekvátní techniky pro zvýšení relevance vyhledávání za cenu zvýšené výpočetní
náročnosti. Tím, že jsou počítače stále výkonnější a datové sady se snadno
vejdou do dostupné paměti RAM, neměl by být při efektivní implementaci a
běžných dotazech s výkonem problém. Pokud jsou požadavkem větší objemy dat,
protože například firma roste s přibývajícím počtem zákazníků, pak by dnes
neměl být problém dokoupit si více paměti, která je každým rokem stále levnější.

I kdyby nepřipadalo v úvahu vertikální škálování přidáváním více RAM, existuje
možnost horizontálního škálování na více počítačů a vytvoření distribuovaného
systému. Invertované indexy jsou velmi snadno paralelizovatelné problémy
% TODO term or document distributed. cite something

Ten by ostatně byl nezbytný, pokud by požadavkem byla vysoká
dostupnost dat. Stejná data by byla replikována na více strojů a v případě
selhání jednoho by se instantně použila redundance více strojů.



\subsubsection{Indexování}
Implementace je zaměřená především na ty části, které jsou klíčové pro
demonstraci návrhu. Indexovací fáze používá jednoduchý \bftt{sort based}
algoritmus a nevyužívá techniky více souborů. Do budoucna by bylo užitečné
zjistit, jaké jsou nejvhodnější indexovací metody pro hybridní index oproti
klasickému invertovanému indexu.

\subsubsection{Porovnání výkonu}
Algoritmy pro slučování invertovaných seznamů jsou obzvlášť citlivé na správnou
implementaci a především na zvolený programovací jazyk. Zatímco Python je
ideální prototypovací jazyk, je asi tím nejhorším, v čem by měl být produkční
vyhledávací systém vytvořen. To se sice časem může změnit díky výkonově
orientovanému interpreteru PyPy. Přesto by měla být pro adekvátní porovnání
implementace provedena v jazyce s podporou překladu do nativního strojového
kódu.

Se zpomalujícím Mooreovým zákonem se objevují různé formy paralelismu, které by
měly sloužit jako náhrada za klesající tempo růstu. Pro pečlivě optimalizovaný
kód, které databázové a vyhledávací systémy často využívají, se častěji a
častěji využívají bitově-paralelní instrukce moderních procesorů. I z tohoto
důvodu by měl být zvolen takový jazyk, ve kterém není velkou překážkou tyto
moderní schopnosti procesorů využívat. (cite completesearch C++).


\chapter{Závěr}
Záměrem této práce je poukázat na alternativní techniky v oblasti vyhledávání a
demonstrovat možný vyhledávací systém, který je implementuje. Tento systém by
měl být vhodnější pro potřeby jednotlivců i firem, které vyžadují od
vyhledávacího systému robustnost proti malým změnám v dotazu. Uplatnění by měl
nalézt v menších datových sadách, než na které je dnes kladen důraz, protože
díky omezené velikosti dat je možné přebytečný výpočetní výkon použít k těmto
náročnějším technikám.

Navržený vyhledávací systém byl implementován jako prototyp obsahující popsané
techniky a porovnán s několika dalšími vyhledávači, které byly vytvořeny jako
různé konfigurace v některých populárních open source vyhledávacích systémech
tak, aby odpovídaly popsaným požadavkům.

Implementovaný prototyp byl vytvořen tak, aby byl schopný v první řadě
demonstraci popsaných technik a byl dostatečně flexibilní pro potřeby
experimentování. Výkonově není reprezentativní kvůli implementaci v zvoleném
dynamicky typovaném programovacím jazyce Python, ale byla použita stejná
architektura, kterou by bylo možné pouze přepsat do výkonnějšího jazyka. Bylo
proto provedeno relativní porovnání výkonu mezi určitými typy dotazů s vyšší
mírou tolerance pro běh dotazu, které ukázalo že pro běžný dotaz by vyhledávač
splňoval kritéria pro vyhledávání v reálném čase. 

Analýza nejhorších případů ukázala na typ dotazů, které by mohly být zneužity
proti vyhledávači. Pro tento případ byl nastíněn způsob, jak ho řešit, ač by
zkomplikoval původně jednoduchou architekturu.

V porovnání s alternativami založených na ngramové podobnosti není navržený
systém robustní v případech oddělených slov nebo naopak kompozitních slov.
Možným dalším rozšířením by byla kombinace navržené architektury s ngramy.

\section*{Odkazy}
\url{http://www.dcs.gla.ac.uk/~craigm/publications/lacour08efficiency.pdf}

%\medskip
\bibliographystyle{myplainnat}
\renewcommand{\refname}{Reference}
\renewcommand{\bibname}{Reference}
\setlength{\bibsep}{0pt}
\bibliography{ref}

%\appendix
%\chapter{Konfigurace porovnaných systémů v ElasticSearch}\label{appendix:search_config}
%\section{ElasticSearch}
%\subsection{trigram\_ES}
%\subsubsection{Analyzér}
%Pro ngramové zaindexování lze použít buď ngramový tokenizér
%\bftt{my\_ngram\_tokenizer}, nebo ngramový filtr \bftt{my\_ngram\_filter}.
%Ngramový tokenizér podporuje zvýrazňování, které se mi s ngramovým filtrem
%nepovedlo nastavit ani po vyčerpávajícím hledání. Ve výpisu konfigurace jsou
%uvedeny oba způsoby nastavení. \bftt{my\_short\_words\_filter} vybírá slova
%zahozená tokenizérem \bftt{my\_ngram\_tokenizer}, proto je v něm nastavena
%maximální délka menší než délka trigramu - 2.
%
%\input{appendix/es_settings/trigram_analyzer.tex}
%
%\subsubsection{Mapování}
%Aby fungoval zvýrazňovač pro ngramové indexy, musí být použit \bftt{Fast
%Vector Highlighter}, který funguje, pokud se pro pole nastaví hodnota
%\bftt{with\_position\_offsets} u parametru \bftt{term\_vector}. Nicméně
%nakonec byl použit externí zvýrazňovač, proto by mohlo být ukládání této
%informace vypnuto. Doporučuje se ji použít pro dlouhé texty, což není
%případ použitých datových sad v této práci.
%
%\input{appendix/es_settings/trigram_mapping.tex}
%
%\subsubsection{Dotaz pro vyhledávání}
%Dotaz je navržen, aby došlo k zásahu, pokud dokument obsahuje alespoň
%$70\%$ trigramů dotazu. Vyšší hodnoty začaly rychle ztrácet výsledky a
%nižší hodnoty vracely příliš šumu.
%
%Dotaz se snaží shodu najít napříč třemi indexy. \bftt{shorts} jsou
%ngramy kratší než $3$, protože je ElasticSearch zahazuje. \bftt{title}
%obsahuje všechny tokeny dokumentu (titulku filmu) a \bftt{trigram} je
%trigramový index. Manipulace s váhami jednotlivých polí nemá velký vliv
%na přesnost výsledku.
%
%Zvýrazňování je nastaveno doporučenou formou pro tento typ dotazu, ale
%přesto vynechává překrývající se dotazy (viz TODO). Zvýrazňování je
%provedeno externím přiloženým programem \bftt{trihigh.py}.
%
%\input{appendix/es_settings/trigram_search.tex}


\backmatter


\end{document}
