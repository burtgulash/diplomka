\documentclass[11pt,letterpaper,oneside,openright]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage[left=4cm,right=3cm,bottom=2.5cm,footskip=1.5cm]{geometry}
\usepackage{color}
\usepackage[scaled=0.83]{beramono}
\usepackage{amsmath}

\usepackage[sort,numbers]{natbib}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}

\usepackage{tabulary}
\usepackage{subcaption}

\usepackage{fancyvrb}
\usepackage{pygme}

\usepackage{float}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{caption}
\captionsetup[figure]{name=Obr.}
\captionsetup[table]{name=Tab.}

\pagestyle{plain}
\linespread{1.15}
\setlength{\tabcolsep}{1em}

\definecolor{Darkgreen}{rgb}{0,0.4,0}
\hypersetup{%
    pdfborder={0 0 0},
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue,
}

% \usepackage{sectsty}
% \sectionfont{\fontsize{21}{21}\selectfont}
% \subsectionfont{\fontsize{16}{16}\selectfont}
% \subsectionfont{\fontsize{14}{14}\selectfont}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{30pt}%
    {\parindent 0pt \raggedright \normalfont
        \interlinepenalty=-1
        \Huge \bfseries \thechapter\hspace{.75em}#1\par\nobreak
        \vskip 40pt
    }}
\makeatother

\newcommand{\bftt}[1]{\texttt{\textbf{#1}}}
\newcommand{\boldred}[1]{\textbf{\color{red} #1}}
\newcommand{\horizlina}%
{
    \mbox{}\vspace{1em}
    \hrule
    \mbox{}
}



\begin{document}
\frontmatter

% TITLEPAGE
\begin{titlepage}
\thispagestyle{empty}
\centering
\begingroup
{\LARGE\bfseries Vysoká škola ekonomická v Praze\par}
{\Large\bfseries Fakulta informatiky a statistiky\par}
{\Large\bfseries Katedra informačních technologií\par}
\vspace{1.8cm}
{\large Studijní program: Aplikovaná informatika\par}
\vspace{.2em}
{\large Obor: Informační systémy a technologie\par}
\vspace{1.8cm}
{\Huge\bfseries Návrh vyhledávacího systému pro moderní potřeby\par}
\vspace{.5cm}
{\scshape\LARGE\bfseries diplomová práce\par}
\vspace{4.2cm}
\def\arraystretch{1.6}
\begin{tabular}{rl}
{\Large Student:} & {\Large Tomáš Maršálek \par} \\
{\Large Vedoucí:} & {\Large RNDr. Helena Palovská, Ph.D. \par} \\
{\Large Oponent:} & {\Large doc. RNDr. Petr Strossa, CSc. \par} \\
\end{tabular}
\par
\vspace{4cm}
{\Large\bfseries 2016\par}
\vfill
\endgroup
\end{titlepage}

% PROHLÁŠENÍ
\newpage
\chapter*{Prohlášení}
\noindent Prohlašuji, že jsem diplomovou práci zpracoval samostatně a že jsem uvedl
všechny použité prameny a literaturu, ze které jsem čerpal.\par
\vspace{2.4cm}
\noindent V Praze \today \hfill \dotfill \hspace{2em} \par
\hfill \strut Jméno a příjmení \hspace{3.5em} \strut


% CZ Abstrakt
\newpage
\chapter*{Abstrakt}
\paragraph{Klíčová slova}
Přibližné Vyhledávání v Textu, Prefixové Vyhledávání, Blízkost, Autocomplete, Search As You Type, Vyhledávání s Tolerancí Chyb, Poloinvertovaný index, Hybridní index, Invertovaný index

% EN Abstract
\newpage
\chapter*{Abstract}
\paragraph{Keywords}
Approximate Text Search, Prefix Search, Proximity Search, Autocomplete, Search As You Type, Error Tolerant Text Search, Half-Inverted Index, Hybrid Index, Inverted Index


% TOC section
\newpage
{\hypersetup{hidelinks}
    \tableofcontents
}

\mainmatter
\chapter{Úvod}
Textové vyhledávání se stalo populární v 90. letech s nástupem Webu a s ním
masivního nárůstu informací, které jsou dostupné prakticky každému po pár
kliknutích myši. Toho bylo dosaženo vzestupem velkých webových vyhledávačů ,
které byly schopné exponenciálně rostoucí růst informací
zkrotit~\cite{search_history}.

% Alespoň do jisté míry, protože tyto vyhledávače
% umožňují přítup pouze k té části webu, ke které se lze dostat následováním
% hypertextových odkazů. Většina informací není skrze pouhé odkazy dostupná (deep
% web), protože je např. skrytá v interních databázích, k nimž se lze dostat
% pouze interním vyhledávacím systémem.

Popularita webových vyhledávačů zapříčinila zájem o vyhledávací systémy i u
jednotlivců a firem. Stejné techniky začaly být dostupné v open source
systémech, z nichž nejznámějšími jsou Solr a ElasticSearch vycházející ze
stejné softwarové knihovny pro vyhledávací systémy - Apache Lucene. Někteří
tvrdí, že knihovna Lucene přinesla komoditizaci a demokratizaci na poli
specializovaných vyhledávačů~\cite{dion_almaer,javaworld}.

Vyhledávání by se tedy mohlo jevit jako vyřešený problém. V této práci bude ale
představeno několik méně známých technik, které pokud by se více
popularizovaly, mohly by změnit tenhle pohled a zapříčinit vznik nových
vyhledávacích systémů.


% Pokud by tyto techniky byly natolik užívatelsky výhodné, pak by bylo podivné,
% že by je dodnes nikdo neimplementoval. Některé výzkumné týmy a komerční řešení
% se jimi přímo zabývají, ale v dnešní době platí v softwaru očekávání, že pokud
% není takový systém distribuován zdarma s otevřeným kódem (open source), nezíská
% si popularitu ani mezi nezávislými jedinci, ani mezi firmami. Snad pouze pokud
% byly osobně osloveny obchodním oddělením dané firmy, a tedy tvoří menšinu.
% Dalším důvodem je, že pokud se nějaký systém snaží být nejlepším pro všechny
% možné případy, pak je šance, že nebude nejlepším pro žádný z nich. To otvírá
% šanci pro nová řešení.

\section{Motivace}
Prozkoumávat oblast alternativních vyhledávacích technik jsem začal po
frustraci s vytvářením jednoduchého vyhledávače pro položky jako názvy produktů
eshopu nebo pro vyhledávání v databázi jmen. Vytvoření takového systému je dnes
považováno za vyřešený problém, protože existuje spousta open source databází a
jiných produktů, které se dají přizpůsobit téměř každé situaci.

\subsection{Fragmentace systémů}
Bez použití specializovaného vyhledávacího systému je nejčastějším řešením pro
vyhledávání v databázi použití technik, které poskytují databázové systémy.
Časté řešení je rozdělení textových polí na více částí a vyhledávání separátně
v každé z nich pomocí operátoru \bftt{like}, který umožňuje omezené přibližné
vyhledávání. Např. u databáze lidských jmen se často rozděluje text jména na části
jméno, příjmení a někdy titul a prostřední jméno. Problémy pak nastávají v méně
častých případech, kdy osoby např. nemají příjmení, jejich jméno je složeno z
více slov, než je počet kolonek ve formuláři, apod~\cite{name_falsehoods}.
Často by přitom stačilo modelovat jméno pouze jako jedno textové pole.

Nejčastějším řešením, kdy je požadované pokročilé textové vyhledávání, které
databázové systémy neposkytují, je duplikace dat z primární databáze (např.
MySQL, Postgres) do sekundárního systému pro vyhledávání (např. Solr,
ElasticSearch, Sphinx). Přitom by bylo jednodušší, kdyby databázové systémy
přímo podporovaly solidní vyhledávací systém formou externí komponenty. Např.
databázový systém Postgres některé vyhledávací doplňky poskytuje, ale očividně
ne v takové formě, aby se lidem nevyplatilo fragmentovat data do více systémů.

% BIg Noise - Silver
% Gargage In - Garbage Out

\subsection{Jeden systém pro všechny případy}
V oblasti databázových systémů bylo v posledních několika desetiletích cílem
vytvořit systémy, který by byly přizpůsobitelné pro každou situaci. Podle M.
Stonebrakera je tomuto úsilí konec, protože se ukazuje, že specializovaná
architektura může být řádově výkonnější než tradiční všeobecná
architektura~\cite{Stonebraker:2005:OSF:1053724.1054024}. Příkladem jsou datové
sklady, ve kterých je charakteristická asymetrie mezi vkládáním a čtením dat.
Zatímco všeobecný systém musí být schopný vykonat jakékoliv rozložení zátěže,
ale pokud se tomuto požadavku povolí uzda a nastaví se nutnost vkládat data
dávkově, pak lze změnit uspořádání dat do velmi kompaktní formy, která lépe
zvládá dotazy používané v analytických aplikacích.

Jiným příkladem je použití databázového systému, který primárně uchovává data v
paměti RAM a sekundárně na disku. Tradiční architektura má tohle pořadí
obrácené, protože v 70. letech, kdy vznikala, bylo typické, že kapacita paměti
byla silně omezená. Prudký pokles cen paměti, disků a nástup jiných úložných
formátů (flash) zapříčinil, že toto schéma už není nejvhodnější pro současné
aplikace, protože většina z nich nepracuje s daty, které by se z většiny
nevešly do RAM.

Pokles cen a vyšší výkon hardwaru se týká i oblasti vyhledávačů, což přináší
nové příležitosti pro techniky, které byly doposud opomíjené z obdobných důvodů
zavrhované.

\subsection{Big Data a Small Data}
Jestliže se díky levnější paměti a silnějším procesorům stává výpočetní výkon
dostupnějším, existuje více směrů, jak lze tento jev aplikovat. Prvním z nich
je využít novou výpočetní kapacitu pro zpracování většího množství dat. Druhým
způsobem je výkon jednoduše vyplýtvat. Třetím je použít novou kapacitu pro
komplikovanější problémy.

První z těchto směrů se stal v poslední době velmi populární z několika důvodů.
Jednak se zvýšila dostupnost výpočetní kapacity díky firmám pronajímajícím
volnou kapacitu ve svých datových centrech (cloud computing). Vzestup tzv.
NoSQL databází a distribuovaného výpočetního modelu MapReduce přinesl běžným
uživatelům možnost používat distribuované algoritmy a datové struktury pro
rozsáhlé datové sady přesahující kapacity jednoho počítače (Big Data) bez větší
námahy.

Druhý směr je viditelný např. ve vzestupu dynamicky typovaných programovacích
jazyků, které silně odstiňují programátora od složitostí hardwaru za cenu
řádově pomalejšího výkonu. Tím, že je výpočetní výkon natolik levný, nejedná se
tolik o plýtvání, pokud se efektivně využije dražší čas programátora.

Třetí směr je opačným jevem k fenoménu Big Data, který by se odvozeně z názvu
jmenoval Small Data, a znamená soustředit se na relativně malé objemy dat, ale
s použitím vlastností, které byly při minulých výpočetních kapacitách
vyhodnoceny jako příliš náročné pro praktické použití. Pojem Small Data není
tolik rozšířený jako Big Data a byl několikrát použit v různých významech. V
jednom případě se ale shoduje s významem, v jakém je v této práci
zamýšlen~\cite{small_data}. Autor navrhuje použití nadbytečného výkonu např.
pro interaktivitu v publikovaných vědeckých článcích.

\subsection{Přibližné vyhledávání}
Motivaci této práce charakterizuje pojem Small Data, protože hlavní myšlenkou
je, že spousta problémů firem nepotřebuje produkty pro manipulaci s terabyty
dat, ale často jsou to pouze datové objemy, které dnešní počítače zvládnou
hravě bez opuštění rychlé paměti RAM. Například přibližné vyhledávání v textu
je jednou z oblastí, které není věnováno příliš pozornosti, ačkoliv je jeho
využití nesmírné.

Znatelný rozdíl je v požadavcích pro vyhledávání ve full-textu, tedy rozsáhlých
dokumentech (webové stránky, pdf soubory) a v relativně krátkých textových
řetězcích, jakými jsou např. jména lidí, názvy produktů, geografické lokace,
krátké reklamní sdělení, apod. Rozdíl je především ten, že full-text je tvořen
celými větami jednoho jazyka, zatímco názvy a pojmenování jsou tvořeny často
jen několika málo slovy, které netvoří větu, a často nejsou ani tvořená slovy
jednoho jazyka.

Např. u názvů produktů často nelze rozlišit jazyk, protože jsou slova unikátní
a tím, že obsahují číslice, nelze o nich ani hovořit jako o slovech. Např.
\bftt{MSI GeForce GTX 1060 GAMING X 6G, 6GB GDDR5} je název grafické karty,
\bftt{Bosch TDA502412E} a \bftt{Tefal Autoclean ANTI CALC FV9640} jsou příklady
názvů pro žehličku a \bftt{55" LG OLED55E6V} a \bftt{55" HISENSE H55M3300} jsou
názvy televizorů.

V databázích jmen se často vyskytují jména osob různých národností a i přes
znalost národnosti nelze správně určit jazyk jména. Např. Jakými jazyky by byla
vyhodnocena jména \bftt{Keira Knightley}, \bftt{Keira Knightleyová} nebo
\bftt{Jessica Nováková}? Cílem je pouze poukázat, že vyhledávač jmen by měl být
robustní vůči malým textovým změnám a nespoléhat pouze na jednoduchou analýzu
textu podle detekovaného jazyka. Jazyková detekce je často možná pouze v
dlouhém textu, protože se opírá o statistické metody (např. frekvenční analýza
písmen, párů písmen, slov), která u krátkého textu selže.

Aplikace přibližného vyhledávání by mohla být pro hypotetický vyhledávač v
počítačově generovaných transkripcích mluveného textu nebo textu vygenerovaného
strojem z obrázků nebo videa. U obou případů můžeme předpokládat určitou
chybovost stroje a náročný proces, který by musel opravit tyto chyby před
vložením do databáze. Robustnost přibližného vyhledávače by se s chybami
vypořádala až během vyhledávání. Možná právě absence efektivního přibližného
vyhledávacího systému je důvodem, proč ještě neexistuje populární vyhledávač v
textech zaznamenaných z audio nebo video souborů.

%  Full-text obsahuje celé věty a tedy obsahuje tzv. stop slova
% (a, já, jsem, tu, k, nebo, ...), která pomáhají plynulosti jazyka a navazování
% myšlenek. Názvy stop slova neobsahují.

% Spousta interních databází vyžaduje efektivní a robustní vyhledávání, které

\section{Cíl práce}
Cílem práce je navrhnout praktický vyhledávací systém zaměřený pro
specializované datové sady jako názvy produktů, jména a obecně text obtížně
zpracovatelný jazykovou analýzou. Techniky, které by měl navrhovaný vyhledávač
používat nejsou v populární literatuře ještě velmi známé. Jsou výpočetně
náročnější než dosavadní algoritmy, ale na druhou stranu umožňují efektivnější
vykonávání některých uživatelských rozhraní a pokročilých typů dotazu.

% TODO kam s tim?
% Některé z popisovaných technik byly dlouho opomíjené, protože jejich
% implementace není jednoduchá, nebo dlouho nebyla výpočetně výhodná. Pokrok ve
% výpočetním výkonu, nových počítačových architekturách, vyšších kapacitách
% pamětí a obecně pokrok v hardwaru pro ně ale poskytuje novou příležitost.

Důraz je kladen především na prefixové a přibližné vyhledávání, která jsou
považována za výpočetně velmi náročná. Další navrhovanou technikou je
ohodnocení podle blízkosti (proximity) slov ve výsledcích.

Systém, který je v této práci navrhován se nesnaží být nejlepším řešením pro
všechny případy a explicitně se zaměřuje na specifičtější případy, ve kterých
by přibližné vyhledávání mohlo přinést velkou hodnotu. Z důvodu vyšší výpočetní
náročnosti se navrhovaný systém nesnaží jít stejnou cestou velkých dat (Big
Data), ale přesně opačně by mělo představované řešení nalézt uplatnění pro
specifické případy a menší objemy dat (Small Data).

Práce je koncipována tak, aby přiblížila opomíjené příležitosti v čistě textově
orientovaném vyhledávání, přiblížila dosavadní výzkum v těchto oblastech a na
prototypové aplikaci demonstrovala praktickou proveditelnost navrhovaného
řešení. Prototyp bude porovnán s některými populárními open source
vyhledávacími systémy nakonfigurovanými tak, aby splňovaly podobné požadavky.

\section{Porovnání}
Protože je dalších vyhledávacích systémů mnoho, pro porovnání bylo vytvořeno
pouze několik konfigurací pro ElasticSearch a Postgres, které v současnosti
reprezentují nejpoužívanější open source systémy s porovnatelnými schopnostmi.

\subsection{Relevance}
Vyhodnocení toho, který systém je lepší nebo horší, je z velké míry subjektivní
záležitost. Existují způsoby, jak hodnotit míru relevance vyhledávače pomocí
metrik přesnost (precision) a výtěžnost (recall), pokud se výsledky vyhledávání
uvažují pouze jako množiny. Pro srovnání výsledků vyhledávačů nás bude více
zajímat pořadí, ve kterém se relevantní výsledky vyskytnou a robustnost
vyhledávače při změnách dotazu. Pro uživatele je frustrující, pokud má
vyhledávač vlastnost, že při malé změně dotazu, např. změnou koncovky slova, mu
systém vrátí naprosto odlišné výsledky, rozdílný počet, nebo při této změně
dokonce nic nevrátí. U webového vyhledávání to tolik nevadí, protože i přes
změnu dotazu vyhledávač vrátí velkou množinu výsledků. Uživatel nepozná, jestli
se výsledky ztratily, nebo byly pouze seřazeny níže. U menších dat dojde k
tomu, že se záznamy z výsledků ztratí, čehož si uživatel ihned všimne.

Porovnání bylo vyhodnoceno na několika datových sadách o několika stovkách
tisíc záznamech, které budou reprezentovat datové sady pro případy, ve kterých
by se navrhovaný vyhledávač měl nejlépe uplatnit. Porovnáním jsou ukázky
interakce a zhodnocení několika uživatelů, kterým byl systém prezentován v
porovnání s ostatními implementovanými vyhledávači.

\subsection{Výkon}
U instantního vyhledávání platí pravidlo, že dotazy vykonané pod hranici
100\,ms jsou lidmi vnímány jako instantní~\cite{sto_ms}. Do této doby se musí
vejít jednak samotné vykonání algoritmu, a jednak doba pro přenos dat, která
nebude zanedbatelná, pokud bude vyhledávací aplikace poskytována jako služba ze
vzdáleného serveru.

Návrh nového vyhledávače je prezentován v dosavadní fázi jako prototyp napsaný
v dynamicky typovaném Pythonu, kvůli čemuž nelze provést objektivní srovnání
výkonu. Rychlost běhu programu v Pythonu se oproti nativní implementaci liší
řádově, a proto bude použito hrubé pravidlo jedné vteřiny, tedy pokud očekáváme
výkon prototypu desetkrát pomalejší, pak by se měla doba běhu vejít do
desetinásobku kritické hranice pro lidské vnímání.

Vyhledávač by rovněž neměl být příliš paměťově náročný. Na jednu stranu tím, že
se soustřeďuje na malé objemy dat a paměť se neustále zlevňuje, by neměl být
problém využít volné kapacity pro zvýšení výkonu. Na druhou stranu, pokud
nebude vyhledávač nenasytný, pak se do paměti vejde více dat a nebude nutné
zasahovat na disk, což je u nízkolatenčního vyhledávání velkou překážkou.
Několik náhodných zásahů na disk by znamenalo instantní porušení hranice
100\,ms, proto by měl být systém schopný pojmout všechna data do paměti.



% Díky vzestupu počítačů a především internetu došlo k dramatickému nárustu
% objemu textových informací ve formě snadno přístupné lidem i pro počítačové
% zpracování. Textová data jsou specifická v tom, že jejími tvůrci jsou lidé a ne
% počítačové systémy nebo měřící čidla. Při vyhledávání v takovýchto datech máme
% jako lidé silná očekávání na relevanci výsledků vyhledávání, nicméně kvůli
% charakteristice textových dat je vyhledávání v nich obtížné. Tím, že je úkol
% obtížný, existuje více specifických zaměření, než jedno obecné a použitelné pro
% všechny případy.
%
% S nástupem World Wide Webu v 90. letech došlo k vzestupu Webových vyhledávačů,
% které dnes slouží jako vstupní brána do světa informací.
% ...
%
% V této práci se snažím najít alternativní způsoby v tomto širokém oboru, které
% nejsou velmi známé, a zaměřím se na oblast, která je dle mého názoru jednou z
% nejžádanějších v praktických aplikacích. V současné době se pro ni ale
% používají techniky pro příliš obecné a zdrojově náročné prohledávání Webu a
% nedosahují takových výsledků, jakých by mohly, nebo za příliš vysokou cenu
% komplikovaných řešení.
%
% Velkým důvodem proč převažují techniky pro Webové vyhledávání je psychologický
% - většina problémů, se kterými se potýkají firmy i nekomerční projekty, zdaleka
% nepracuje s takovými objemy dat, se kterým se musí potýkat rozsáhlé webové
% vyhledávače nebo světové sociální sítě. V praxi jsou tyto problémy často
% řešitelné s použitím pouze jednoho počítače. Nicméně lidé přehnaně věří, že
% jejich projekty budou jednou dosahovat obdobných závratných velikostí, a proto
% volí řešení nevhodné pro jejich problémy.
%
% V ideálním případě by měla být podpora pro efektivní textové vyhledávání
% zabudována přímo do databázových systémů, ale v současné době není jejich
% podpora ideální, a proto aplikace s podporou textového vyhledávání používají
% kromě databáze ještě paralelní systém specializovaný pro text. Důvod je ten, že
% je obtížné vytvořit vyhledávač v takové formě, aby vyhovoval obecným
% požadavkům, na které se databáze používají. Techniky představené v této práci
% by měly být vhodnější pro generické účely, než ty, které používají Webové
% vyhledávače a ze kterých se čerpá inspirace pro textové indexy vestavěné do
% databází.
%
% ...
%
% V první části uvedu problém vyhledávání v textu, aby čtenář pochopil z jakého
% základu tato práce vychází. V druhé části na současný stav v praktickém světě
% textového vyhledávání a jak jsme se do něj dostali. Pak se zaměřím na algoritmy
% a datové struktury textového vyhledávání a alternativní techniky, které jsou v
% některých případech exotické a nepříliš vhodné na obecné použití, ale jiné
% které by dle mého názoru zasloužily větší pozornost, protože jejich zaměření
% odpovídá současným potřebám více, než na které je v dnešní době kladen největší
% důraz. V další části rozeberu vybranou oblast textového vyhledávání více do
% hloubky a popíšu problémy, které se v ní vyskytují. Zároveň rozeberu jejich
% řešení různými výzkumnými týmy a můj návrh na řešení. V sekci Analýza
% kvantitativně poukážu na konkrétní problémy popsané v předchozí sekci a
% problémy implementace takového systému. Na závěr představím možná budoucí
% řešení a další kroky k vylepšení, aby bylo možné vyhledávací systém nasadit v
% praxi.
%
% \chapter{Motivace}
%
% \subsection{Databáze a textové vyhledávání}
% Textové vyhledávání se často uvádí odděleně od databázových systémů, přestože
% by v ideálním případě mělo být součástí databází. Důvodem je silně různorodá
% povaha textových dat s nejednoznačným způsobem zacházení. Pokud v databázích
% pracujeme například s čísly, pak je situace celkem snadná, protože čísla jsou
% snadno a jednoznačně porovnatelná, ať už to jsou floating nebo celá čísla. Díky
% seřaditelnosti pak můžeme použít algoritmus binární vyhledávání, nebo
% dynamickou obdobu v podobě binárních vyhledávacích stromů.
%
% Jiné datové typy jako třeba intervaly jsou složitější, ale přesto existují
% pevně definovatelné způsoby pro jejich seřazení a tedy snadné vyhledávání.
% Intervalovými daty mohou být jednorozměrná časová rozmezí, dvourozměrná
% geografická data, nebo vícerozměrná data často používaná v počítačové grafice.
% Existuje pro ně velké množství relativně efektivních algoritmů a aktivní
% výzkum. Časová data jsou na první pohled lehce seřaditelná, jenže kvůli lidským
% "obohacením", jako jsou časové zóny, letní čas, nebo více dimenzí času (čas
% platnosti, čas záznamu) se jejich zacházení v databázích komplikuje. Textová
% data generovaná lidskou řečí jsou ještě komplikovanější a často se řeší mimo
% databázové systémy. Ve výsledku používá spousta uživatelských aplikací s
% databázovou podporou ve skutečnosti dva systémy - kromě klasické databáze ještě
% speciální systém pro relevantní a efektivní textové vyhledávání.
%
% Problém s textem, který nás zajímá, je jeho nejednoznačná seřaditelnost.
% Představme si databázi lidských jmen a aplikaci určenou pro vyhledávání v nich.
% Počítá se s tím, že pokud vyhledáváme konkrétní osobu, nevíme přesně jak je
% její jméno v databázi uloženo. Kdybychom věděli, že se Jaromír Kobliha v
% databázi vyskytuje v konkrétním tvaru Jaromír Kobliha nebo Kobliha, Jaromír
% nebo Dr. Jarda Kobliha, pak jednoduše zadáme dotaz na přesnou shodu a máme
% vyřešeno. Jenže naše očekávání jsou jiná. Člověk by všechny tyto tvary jména
% vyhodnotil ekvivalentně, a tím pádem je naším úkolem vytvořit podobně chytrý
% systém anebo alespoň iluzi takového systému.
%
% Jména mohou být jednoduše rozdělena na více sloupců - křestní jméno, prostřední
% jméno, příjmení, titul(y), tak jak to známe, pokud vyplňujeme kdejaké
% formuláře. To umožní systému provést oddělený dotaz v každém tomto sloupci a
% sloučit výsledky. Nebo jména seřadit nejprve podle příjmení, pak podle jména a
% titulů naposled (Kobliha, Jaromír, Dr.), protože předpokládáme, že existuje
% méně příjmení než křestních jmen a ještě méně titulů. Tohle je ovšem jenom
% heuristika, kterou nelze aplikovat např. ve Vietnamu (Nguyen a Tran tvoří 50\%
% všech příjmení) nebo v Jižní Korey (Kim, Lee, Park a Choi tvoří 50\% všech
% příjmení). Jiné kultury nemají ani koncept příjmení, proto je takový systém
% obecně nedostatečný.
%
% Pro větší záznamy - celé dokumenty o několika stovkách až tisících slov - nelze
% ani uvažovat seřaditelnost podobně naivním způsobem. Přestože je vyhledávání v
% textu obtížné, existují možnosti, jak povahu lidského textu využít a vytvořit
% algoritmy umožňující efektivní vyhledávání.
%
% Statisticky můžeme pozorovat unikátní povahu lidského jazyka v několika statistických pozorováních.
% % Frekvenční analýza - codebreaking
% % 1, 2,3 - více jedničěk distribuce
% % zipf distribuce slov
% % redundance - deflate a jiný textový komprese
%
% [1] \url{https://en.wikipedia.org/wiki/Information_retrieval#History}
% [1] \url{https://www.theguardian.com/commentisfree/2015/apr/18/google-eu-monopoly-inquiry-too-late-to-stop}


% \section{Změny v architekturách počítačů a počítačových systémů}
% Stonebraker o databázích
% sloupcové a in-memory db
% něco o nosql
% hierarchie paměti
% ssd
% nvram

% konvergence db a fulltextu. Podobnost se sloupcovými db. Not yet because of
% reasons below. Columnar dbs for analytical slow access. tens of thousands
% columns needed for each word. If some words have short inverted list, then
% waste,because blocks have minimum size.

% \subsection{Nové potřeby ve vyhledávání}
% \subsection{Vertikální vyhledávání}
% Dle hlavního vedoucího výzkumu v Googlu, Petera Norviga, je úspěch Googlu
% založen ne na lepších algoritmech, než by měli ostatní, ale jednoduše tím, že
% má více dat.
% % přístup googlu - chytré scrapování. Přitom data existují ve zpracované formě.
% % Nedostatek vertikálního a site search? Špatné nebo obtížné open source
% % řešení?
% \subsection{Linked data}
% \subsection{Mobilní zařízení}


\chapter{Textové vyhledávání}
V této části práce budou představena teoretická východiska - tedy techniky,
kolem kterých práce staví. Budou představena některá pokročilá uživatelská
rozhraní pro vyhledávací systémy, algoritmy pro vyhledávání v textu a
rozšiřující algoritmy pro techniky, na které se práce zaměřuje.

\section{Definice problému vyhledávání}
Problém vyhledávání, kterým se zde budeme zabývat spočívá v existenci velké
datové sady (kolekce) jednotlivých položek obsahujících text, které nazveme
záznamy nebo také dokumenty. Samotné vyhledávání přichází ve chvíli, kdy
uživatel vyšle požadavek na vyhledávací systém (dotaz) sestávající z několika
slov a vzápětí od systému očekává odpověď, která mu vrátí nejpřesnější možný
výsledek nebo množinu několika výsledků seřazených dle relevance k jeho dotazu.
Možnosti uživatelského rozhraní takového systému jsou popsány v
\S\,\ref{sec:uzivatelske_rozhrani}.

\section{Povaha textových dat}
V této práci se budeme zabývat textem, který reprezentuje přirozený jazyk.
Nebudou nás zajímat náhodně vygenerovaná data s charakterem textu, textová
reprezentace jiných dat nebo sekvence DNA, na které se často používají podobné,
ale přesto rozdílné, metody. Přirozený jazyk je charakterizován několika až
překvapivými pravidly, které je dobré znát před vstupem do oblasti, která se
zpracováním těchto dat zabývá.

\paragraph{Zipfův zákon} říká, že kdybychom seřadili slova nějaké větší textové
sady podle počtu výskytů, pak pořadí jednoho takového slova $\bftt{i}$ vzhledem
k jeho frekvenci $f_i$ bude nepřímo úměrné podle vztahu \[i \propto \frac{k}{f_i}\]

Zipfův zákon charakterizuje danou textovou sadou konstantou
\bftt{k}. Např. pro jednu datovou sadu, se kterou zde budeme pracovat (databáze
filmů) jsou pěti nejfrekventovanějšími slovy \bftt{the} ($80\,999\times$),
\bftt{of} ($31\,026\times$), \bftt{a} ($24\,499\times$), \bftt{in} ($12\,226
\times$) a \bftt{la} ($11\,417\times$). Na první pohled je patrná exponenciálně
klesající frekvence s rostoucím pořadím, jejichž součin je zhruba konstantní.
Modifikace tohoto zákona, která ještě blíže odpovídá reálným datům je
Zipf-Mandelbrotův, nebo také Zipf-Paretův zákon~\cite{neumann_zipf}.

\paragraph{Heapsův zákon} popisuje druhou charakteristiku slovníku textu
přirozeného jazyka, se kterým musí algoritmy počítat. Ten empiricky vyjadřuje
počet slov dané textové kolekce v závislosti na její velikosti. Tedy kolik
unikátních slov bude text obsahovat, pokud obsahuje celkově 1\,000, 100\,000,
10\,000\,000, ... slov. Heapsův zákon stanovuje závislost \[k \times n^b\] s
klíčovou konstantou \bftt{b}, která je sice závislá na textové kolekci, ale
prakticky má hodnotu kolem $0.5$. Tedy počet unikátních slov roste jako druhá
odmocnina celkového počtu slov. Zajímavé je pozorování, že počet slov neustále
roste, přestože by se mohlo zdát, že jazyk má omezený počet slov. Čím více dat
ale máme, tím větší je šance, že se v něm budou objevovat různá jména, smyšlená
slova a překlepy, které se ale také počítají jako unikátní slova.

\section{Uživatelské rozhraní} \label{sec:uzivatelske_rozhrani}
Základním uživatelským rozhraní pro textové vyhledávání je obyčejný řádek, do
kterého uživatel napíše svůj dotaz a po potvrzení mu vyhledávač obratem zašle
sadu záznamů (dokumentů), které ohodnotí jako nejrelevantnější vzhledem k jeho
dotazu.  Tento koncept se silně osvědčil díky svojí jednoduchosti. Některé
vyhledávače rozšiřují tento jednoduchý koncept například tím, že s uživatelem
reagují už během jeho vkládání dotazu a nečekají na potvrzení.

% Tahle vlastnost se začala
% rozšiřovat na počátku tisíciletí, kdy začaly webové stránky více používat
% techniku AJAX - tedy komunikace webové stránky se serverem bez toho, aniž by
% uživatel explicitně vyslal požadavek.

Vyhledávače interpretují dotaz nejčastěji tak, že text rozdělí na slova a
naleznou všechny dokumenty, které obsahují všechna tato slova zároveň.
Kdybychom si chtěli dotaz představit v explicitní formě, v jaké ji vidí
vyhledávač, pak by např. dotaz \bftt{forrest gump} vypadal jako \bftt{forrest
AND gump}, kde operátor \bftt{AND} označuje konjunkci.

Záleží na vyhledávačích, jaké další funkce uživateli poskytnou. Mezi populární
patří operátory \bftt{NOT} a frázové vyhledávání. Operátor \bftt{NOT} se
vztahuje ke konkrétnímu slovu a má efekt, že tohle slovo nesmí být přítomno v
nalezených dokumentech. Frázové vyhledávání slouží k odfiltrování výsledků,
které nemají slova fráze těsně vedle sebe. Ačkoliv jsou tyto dva operátory
takřka standardem, přesto jsou v různých vyhledávačích implementovány různým
způsobem, např. právě slovem \bftt{NOT} nebo symbolem \bftt{-}, a frázové
vyhledávání se interpretuje různě v závislosti na vyhledávači. Navíc samotné
vyhledávače obměňují svoji rozšířenou funkcionalitu, a proto i operátory, které
dříve fungovaly a spousta uživatelů si na ně zvykla, buďto už nefungují, nebo
nesplňují dřívější
očekávání~(např.~\cite{google_operator_kill1,google_operator_kill2}).

Ve výsledku jsou rozšiřující funkce vyhledávačů vlastností pro stálé a
pokročilé uživatele. Pokud tedy implementujeme nový vyhledávač, který nemá
letitou stálou základnu uživatelů, musíme implementovat pokročilé vlastnosti
bez použití pokročilých operátorů. Tedy vyhledávač dostane pouze základní
textovou informaci, kterou pak musí inteligentně interpretovat.

\subsubsection{Inkrementální vyhledávání} \label{sec:inkrementalni_vyhledavani}
Jednou technikou, kterou může vyhledávací systém implementovat bez nutnosti
učit uživatele svým pokročilým funkcím, je vyhledávání ještě před tím, než
uživatel dokončí svůj dotaz. Této technice se říká mnoha způsoby - kromě
inkrementálního vyhledávání také instantní vyhledávání, typeahead search,
search as you type, a další. Pro uživatele je tento způsob interakce pohodlný
jednak v tom, že nemusí dokončovat svůj dotaz, a jednak že dokonce nemusí znát
svůj dotaz úplně, a přesto mu vyhledávač nalezne odpověď. Inkrementální
vyhledávání zkracuje interakční smyčku s uživatelem. Ten namísto opakovaného
vkládání a potvrzování nového dotazu, pokud mu nevyhovují výsledku, pouze mění
svůj dotaz a vyhledávač reaguje instantně.

V plné formě by měl inkrementální vyhledáváč totožné rozhraní pro výsledky,
které uživateli prezentuje ještě před dokončením a zároveň pro ty, které
uživateli vrátí po potvrzení dotazu. Nejen uživatelské rozhraní by bylo stejné,
ale i množina výsledků a jejich pořadí, které v obou případech vrátí. Ve slabší
formě má dvě rozhraní. Kromě běžného rozhraní pro prezentaci výsledků má ještě
tzv. našeptávač, který reaguje instantně s každým nově zadaným písmenem dotazu.
Díky oddělení obou rozhraní nemusí našeptávač vracet tytéž výsledky, jaké by
vyhledávač vrátil po potvrzení - proto je to pouze našeptávač.

Plná forma inkrementálního vyhledávání (prefixové vyhledávání) je striktně
náročnější na výpočetní výkon, protože vykonává plný dotaz pro každé písmeno
dotazu. To je možné řešit ukládáním průběžných výsledků nedokončených dotazů do
mezipaměti s krátkou žívotností.  Populární je slabší forma inkrementálního
vyhledávání využívající dvou uživatelských rozhraní, která vykoná pouze
výpočetně méně náročnou verzi dotazu při každém novém písmenu.  Jejím příkladem
by mohlo být vyhledávání pouze v titulcích nebo názvech dokumentů při
našeptávání a plné vyhledání v celé datové sadě po potvrzení dotazu.

Jiným způsobem je využití záznamu všech dotazů uživatelů (query log). Toho
využívají velké webové vyhledávače, které díky velkému počtu svých uživatelů a
velkého množství jimi vykonaných dotazů mohou tuto informaci zpětně zabudovat
do našeptávače. Statistickými metodami pak určí nejlepší možné dokončení
částečně zadaného dotazu~\cite{Bar-Yossef:2008:MSE:1453856.1453868}.

Prefixově interpretovaný dotaz může vyhledávač vyhodnotit dvěma základními
způsoby. V prvním každé zadané slovo dotazu uvažuje jako prefix nedokončeného
slova. Např. dotaz \bftt{motor pil} bychom interpretovali prefixově v notaci
\bftt{motor* pil*} a očekávali od něj výsledek např. \bftt{motorová pila}. V
druhé variantě interpretujeme prefixově pouze poslední slovo, tedy \bftt{motor
pil*}. Odůvodněním je, že očekáváme, že uživatel vkládá nová písmena na konec
dotazu a ne, že je vkládá v náhodném pořadí.

\subsection{Přibližné vyhledávání}
V některých případech je žádoucí, když vyhledávač pomáhá vyhledávat navzdory
morfologii jazyka. Při prohledávání celých textových dokumentů je pro užívatele
frustrující situace, kdy hledá výraz - nejčastěji v prvním mluvnickém pádu -
ale výskyt fráze je v dokumentu v jiném pádu nebo čísle. Přibližným (fuzzy)
vyhledáváním se také rozumí, pokud dokáže systém napovědět, nebo rovnou
vyhledat výsledky, přestože dotaz obsahuje překlepy (např. \bftt{metamatyka}
$\rightarrow$ \bftt{matematika}), zdvojená písmena nebo naopak (\bftt{forest
gump}  $\rightarrow$ \bftt{forrest gump}), nesprávné rozdělení slov (\bftt{i
phone}  $\rightarrow$ \bftt{iphone}), inteligentní nahrazování některých slov
jejich synonymy, nebo expanze zkratek na plný výraz (\bftt{U.S.  $\rightarrow$
United States}).

Některé techniky pro přibližné vyhledávání je obtížné implementovat, protože
závisí na podrobné analýze textu. Např. přibližná shoda slov na základě
morfologie závisí na správné detekci jazyka dokumentu, nebo dokonce
jednotlivých slov, protože v dnešním globalizovaném světě není netypické
míchání cizích slov do textu jiného jazyka.  Nahrazování slov synonymy a
expanze zkratek mohou být závislé na oblasti, ze které dokumenty pocházejí, a
vyžadovat definování nahrazovacích pravidel experty z oblasti. Oproti tomu
kontrolování překlepů je relativně snadno implementovatelné, protože je závislé
pouze na znalosti kompletního slovníku datové kolekce.

\subsection{Filtrované vyhledávání} \label{sec:faceted_search}
Textové vyhledávání může být kombinováno s různými filtry pro vlastnosti, které
výsledky obsahují. Dokonce ještě přes započetím vkládání dotazu může být
uživateli prezentována omezená množina celé kolekce, seřazená např. podle
popularity v poslední době. Vedle této množiny mohou být uživateli představeny
různé filtry nebo všechny (nebo alespoň nejpopulárnější) kategorie, kterými lze
datovou sadu rozdělit. Uživatel pak dokonce nemusí ani vepisovat dotaz a pouze
se k výsledkům dopracuje postupnou interakcí s nabízenými filtry, které
postupně reflektují zmenšující se množinu. Tedy např. z nabídky filtrů zmizí
všechny kategorie, které nemají v omezené množině zastoupení. Pole pro
vyhledávání pak lze chápat jako pouze další filtr. Tomuto rozhraní se také říká
facetové vyhledávání~\cite{tunkelang2009faceted}.


\newpage
\section{Algoritmy pro textové vyhledávání}
Implementace a výběr algoritmů závisí na vlastnostech, které po vyhledávači
požadujeme a na povaze dat, ve kterých bude vyhledávat. Budeme předpokládat, že
datová sada, ve které se bude vyhledávat, je dostupná celá nebo její většina
ještě před tím, než dojde k prvnímu dotazu. V takovém případě hovoříme o
offline vyhledávání a můžeme použít čistě statické algoritmy.  Opakem by byla
situace, kdy text přichází jako proud jednotlivých záznamů, a v tomto případě
hovoříme o tzv. online problému. Pokud bychom neměli možnost uchovat všechny
tyto přicházející záznamy v nějaké datové struktuře, ale pouze nějakou omezenou
část, pak bychom museli použít tzv. streamovací algoritmy. Pokud bychom mohli
uchovat všechny záznamy, ale neměli bychom je všechny dostupné předem, pak
bychom datovou strukturu budovali postupně a využili k tomu dynamické algoritmy
(v kontrastu ke statickým). Oproti statickým datovým strukturám musí být
dynamické přizpůsobeny na postupné přidávání nových záznamů, jejich úpravy a
odebrání a zároveň vykonávat dotazy přícházející od uživatelů. Statická datová
struktura je jednou vytvořena a dále se nemění - je tedy vytvořena pouze pro
čtení.

Dynamické algoritmy jsou obtížné na implementaci, protože musí neustále
synchronizovat přístup do datových struktur zvlášť pro zápis a pro čtení.
Naopak statické algoritmy a datové struktury lze maximálně přizpůsobit datům,
např. provedením těsné komprese dat, která by v případě dynamické datové
struktury ztěžovala vkládání nových záznamů. Algoritmy pro textové vyhledávání
mají často statický charakter, protože se velká část dat nemění. Nové záznamy
nebo úpravy stávajících záznamů lze proto uchovat ve vejdlejší a menší
dynamické datové struktuře a při dosažení určité velikosti provést sloučení s
větší statickou v jednorázové dávkové operaci. Sloučení nemusí vyústit pouze v
jednu velkou datovou strukturu, ale může se slučovat postupně. Takové datové
struktuře říkáme vícegenerační, protože je tvořena několika generacemi, ve
kterých musíme paralelně provést vstupní dotaz a následně sloučit výsledky.

\subsection{Index}
Textová data, ve kterých se vyhledává, můžeme ponechat v původní textové
reprezentaci, nebo ji zakódovat tak, aby se urychlilo vyhledávání. Pokud
uvažujeme text jako posloupnost slov, pak je jednoduchým indexovacím schématem
převést každé slovo v textu na nějaké celé číslo a uchovat si k němu pozici, na
které se v původním textu nachází. Ntice (identifikátor dokumentu,
identifikátor slova, pozice v dokumentu) bude unikátně označovat jeden výskyt.
Informace o překladu ze slov na konkrétní číslo (identifikátor) bude uložena v
datové struktuře, kterou označíme jako slovník (lexikon).

Výhodou uchování datové sady v indexovaném formátu bude její menší velikost,
čehož dosáhneme navíc díky kompresi, která může využít nerovnoměrné povahy
textových dat (podle zipfova zákona). Často se vyskytujícím slovům mohou být
přiřazena malá čísla a naopak unikátnějším slovům větší čísla. Menší velikost
datové struktury bude znamenat, že se jí více vejde do paměti, což umožňuje
vyhnout se pomalým dotazům na externí perzistentní úložiště.

Vyhledání slova v takové datové struktuře znamená převod vstupních slov na
korespondující identifikátory pomocí slovníku a následný průchod celou datovou
kolekcí. Při průchodu si zaznamenáme všechny dokumenty, které obsahují zároveň
všechna dotazovaná slova a jejich pozice v dokumentu.

Indexu, který si ponechává informaci o pozicích výskytu, říkáme poziční index.
Ten je ekvivalentní s původními daty ve smyslu, že je i při zamíchání ntic
výskytu můžeme zpětně zrekonstruovat. Pozice ale nutně ukládat nemusíme
(nepoziční index), a v takovém případě bude index odpovídat množině slov pro
každý dokument.

% TODO codes.

\subsection{Invertovaný index} \label{sec:inverted_index}
Protože je typickým dotazem velmi krátký text v porovnání s dlouhým dokumentem,
průchod obyčejným indexem od začátku do konce vrátí pouze malé množství
nalezených dokumentů poměrně k počtu všech dokumentů. Invertovaný index
(inverted index) je datová struktura, která tohohle využije, protože umožňuje
rychle nalézt pouze ty dokumenty, které obsahují konkrétní slovo.
% TODO průměrná délka dotazu google, yahoo, atd.

Obyčejný index lze převést na invertovaný index tak, že k sobě sloučíme všechny
výskyty - tedy páry (id\_dokumentu, id\_slova, pozice) - podle identifikátoru
slova. Vyhledání dokumentů v takto upraveném indexu proběhne jednoduše tak, že
pro dotazované slovo vyhledáme jeho konkrétní sloučenou skupinu výskytů. Tyto
skupiny (invertované seznamy) jsou prakticky množinou dokumentů, ve kterých se
slovo vyskytuje. Vyhledání více slov znamená pouze dohledání korespondujících
invertovaných seznamů a jejich sloučení provedením zvolené množinové operace.
Protože je možné uchovat invertované seznamy seřazené podle identifikátoru
dokumentu, je sloučení velmi rychlou lineární operací nad více invertovanými
seznamy.

Indexu se říká invertovaný, protože se efektivně v ntici výskytu zamění pořadí
dokumentu a slova a obyčejný index se seřadí dle tohoto nového pořadí. Pro
kontrast k invertovanému indexu budeme nazývat obyčejný index jako dopředný
index (forward index). Invertovaný index by měl být znám všem, kdo čtou knihy,
protože rejstřík na posledních stránkách není ničím jiným než právě
invertovaným indexem, kde jedním záznamem je stránka knihy.

Invertovaný index má ještě lepší kompresní vlastnosti než dopředný index,
protože může místo instantních kódů používat efektivnější blokové kompresní
metody, které drasticky sníží velikost invertovaných seznamů odpovídajících
často se opakujícím slovům. Má však horší kompresní vlastnosti pro záznamy o
pozicích výskytu, protože se v tomhle schématu nevyskytují striktně za sebou.
Komprese bude detailněji popsána dále v \S\,\ref{sec:compression}.


\subsection{Prefixové vyhledávání} \label{sec:prefix_search}
Instantní vyhledávání můžeme implementovat více způsoby. Nejjednodušším je
zaindexování všech možných prefixů (předpon slova) a slova samotného. Tedy
např. pro slovo \bftt{podlaha} se zaindexují prefixy \bftt{p}, \bftt{po},
\bftt{pod}, \bftt{podl}, \bftt{podla}, \bftt{podlah}, \bftt{podlaha}. Při
vyhledávání není nutné uživatelem nedokončený dotaz nijak upravovat, protože
při existující shodě v dokumentech bude odpovídající výraz v takovém indexu
přítomný. Nevýhodou je několikanásobná velikost výsledného indexu, protože se
efektivně indexuje kvadratické množství slov. Často se velké množství omezuje
minimální a maximální délkou prefixů¸ které se zaindexují. V takovém případě
pak zjevně vyhledávač nemůže vyhledávat při krátkých dotazech a nebude přesný
pro delší prefixy.

Jiným způsobem je použít klasický invertovaný index beze změny a vypočítat
prefixy až při dotazování. Docílíme toho přepsáním dotazu do formy konjunkce
disjunkcí. Např. dotaz \bftt{motor* pil*} by byl přepsán do tvaru
\[\bftt{(motorová OR motorových OR ...) AND (pila OR pil OR pilách OR ...)}\]
podle všech odpovídajících slov ve slovníku. Problém nastane u krátkých
prefixů, protože odpovídají velkému počtu slov. Dotaz s disjunkcí tisíců
invertovaných seznamů může být i méně výkonný než prostý průchod celou datovou
kolekcí. Řešením velké disjunkce je poloinvertovaný index popsaný v sekci
\S\,\ref{sec:half_inverted}.

\section{Algoritmy pro přibližné vyhledávání}
Tato práce se zabývá přibližným vyhledáváním pouze z čistě textového pohledu.
Text je považován pouze za posloupnost znaků, slov nebo jejich kombinaci, ale
nijak neuvažuje v potaz význam slov. Tedy nejsou uvažovány techniky jako
hledání synonym nebo expanze zkratek, které silně závisí na kontextu a na
obsahu dat.

\subsection{Stematizace}
Velkým problémem textových vyhledávačů je tvarosloví jazyka (morfologie - různé
tvary slov, např. dobrý, dobrou, dobrému, dobrých). Například při dotazování
vyhledávače na výraz \bftt{motorová pila} nás jistě budou zajímat i výsledky,
které obsahují text \bftt{motorové pile} nebo \bftt{motorových pilách}. Nelze
po uživateli požadovat, aby vyjmenoval všechny tvary slov, které zadává.

V češtině a v dalších indoevropských jazycích dochází k nejvíce morfologickým
změnám na koncích slov. Klasickým řešením je před indexováním dat provést
jazykovou analýzu a převést slova na kmenový tvar (stematizace). Ve výsledku
místo slova \bftt{motorových} zaindexujeme kmen \bftt{motor} a poté při
vyhledávání se provede identická konverze dotazovaných slov. Vyhledávač by tedy
původní výraz \bftt{motorová pila} nejprve převedl na kmenový tvar \bftt{motor
pil} a vykonal dotaz s tímto modifikovaným výrazem. Co se považuje za kmen
slova je určeno vybraným algoritmem. Kromě stematizace existuje ještě technika
lemmatizace, která místo kmenu slova hledá jeho základní (slovníkový) tvar.

V jazykové analýze existuje mnoho problémů. Především je závislá na konkrétním
jazyce. Některé datové kolekce navíc obsahují záznamy ve více jazycích. Pokud
je text dostatečně dlouhý nebo obsahuje znaky specifické jen pro daný jazyk,
lze jazyk záznamu detekovat frekvenční analýzou před morfologickým rozborem. V
jiném komplikovaném případě může textová sada obsahovat záznamy v jednom
jazyce, ale jednotlivá slova mohou být v jiném. Pak jednoduchá frekvenční
analýza nebude dostačovat a museli bychom zvolit složitější techniky pro
jazykový rozbor, které jsou ale mimo rozsah této práce.

Problém se stematizací a lemmatizací je navíc nejednoznačnost základního tvaru.
Například slovo \bftt{tancích} může mít základní tvar \bftt{tank} nebo
\bftt{tanec} podle významu slova, který by člověk určil z kontextu věty.

\subsection{Editační vzdálenost}
Obecně můžeme podobnost dvou textových řetězců měřit pomocí tzv. editační
vzdálenosti (edit distance). Základní editační vzdálenost měří minimální počet
písmen, které musí být odebrány nebo přidány, aby z prvního řetězce vzniknul
druhý. Např. mezi slovy \bftt{karel} a \bftt{kremrole} je editační vzdálenost
5, protože bychom první slovo přeměnili na druhé odebráním znaku \bftt{a}
a přidáním znaků \bftt{m},~\bftt{r},~\bftt{o}~a~\bftt{l} na patřičná místa.
Základní editační vzdálenost můžeme alternativně vypočítat jako \[\bftt{edit}(x,
y) = |x| + |y| - 2\,\bftt{lcs}(x, y)\]

kde $\bftt{lcs}$ (longest common subsequence) označuje nejdelší společný
podřetězec textů $x$ a $y$ a $|x|$ znamená délku textu $x$. Tato metrika se
proto také označuje jako vzdálenost nejdelších společných podřetězců.

\paragraph{Levenshteinova vzdálenost} je rozšířením~\cite{Levenshtein66}, které
kromě odebrání a přidání uvažuje navíc záměnu písmen (substituce) za jednu
operaci s váhou 1.  Prakticky se liší pouze tím, že záměnu písmen penalizuje
vahou 1, zatímco základní editační vzdálenost bere záměnu jako jedno přidání a
jedno odebrání, tedy s penalizací 2. Pokud se běžně hovoří o editační
vzdálenosti, je jí myšlena právě ta Levenshteinova, proto budeme označovat
pojmem editační vzdálenost právě ji.

\paragraph{Damerau-Levenshteinova vzdálenost} je druhým
rozšířením~\cite{Damerau:1964:TCD:363958.363994}, které navíc uvažuje záměnu
dvou sousedících písmen (transpozice) jako operaci s vahou 1. Bez této podmínky
by byla vzdálenost transpozice vypočtena jako jedno odebrání a jedno přidání
celkem s váhou 2.

\paragraph{Prefixovou editační vzdálenost} definujeme abychom mohli zkombinovat
inkrementální vyhledávání s tolerancí překlepů. Vypočteme ji jako nejmenší
editační vzdálenost prvního řetězce k prefixům druhého \[\bftt{p\_edit}(x, y) =
\min_{p \in prefix(y)} \bftt{edit}(x, p)\]

kde $prefix(y)$ označuje množinu všech prefixů řetězce
$y$~\cite{Bast:2013:EFS:2457465.2457470}. Př.  $\bftt{p\_edit}(ker, karel) =
1$, protože $\bftt{edit}(ker, kar) = 1$. Oproti předchozím metrikám není
symetrická v $x$ a $y$.


\subsection{Výpočet editační vzdálenosti}
Pro obyčejnou, Levenshteinovu i Damerau-Levenshteinovu vzdálenost se běžně
používá Wagner-Fischerův tabulkový
algoritmus~\cite{Wagner:1974:SCP:321796.321811} s kvadratickou náročností
$\Theta(|x||y|)$, kde $|x|$ a $|y|$ jsou délky dvou porovnávaných řetězců.
Algoritmus byl nezávisle objeven více lidmi (např.  Vintsyuk, Needleman -
Wunsch, a další)~\cite{Navarro:2001:GTA:375360.375365}.  Obdobný tabulkový
algoritmus se používá pro výpočet nejdelšího řetězce se stejnou kvadratickou
náročností, proto nepomůže výpočet přes tuto alternativní cestu. Pro vyšší
výkon můžeme použít Myersovu bitově paralelní adaptaci tohoto
algoritmu~\cite{Myers:1999:FBA:316542.316550}.

\subsection{Hranice pro editační vzdálenost}
Ohodnocení všech položek slovníku editační vzdáleností ku dotazovanému slovu je
pro naše potřeby zbytečně obecný problém. Nám bude stačit nalezení slov do
určité vzdálenosti, jehož výpočet je efektivnější a hlavně při správně zvolené
datové struktuře nemusí vypočítávat vzdálenost ke všem slovům.

Omezení se provádí adaptivně podle délky slova. Vysoká tolerance pro krátká
slova by znamenala spoustu shod mezi slovy, které by ale uživatel nevyhodnotil
jako podobné. Např. vzdálenost mezi krátkými slovy \bftt{na} a \bftt{ty} je 2,
přitom spolu nemají slova pranic společného. Naopak u delších slov je větší
šance, že dojde k překlepům, proto pro ně můžeme práh zvýšit. Dokonce pro velmi
dlouhá slova bude platit, že ve slovníku bude pouze omezené množství dlouhých a
zároveň podobných slov, proto si můžeme dovolit limit nastavit poměrně vysoko.

Jako řešení můžeme definovat relativní editační vzdálenost jako
\[\bftt{edit\_rel}(x, y) = \frac{\max (|x|, |y|) - edit(x, y)}{\min (|x|,
|y|)}\]
která zohlední délku obou slov. Nebo jednoduše nastavit konfigurovatelnou
hranici podle délky vyhledávaného slova. Např. pro slovo delší než 10 znaků se
použije limit vzdálenosti 4, pro slova delší než 7 limit 3, apod.

Díky omezení maximální editační vzdálenosti lze snížit náročnost algoritmu.
Místo vypočítání celé tabulky vypočteme pouze hodnoty ve vzdálenosti $k$ od
diagonály s náročností $\mathcal{O}(\min (|x|, |y|) \cdot
k)$~\cite{Ukkonen:1985:AAS:4620.4626}.


\subsection{N-gramová podobnost}
Pro přibližné vyhledávání můžeme indexovat jiné díly původního textu než
jednotlivá slova. N-gram (také q-gram) je posloupnost několika po sobě jdoucích
znaků. Např. n-gramy slova \bftt{podlaha} o délce 3 (trigramy) jsou \bftt{pod},
\bftt{odl}, \bftt{dla}, \bftt{lah}, \bftt{aha}. V tomto případě se jejich znaky
překrývají, a proto hovoříme o překrývajících se trigramech.

Podobnost dvou řetězců pomocí n-gramů můžeme efektivně zjistit pomocí
Jaccardovy podobnosti množin (coherence). Tu zjistíme jako poměr velikosti
průniku ku velikosti sjednocení obou množin \[\bftt{jacc}(X, Y) = |X \cap Y|\
/\ |X \cup Y|\]

Pokud porovnávané texty reprezentujeme jako dvě množiny jejich n-gramů,
vypočteme jejich podobnost jako podobnost těchto množin. Jaccardova podobnost
může dosahovat pouze hodnot od $0$ do $1$ a díky tomu můžeme jednoduše odvodit
vztah pro Jaccardovu vzdálenost jako $1 - \bftt{jacc}$.

Tento způsob funguje pro krátké řetězce a nízkou toleranci odlišnosti. Podobná
slova budou sdílet všechny n-gramy vzdálené $n$ pozic od místa, kde mezi nimi
došlo ke změně. Např. téměř totožná slova \bftt{kremrole} a \bftt{kremrule} s
Levenshteinovou vzdáleností 1 budou mít Jaccardovu podobnost trigramů pouze
$0.5$. Aby metoda fungovala, musí být text dlouhý alespoň dvakrát delší než
velikost n-gramu, pokud ke změně dojde uprostřed textu. Proto tenhle způsob
selhává u kratších slov.

Kromě Jaccardovy podobnosti můžeme použít i další metriky pro podobnost množin
jako euklidovskou, kosínovou nebo Jaro-Winklerovu vzdálenost, pokud převedeme
množiny na vektory, kde jedna dimenze označuje přítomnost jednoho znaku nebo
n-gramu textu (vector space model pro textové řetězce).

% Vztah mezi Jaccardovou metrikou a základní editační vzdáleností je
% $\bftt{jacc}(x, y) = 1 - |\bftt{lcs}(x, y)|\ /\ (|\bftt{lcs}(x, y)| +
% \bftt{edit}(x, y))$, pokud jsou prvky množiny jednotlivá písmena textu.

\subsection{Reprezentace slovníku}
Pokud zvolíme jako fuzzy techniku stematizaci, pak lze implementaci slovníku
pro invertovaný index provést jednoduchou hashovací tabulkou, která si u
zaznamenaných kmenů slov poznamená všechny nalezené původní tvary slova. V
praxi se původní slova často zahazují a nerozlišuje se ve výsledcích
vyhledávání rozdíl mezi přesným zásahem a odvozeným tvarem. Pro slovník
n-gramového indexu rovněž postačí obyčejná hashovací tabulka.

\subsection{Prefixový slovník} \label{sec:prefix_dict}
Pro nalezení všech slov v prefixovém indexu, které odpovídají jednomu prefixu
můžeme použít reprezentaci slovníku seřazeným seznamem
slov~\citep[kap.~4]{buttcher2010information}. V něm nalezneme první a poslední
slovo, která odpovídají prefixu. Všechna slova mezi nimi jsou výsledkem
hledání. Tuto reprezentaci můžeme efektivně komprimovat technikou front-coding,
která rozdělí seřazený seznam na bloky o konstantní velikosti (např. 8) a pro
každý blok uloží nejdelší prefix, který sdílí všechna slova bloku. Technika
dobře funguje pro slova přirozeného jazyka, protože díky tendenci výskytu
morfologie ke konci slov se ve slovníku bude objevovat značné množství silně
komprimovatelných úseků jako např. \bftt{hody}, \bftt{hodnota}, \bftt{hodnoty},
\bftt{hodný}, \bftt{hodná}, \bftt{hodnému}, atd. Implementace obyčejnou
hashovací tabulkou je pro prefixy nevhodná, protože si neuchovává informaci o
lexikografickém pořadí slov.

\subsection{Fuzzy slovník}
U n-gramového přibližného vyhledávání se přibližnost vyhodnocuje až po získání
množiny všech n-gramů v dokumentu. Naopak editační vzdálenost se může
vyhodnotit už ve slovníku ještě před přístupem do invertovaných seznamů.
Existuje více datových struktur, které umožňují efektivní vyhledání všech slov
do určité vzdálenosti od hledaného - tedy bez vyčerpávajícího průchodu celým
slovníkum.

\subsubsection{Metrické stromy}
Protože editační vzdálenost splňuje trojúhelníkovou nerovnost
\[\bftt{edit}(x,y) \leq \bftt{edit}(x,z) + \bftt{edit}(z,y)\]

lze pro hledání v prostoru řetězců použít tzv. metrické stromy, které díky ní
efektivně prořezávají prostor s objekty, které lze danou metrikou porovnat.
Burkhard-Kellerův strom (BK-strom)~\cite{Burkhard:1973:ABF:362003.362025} je
metrický strom, který se používá pro diskrétní metriky, jakou je právě editační
vzdálenost.

Představme si, že strom ve výchozím stavu obsahuje pouze jediné slovo (kořen
stromu). Vložení druhého slova proběhne tak, že se mezi těmito slovy vypočte
jejich vzdálenost \bftt{d} a nové slovo se vloží jako potomek se štítkem
\bftt{d}. Další slova se vloží stejným způsobem, pouze s rozdílem, že když
dojde ke kolizi - tedy potomek se štítkem \bftt{d} už existuje - vloží se nové
slovo rekurzivně jako potomek tohoto slova. Slovník sestavíme vložením všech
slov z datové sady.

Vyhledání začíná porovnáním dotazovaného slova s kořenem stromu. Pokud je
vzdálenost mezi těmito slovy \bftt{d} menší než zvolený limit \bftt{k}, bude
vráceno jako jeden ze zásahů. Vyhledávání pokračuje stejným způsobem rekurzivně
pro všechny potomky tohoto slova, které nesou štítek v rozsahu \bftt{d - k} až
\bftt{d + k}. Důvod pro tento rozsah vyplývá z trojúhelníkové nerovnosti.

Metody využívající rozdělování metrického prostoru jsou pro Levenshteinovu
vzdálenost méně časově i podle paměťové náročnosti výkonné než jiné algoritmy,
které budou představeny
dále~\cite{Celikik:2009:FES:1529282.1529669,Boytsov:2011:IMA:1963190.1963191}.

\subsubsection{Hashovací metoda} \label{sec:hash_fuzzy}
Dvě podobná slova sdílejí netriviální nejdelší společný podřetězec (lcs). Slova
zaindexujeme tak, že do hashovací tabulky vložíme všechny jejich odvozené lcs
podřetězce. Ty pro každé slovo získáme odstraněním všech podmnožin písmen do
požadované velikosti. Např. pokud slovník omezíme pro vyhledávání do
vzdálenosti 2, pak do tabulky vložíme jako klíč všechna odvozená slova, která
vzniknou odstraněním všech $n$ písmen a všech jejich ${n \choose 2}$ párů. Jaho
hodnotu klíče v tabulce použijeme původní slovo. Např. slovo \bftt{ruka}
zaindexujeme odvozenými podřetězci \bftt{uka}, \bftt{rka}, \bftt{rua},
\bftt{ruk}, \bftt{ka}, \bftt{ua}, \bftt{uk}, \bftt{ra}, \bftt{rk}, \bftt{ru}.
Při hledání podobných slov provedeme stejnou operaci s dotazovaným slovem a
zkusíme najít shodu v tabulce pro každý jeho odvozený podřetězec. Každý
nalezený zásah prověříme vypočtením vzdálenosti klasickým tabulkým algoritmem,
protože tahle metoda může přestřelit. Např. $\bftt{edit}(ruka, karu) = 4$,
přestože by došlo k zásahu na podřetězcích \bftt{ru} a \bftt{ka} vzniklých
odstraněním pouze dvou znaků.

Tato metoda je jednou z nejrychlejších, ale za cenu obrovské náročnosti na
paměť, jelikož je pro maximální strop editační vzdálenosti \bftt{k} každé slovo
zaindexováno $\sum_{i \leq k} {n \choose i}$~krát.

Původní algoritmus  využívající tuto techniku pro překlepy do vzdálenosti 1
(Mor - Fraenkel) byl popsán v~\cite{Mor:1982:HCM:358728.358752}. Zobecnění pro
větší vzdálenosti bylo popsáno nezávisle na sobě jako FastSS~\cite{FastSS},
Boytsov~\cite{Boytsov:2011:IMA:1963190.1963191} a Symmetric
Delete~\cite{Faroo_symmetric_delete}. Několik variant algoritmu je popsáno
v~\cite{FastSS} a~\cite{Bast:2013:EFS:2457465.2457470} popisuje modifikovanou
kompaktnější verzi s názvem DeleteMatch, která řeší problém s velikostí.

\subsubsection{Trie} \label{sec:trie}
Populární datovou strukturou, do které lze relativně kompaktně uložit sadu slov
a která podporuje rychlé vyhledávání je trie, také známá jako prefixový strom.
Jedno slovo do trie uložíme tak, že ho rozdělíme na písmena a každé z nich
vložíme jako jeden uzel cesty ve stromu. Např. slovo \bftt{ruka} a \bftt{ruda}
se uloží jako \bftt{/r/u/k/a} a \bftt{/r/u/d/a}, pokud použijeme notaci pro
zápis cest v hierarchii souborového systému. V tomto případě budou v
\uv{adresáři} \bftt{u} dva podadresáře \bftt{k} a \bftt{d}. Trii lze
komprimovat tím, že se sloučíme všechny cesty, které mají pouze jednoho
potomka. Stejný příklad by byl v komprimované trii reprezentován cestami
\bftt{/ru/ka} a \bftt{/ru/da}, kde adresář \bftt{/ru} obsahuje dva podadresáře
\bftt{ka} a \bftt{da}. Úspory paměti je docíleno sdílením prefixů. Trie je v
podstatě kompresní technika front-coding (viz~\S\,\ref{sec:prefix_dict})
aplikovaná rekurzivně, a proto má tahle datová struktura pro prefixové
vyhledávání podobné vlastnosti jako seřazený seznam slov. Pro přibližné
vyhledávání je ještě efektivnější, protože pro jakýkoliv prefix je schopna
velmi rychle vrátit všechny potomky.

Výpočet editační vzdálenosti mezi dvěma slovy proběhne standardně tabulkovou
metodou. U vyhledání všech podobných slov ale proběhne pouze jednou pro řádky
tabulky u všech slov, které odpovídají sdílenému prefixu mezi těmito slovy.
Např. pokud v trii z předchozího příkladu hledáme slovo \bftt{luka}, pak můžeme
vyhodnotit editační vzdálenost mezi prefixy \bftt{lu} a \bftt{ru} pro obě slova
\bftt{ruka} a \bftt{ruda} a dokonat zbytek výpočtu pro obě slova zvlášť.

Při průchodu trií jsou jednotlivé větve postupně prořezávány, pokud hodnota
editační vzdálenosti v tabulce překročí pro danou větev stanovenou hranici.
Nejefektivnější je algoritmus v případě, pokud nevyhledáváme slova od prvního
písmene slova. Pokud začíná vyhledání od prvního písemene, musí se otestovat
všechny větve odpovídající prvním písmenům, protože i na nich může dojít k
překlepu. Prohledání stromu od druhého písmene je o poznání efektivnější,
protože už se uvažuje pouze podstrom, který odpovídá stejnému prvnímu písmenu,
které má dotazované slovo. Tento jev může být využit více způsoby.

Prvním je datová struktura pojmenovaná FB-trie (Forward -
Backward)~\cite{Mihov:2004:FAS:1105587.1105590}, která sestává z dvou instancí
trie. První je stejná jako ta výše popsaná a druhá pouze s rozdílem, že
zaindexuje slova v obráceném pořadí. Díky tomu, že prohledávání v trii je
výhodné až od několikátého písmene, využije se druhé trie, která nalezne zbylá
slova po odřezání větví prvního písmene v první trii. Podle experimentálního
srovnání~\cite{Boytsov:2011:IMA:1963190.1963191} je FB-trie nejefektivnější
reprezentací fuzzy slovníku s praktickým slovníkem. Jiný používaný přístup pro
zefektivnění velkého větvení na začátku trie je penalizace neshody na prvních
pozicích vyšší vahou.

Druhým způsobem je nalezení slov editační vzdáleností s prefixově dynamickým
prořezáváním. Při omezení limitu pro editační vzdálenost podle délky slova
budeme pro slova delší než 4 znaky prohledávat celou trii do nějaké konstantní
vzdálenosti, např. 2. Pro slova delší než 7 znaků s limitem např. 3. Tenhle
způsob povoluje odlišnosti na začátku i na koncích slov se stejnou penalizací.
Využijeme toho, že v trii procházíme postupně prefixy dotazovaného slova, pro
které stanovíme stejné prořezávací limity, jako kdybychom postupně prohledávali
prefixy jako celá slova. Tedy při délce prefixu 3 budou prořezány všechny větve
s prefixovou editační vzdáleností vyšší než 1, při délce prefixu 5 vyšší než 2,
apod.

Tenhle způsob je omezený pro překlepy na začátcích slov, ale díky vyššímu
prořezávání můžeme stanovit vyšší limity pro delší slova. Efektem bude vyšší
tolerance překlepů na koncích slov. Změny slov, které se vyskytují v nejvyšší
míře na koncích slov v důsledku morfologie, můžeme uvažovat jako překlepy.
Tenhle způsob bude pro tyto případy výpočetně efektivní a můžeme ho použít jako
náhradu stematizace, která je nezávislá na jazyce. Podmínkou je pouze, že
uvažujeme jazyk, který má většinu morfologie na koncích slov, jakým je zejména
čeština. Tato metoda je experimentálně otestována v \S\,\ref{sec:results} jako
metoda \bftt{edit\_dyn} a \bftt{edit\_dyn\_prefix}.



\subsubsection{Jiné metody}
Kromě několika populárních metod zde popsaných poskytuje
\cite{Boytsov:2011:IMA:1963190.1963191} taxonomii algoritmů zabývající se
obecným problémem přibližného hledání v lexikonu.
\cite{Bast:2013:EFS:2457465.2457470} některé tyto metody rozšiřuje a zabývá se
algoritmy pro prefixové hledání v lexikonu.


% ngram - krátký slova
% permuterm

\subsection{Reprezentace invertovaných seznamů}
Nejjednodušším způsobem, jak uložit invertované seznamy je vytvořit jeden
soubor pro každé vyskytující se slovo. Po zaindexování celé kolekce je poměrně
snadné index upravovat, protože stačí pozměnit jen ty invertované seznamy,
které odpovídají změnám v datech. Současné operační a souborové systémy ale
nejsou přizpůsobené pro miliony malých souborů a navíc by došlo k plýtvání
místem, protože i soubory obsahující pouze jeden znak si uchovávají větší
strukturu s metadaty. Řešením je invertované seznamy uchovat sekvenčně v jednom
velkém souboru. Tenhle způsob je kompromisem mezi efektivitou při vyhledávání a
možností modifikovat index. Jeden velký soubor lze prakticky modifikovat pouze
jako celek. Za každým blokem invertovaného seznamu je mozné nechat nějaké volné
místo, a předpokládat, že úpravy budou přicházet rovnoměrně, ale i přesto bude
občas nutné provést změnu celého souboru.

Index může být rozdělen na dva podindexy. Hlavní statický, rychlý pro čtení,
ale neefektivní pro jednotlivé úpravy a vedlejší dynamický, který přijímá změny
v reálném čase a postupně je dávkově slučuje s hlavním. Hlavní index je
přizpůsobem pro vysoký výkon a kompaktnost a vedlejší je přizpůsobem pro zápis.
Efektivní způsob, jak zabránit nákladné rekonstrukci hlavního indexu je použít
množství menších indexů ve vzrůstající velikosti a změny z dynamického indexu
postupně s těmito slučovat od nejmenšího po
největší~\citep[kap.~4]{Manning:2008:IIR:1394399}.
%  Tenhle přístup používá knihovna Apache Lucene~\cite{lsm_lucene}.

\subsubsection{Komprese} \label{sec:compression}
Invertované seznamy jsou pouze seznamy identifikátorů dokumentů - tedy celých
čísel, které bývají uloženy v seřazeném pořadí. To umožňuje efektivní kompresi
tím, že uložíme pouze rozdíly mezi identifikátory (delta komprese), které budou
vždy nezáporné, protože je posloupnost vzrůstající. Tyto rozdíly budou oproti
původním identifikátorům pouze poměrně malá čísla, která můžeme zmenšit
efektivními kompresními kódy pro celá čísla.

Komprese invertovaných seznamů má několik opodstatnění. Zmenšením velikosti
indexu docílíme, že větší množství dat může být obslouženo bez jejich hledání
na pomalém persistentním úložišti, ať už je to pevný disk nebo ssd. Pokud už k
zásahu do externího úložiště dojde, pak komprimovaný seznam bude přenesen do
RAM rychleji i přes čas strávený dekódováním. Vysvětlením je úzké hrdlo při
přenosu dat (I/O bottleneck) oproti rychlosti dnešních procesorů. Algoritmy pro
dekompresi mohou být velmi rychlé a tento jev zaznamenáme i na rychlejších ssd
úložištích a dokonce i pokud uvažujeme celou operaci výhradně v paměti
počítače. Dekódování může být díky několika úrovním vyrovnávací paměti
procesoru rychlejší než přístup k nekomprimovaným datům v RAM.
% TODO cite ssd, memory hierarchy model, atd?

\paragraph{Instantní kódy} uvažují každé číslo samostatně a snaží se
zkomprimovat menší čísla na úkor větších. Příkladem jsou bitově orientované
kódy Elias $\gamma$ a $\delta$ a Golomb/Rice, nebo bytově orientované schéma
VByte. Bitové kódy jsou kompaktnější, ale bytové rychlejší pro dekódování na
současných procesorových architekturách.
% TODO cite Elias codes

\paragraph{Blokové kódy} naopak počítají při kompresi s celými bloky, které se
snaží maximálně zkomprimovat jako celek. Výhodou je vyšší komprese, protože se
využívá charakter distribuce čísel v celém bloku. Instantní kódy také mohou
využívat statistiky frekvencí jednotlivých čísel, ale pouze globálně. Blokový
kód může využít lokální distribuce čísel.

Nevýhodou blokových kódů je pomalejší přístup, pokud chceme získat pouze jeden
prvek, pak musí být prakticky dekódován celý blok.  Proto se volí kompromis a
velikost bloku je omezená pouze na několik stovek až tisíců prvků. Při
sekvenčním přístupu u slučování invertovaných seznamů se ale využije velká část
prvků bloku, proto jsou bloková schémata pro invertované indexy efektivní.
Příkladem jsou např.  kompresní rodiny Frame of Reference (bit packing, FOR,
PFor, PForDelta, ...) a v poslední době se rožšiřující velmi efektivní kódování
Elias-Fano~\cite{DBLP:journals/corr/abs-1206-4300,Curtiss:2013:USS:2536222.2536239}.

~\\
Experimentální srovnání různých typů komprese pro invertované seznamy bylo
provedeno např. v~\citep[addenda~6]{buttcher2010information}
nebo~\cite{Zhang:2008:PCI:1367497.1367550}.

\subsubsection{Poloinvertovaný index} \label{sec:half_inverted}
Velkou výpočetní zátěží u prefixových a fuzzy indexů je disjunktivní sloučení
(OR) většího množství invertovaných seznamů.  Většinou v literatuře textového
vyhledávání potkáme techniky pro konjunktivní sloučení (AND), které je naopak
tím efektivnější, čím více invertovaných seznamů v něm učinkuje.  Analogicky ke
konjunkci množin - čím více náhodných množin, tím menší je pravděpodobnost, že
bude jejich prvek ve všech najednou. U disjunkce platí, že čím více množin, tím
bude sloučená množina zpravidla větší.

Sloučení invertovaných seznamů je problém, pokud jejich množství narůstá.
Zejména to platí pro krátké prefixy, protože ve slovníku k nim budou
korespondovat tisíce slov. Problém velkého množství slov u krátkých prefixů je
i u hranových n-gramů, jenže u nich jsou invertované seznamy předsloučeny během
indexování. Právě zde přichází myšlenka poloinvertovaného (hybridního) indexu,
který během indexování spolu předsloučí ty invertované seznamy, které mají
velkou šanci, že by byly sloučeny během vykonání dotazu (materializace
invertovaných seznamů). Tahle technika je obzvlášť vhodná pro čistě prefixové
indexy bez fuzzy rozšíření. Velkou pravděpodobnost sloučení totiž mají slova,
která jsou abecedně blízko sebe.

Pokud je index rozšířený o pozicovou informaci, pak sloučením všech
invertovaných záznamů získáme zpět původní datovou sadu, pouze zakódovanou
(dopředný index). Index se nazývá hybridní, protože je hybridem mezi
invertovaným a dopředným indexem. Hybridní index je velmi podobný dopřednému
indexu v jednom z prvních popisů architektury Googlu. To, co se zde myslí
skupinou slov v hybridním indexu, odpovídá barelu (barrel) v jejich
architektuře.  Pouze s rozdílem, že oba pojmy slouží k jinému účelu. Jejich
dopředný index byla pouze mezifáze před vytvořením invertovaného indexu,
zatímco u hybridního indexu se tahle datová struktura struktura používá přímo k
vyhledávání~\cite{Brin:1998:ALH:297810.297827}.

\mbox{}
\begin{figure}[H]
\centering
\begin{tt}
\begin{tabulary}{\textwidth}{LLL}
\textbf{původní text} & \textbf{dopředný index} \\
\hline
1: \bftt{dokument text a index}         & 1: [2, 7, 1, 3] \\
2: \bftt{invertovaný index pro text}    & 2: [4, 3, 6, 7] \\
3: \bftt{dokument a text}               & 3: [2, 1, 7] \\
4: \bftt{pro text je index}             & 4: [6, 7, 5, 3] \\
\vspace{.5cm}\\
\textbf{invertovaný index + slovník} & \textbf{hybridní index} \\
\hline
1, \bftt{a}:           [1, 3]        & \bftt{a-d}: [1:1, 1:2, 2:2, 3:1] \\
2, \bftt{dokument}:    [1, 2]        & \bftt{i-j}: [1:3, 2:3, 2:4, 4:3, 4:5] \\
3, \bftt{index}:       [1, 2, 4]     & \bftt{p-t}: [1:7, 2:6, 2:7, 3:7, 4:6, 4:7] \\
4, \bftt{invertovaný}: [2] & \\
5, \bftt{je}:          [4] & \\
6, \bftt{pro}:         [2, 4] & \\
7, \bftt{text}:        [1, 2, 3, 4] & \\
\end{tabulary}
\end{tt}
\caption{Příklad typů indexu}
\label{tab:index_examples}
\end{figure}

\subsubsection{HYB} \label{sec:hyb} je typ hybridního invertovaného indexu,
který během indexování předsloučí invertované seznamy slov v abecedním
rozsahu~\cite{Bast:2006:TLF:1148170.1148234}. Předsloučené (materializované)
seznamy mohou mít rozsahy např. \bftt{[A-EN], [EN-NUL], [NUL-QU], [QU-Z]} -
tedy první invertovaný seznam by obsahoval slova začínající na \bftt{A} až
\bftt{EN}. Disjunkce dotazu v téhle struktuře je už buďto předsloučená, nebo
během vykonání dotazu za běhu sloučí pouze malé množství sousedících
materializovaných seznamů. Ke každému záznamu ve sloučeném seznamu musí být
poznamenáno, ke kterému slovu patří. Dotazy, které nevyužijí celý rozsah
seznamu se díky této dodatečné informaci vyfiltrují od slov, které zúženému
rozsahu neodpovídají.

Tím, že jsou některé seznamy sloučeny do většího, budou dotazy, které by těchto
seznamů využily, penalizovány. Naopak komplexní disjunktivní dotazy budou
efektivnější než bez materializace. V klasickém invertovaném indexu je kvůli
zipfově zákonu velký nepoměr náročnosti při zpracování krátkých a dlouhých
seznamů. Hybridní index tenhle rozdíl srovnává za cenu, že původně efektivní
dotazy budou běžet zhruba stejně dlouho, jako ty původně náročné.

Bylo by možné materializovat všechny prefixy, které se ve slovníku budou
nacházet a prefixové dotazy by díky odpadnutí disjunkcí až při dotazování byly
velmi efektivní, ale u fuzzy dotazů nelze dopředu předpovědět, které
invertované seznamy v nich budou figurovat, protože závisí na konkrétním slovu
dotazu. Tohle schéma by navíc bylo velmi paměťově náročné (na druhou stranu by
mělo být vysoce kompresibilní). HYB oproti tomu, až na nutnost uložení
identifikátoru slova ke každému výskytu dokumentu, zabírá v paměti stejně místa
jako obyčejný invertovaný index

\subsubsection{Bitmapy}
Protože představuje jeden invertovaný seznam množinu všech dokumentů, ve
kterých se vyskytuje jedno slovo, existuje jiná technika pro uložení této
množiny - bitmapa. Bitmapové kódování množiny použije jeden kladný bit na
pozici dané identifikátorem dokumentu, pokud se slovo v seznamu vyskytuje, a
záporný, pokud nikoliv. Bitmapy poskytují velmi rychlé algoritmy přesně pro ty
množinové operace, se kterými invertovaný index operuje - tedy konjunktivní
(AND) a disjunktivní (OR) sloučení. Pro jejich vykonání existují extrémně
rychlé bitově-paralelní operace zabudované přímo v instrukčních sadách
procesorů.

Tato reprezentace množiny poskytuje vysokou kompresi pro množiny s vysokou
hustotou. Velikost reprezentace je přesně $\bftt{n}\ /\ 8$ bytů, kde \bftt{n}
je počet dokumentů. Bohužel kvůli zipfově distribuci slov je většina
invertovaných seznamů velmi řídká, a proto nejsou bitmapy pro invertované
indexy používány.

Bitmapy navíc ukládají pouze množinovou informaci o existenci slova v
dokumentu, ale takto jednoduše v nich nelze uložit pozicovou informaci. To z
nich dělá nevhodného kandidáta pro potřeby vyhledávače, který ohodnocuje
výsledky podle blízkosti slov a zvýrazňuje výsledky na základě právě pozicové
informace.

U hybridního indexu se sloučenými seznamy je hustota množiny vyšší, ale přesto
z experimentálního měření není dostatečná, aby zdůvodnila použití bitmap. Pro
datovou sadu filmů a minimální velikost hybridního seznamu 2000 byla naměřena
hodnota 34 jako průměrný rozdíl mezi po sobě jdoucími identifikátory dokumentů.
To znamená, že v průměru by se v bitmapě objevil jeden kladný bit po každých 34
záporných bitech, což je pro odůvodnění bitmapy příliš řídké.

Pro komprimované bitmapy by se snížila účinnost jednoduchých bitových operací a
vzájemně by si mohla překážet s relativně vysokou hustotou u hybridního
seznamu, protože kompresní schémata pro bitmapy (např. WAH, EWAH, Concise nebo
Roaring~\cite{DBLP:journals/corr/LemireKK16}) jsou stavěné pro velmi řídké
množiny.

\subsubsection{Jiné přístupy}
Invertované indexy jsou použitelné zejména pro lidský jazyk, který odpovídá
vlastnostem zipfova a heapsova zákona. Text je modifikován odstraněním
neabecedních znaků a rozdělením na slova. V důsledku odstranění některých částí
textu není možné text z indexu zpětně rekonstruovat.

Jiné přístupy, které jsou vhodné i pro text neodpovídající lidské řeči (např.
biologické sekvence) jsou založené na suffixových stromech, suffixových polích
nebo waveletových stromech.

Kromě invertovaného indexu existují i alternativní datové struktury umožňující
rychlé vykonání dotazu a v některých případech i kompaktnější index. V poslední
době se rozšířila popularita tzv. stručných (succint) datových struktur, které
uchovávají data velmi kompaktně za použití bitových polí a bitových algoritmů.
Takovou datovou strukturou je např. waveletový
strom~\cite{Grossi:2003:HET:644108.644250} který našel uplatnění i jako
reprezentace invertovaného
indexu~\cite{Ferragina:2007:CRS:1240233.1240243,FERRAGINA2009849}. Tento
přístup je poměrně nový a má proti klasickému invertovanému řadu výhod
(kompaktnost, možnost více seřazení~\cite{Navarro2010}), ale ten stále zůstává
dle rychlosti vykonání průměrného dotazu nepokořen. Důvodem je zejména jeho
uspořádání v paměti umožňující téměř perfektní sekvenční přístup, který
současné hardwarové architektury silně zvýhodňují. AutoTree~\cite{Weber2007} je
obdoba waveletového stromu s aplikací pro prefixové dotazy.

\section{Algoritmy pro sloučení invertových seznamů}
První část vykonání dotazu, prohledání slovníku, a poslední část, ohodnocení a
seřazení výsledků, by měly být relativně rychlé. Slovník odpovídá pro lidské
jazyky heapsově zákonu, a proto je jeho velikost oproti velikosti všech dat
relativně malá.  Stejně tak počet výsledků by v průměrném případě měl být
poměrně malý. Získání a sloučení invertovaných seznamů by proto mělo dominovat
celkovému času doby vykonání dotazu.

Konstantním výzkumem na poli vyhledávačů jsou algoritmy pro jejich efektivní
slučování zejména v konjunktivní formě (průnik, intersection). V této práci nás
kvůli prefixům a překlepům bude zajímat i disjunktivní forma sloučení
(sjednocení, union, merge), která má velkou roli v prefixových a fuzzy
variantách indexu.

Invertovanými seznamy se rozumí vzrůstající posloupnosti identifikátorů
dokumentů. Ty mohou být obohaceny o pozicovou informaci a ohodnocení, ale pro
jednoduchost zde budeme uvažovat pouze identifikátory dokumentů.

\subsection{Strategie sloučení}
Existují dvě hlavní strategie, jak seznamy
sloučit~\cite{Lacour_efficiencycomparison}. První z nich tzv. sloučení slovo po
slovu (term-at-a-time, \bftt{taat}). Ta nejprve získá invertované seznamy
každého slova a postupně buduje výsledný seznam sloučením mezivýsledku a
dalšího seznamu v pořadí. Např. v konjunktivním sloučení nejprve zpracuje první
dva seznamy do jejich průniku a tenhle mezivýsledek postupně slučuje se zbylými
seznamy. V konjunktivním sloučení se vyplatí seřadit seznamy podle velikosti od
nejmenšího po nejdelší, protože průnik množin je nejefektivnější u sloučení s
co nejmenší množinou.

V druhé strategii procházíme všechny slučované seznamy naráz, jeden dokumentový
identifikátor po druhém (document-at-a-time, \bftt{daat}). I zde platí, že
konjunkce je nejefektivnější po předseřazení seznamů podle velikosti. Oproti
prvnímu má tenhle způsob výhodu, že nemusí materializovat jednotlivé disjunkce,
ale tvoří je za běhu. Tím je výhodnější v případě výskytu invertovaných seznamů
v pomalejším externím úložišti.

\subsubsection{Konjunkce}
Nejjednodušším způsobem vykonání průniku je lineární průchod seznamy a
postupným zaznamenáváním prvku, který se vyskytuje ve všech seznamech -
algoritmus \bftt{zipper}. V situaci, kdy slučujeme dva seznamy - jeden velmi
krátký oproti druhému - je efektivní použít nějaké přeskakovací schéma v delším
seznamu, protože většina dokumentů delšího seznamu nemusí být vůbec brána v
úvahu. Algoritmy s přeskakováním jsou např. binární nebo exponenciální
\bftt{galloping} vyhledávání ve zbytku delšího seznamu, nebo použití
přeskakovacích tabulek (metody \bftt{skip list}, \bftt{skipper},
\bftt{lookup}).  \cite{Sanders:2007:III:2791188.2791195} poskytuje
experimentální měření efektivity těchto algoritmů i při použití delta komprese.

\subsubsection{Disjunkce} \label{sec:disjunctive_merge}
Disjunktivní sloučení je stejný problém jako sloučení podseznamů v řadícím
algoritmu mergesort. Ten se řeší postupným vybíráním minimálního prvku z $k$
seznamů pomocí prioritní fronty (K-way merge) s náročností $\mathcal{O}(n \log
k)$ při použití binární haldy. Díky tomu je disjunktivní sloučení formou taat i
daat algoritmicky totožné, ale daat má výhodu možnosti překakování. Při dotazu
typu \uv{konjunkce disjunkcí} totiž můžeme vynechat velké množství operací
disjunkce, protože v postupující konjunkci průběžně zjišťujeme, které dokumenty
mohou být přeskočeny.

Existuje několik možností pro zefektivnění. Prvním je povšimnutí si, že některé
seznamy budou vyčerpány dříve než ostatní, a proto je můžeme z binární haldy
odstranit předčasně. Druhá modifikace je nepoužití binární haldy pro prioritní
frontu, ale obyčejného seznamu, pokud používáme hybridní index s malým počtem
seznamů ke sloučení. Konstantní faktory vytahování a vkládání prvku binární
haldy mohou být jednoduchým seznamem sníženy. Oproti slučování v mergesortu je
ve slučování konjunkce disjunkcí rozdíl v neustálém přeskakování prvků. Pro
disjunkci znamená jedna přeskakující operace vytažení všech prvků z prioritní
fronty, provedení jejich posunutí a vložení zpět do fronty. Disjunkci můžeme
více přizpůsobit přeskakování oproti posunutím po jednom dokumentu tak, že
seznamy seřadíme od největšího po nejmenší a přeskočení ukončíme v momentu, kdy
alespoň jeden seznam vrátí prvek, na který přeskakujeme. Pouze si uchováme
informaci, že později musíme dorovnat zbytek seznamů k témuž dokumentu. Seznamy
seřadíme od největšího, protože očekáváme větší pravděpodobnost výskytu
dokumentu v delším seznamu, čímž zvýšíme šanci předčasného ukončení operace.
Tyto metody jsou experimentálně porovnány dále
v~\S\,\ref{sec:rychlost_vyhledavani}.



%TODO
% wand, mwand

\section{Ohodnocení výsledků dotazu} \label{sec:ohodnoceni_vysledku}
Poslední fáze vykonání dotazu je ohodnocení všech nalezených výsledků, jejich
seřazení a zvýraznění výskytů v textu (highlighting, snippet generation). V
téhle fázi je důležité se rozmyslet, jestli je nutné zpracovávat všechny
výsledky, nebo pouze \bftt{k} nejlépe ohodnocených (top-k). Zpracování všech
znamená seřazení celé výsledné množiny v čase $\mathcal{O}(n \log n)$, oproti
částečnému zpracování, kdy můžeme použít pouze několik iterací heapsortu v
$\mathcal{O}(n + k \log n)$ nebo algoritmus pro částečné seřazení quickselect v
očekávané složitosti $\mathcal{O}(n + k \log k)$.

\subsection{Předdefinované pořadí}
Dokumenty v invertovaných seznamech nemusí být nutně seřazeny vzestupně podle
jejich identifikátorů. Jiným způsobem je seřazení podle klesající váhy, která
je u každého dokumentu stanovena před zaindexováním. To umožňuje předčasné
ukončení dotazu (early termination) namísto průchodu celým seznamem, protože
pokud systém vrací pouze nejlepších \bftt{k} dokumentů, pak nemají dokumenty s
nízkým ohodnocením šanci se do nich vejít.

Výpočetně by bylo výhodnější, pokud by se uvažovalo pouze předdefinované
pořadí, protože by bylo možné ho předpočítat během indexování a invertovaný
index seřadit podle něj. Případně index uložit ve více pořadích za cenu více
potřebného úložného prostoru. Více uložených pořadí používá např. sloupcová
databáze C-Store~\cite{Stonebraker:2005:CCD:1083592.1083658}.

\subsubsection{Business metriky}
Předdefinovanou váhou může být zvolena jakákoliv metrika relevantní pro datovou
sadu, např. uživatelské hodnocení u filmů nebo knih, počet zakoupení produktu,
popularita produktu, atd. Uživatel si dokonce může zvolit vlastní preferované
pořadí, např. sestupně nebo vzestupně podle ceny nebo data uvedení. Ohodnocení
může být stanoveno i tak, aby maximalizovalo mikroekonomickou užitkovou funkci
a v důsledku zisk firmy, která vyhledávač provozuje na svém eshopu.

% ekonomická funkce

% business metrics
\subsubsection{Analýza vazeb}
Pro dokumenty u webových vyhledávačů se používá ohodnocení na základě
popularity webových stránek ve webovém grafu. Pro ohodnocení popularity se
používají algoritmy pro míry centrality, z nichž nejpopulárnější je
PageRank~\cite{Page99thepagerank}.

\subsubsection{Více průchodů}
Některé vyhledávače implementují víceprůchodové ohodnocení tak, že jejich první
fáze odfiltruje určitou množinu relevantních dokumentů pomocí funkce s nízkou
náročností a pro zbylé aplikuje sofistikovanější
metody~\cite{Yin:2016:RRY:2939672.2939677}.

\subsection{Statistické metody}
U dlouhého textu můžeme použít statistiky počtu slov k určení jeho relevance k
nějakému slovu. Na tomhle principu stojí většina ohodnocovacích metod a metod
pro dolování dat, které pracují s dlouhým textem.

Hodnotou \bftt{tf} (term frequency) označíme počet výskytů slova v daném
dokumentu a \bftt{df} (document frequency) bude značit počet výskytů slova v
celé textové kolekci. \bftt{idf} (inverse document frequency) je převrácená
hodnota \bftt{df}. Statistické metody kombinují tyto dvě hodnoty, proto tomuto
modelu říká \bftt{tf.idf}. Intuice \bftt{tf} je, že čím vícekrát se slovo v
dokumentu objevuje, tím je pravděpodobnější, že je k dokumentu relevantní.
\bftt{idf} je korekcí pro často se vyskytující slova. Větší váhu dá těm slovům,
která jsou v datové kolekci objevují málo často (vysoká hodnota \bftt{idf}),
protože mají vysokou rozlišovací schopnost. Četná slova jako předložky nebo
zájmena o dokumentu prakticky nic nevypoví. Vztah pro základní funkci
\bftt{tf.idf} kombinující obě statistiky je \[\bftt{tf.idf}_{t, d} = (1 + \log
\bftt{tf}_{t, d}) \times \log_{10} N / \bftt{df}_{t}\]

\subsubsection{Problémy statistických přístupů}
Už od počátků velkých webových vyhledávačů v 90. letech se čistě textové
statistické metody založené na tf.idf zneužívaly, protože každý mohl upravit
text na své webové stránce tak, aby se lépe jednoduchým statistikám
přizpůsobila.

Stačí stránku zahltit co největším počtem různých slov (keyword stuffing), aby
byla zvýšená pravděpodobnost, že ji vyhledávač uzná relevantní při náhodném
dotazu. Pro ještě větší úspěšnost lze důležitá slova na stránce duplikovat pro
zvýšení parametru \bftt{tf}. Tenhle problém do nějaké míry řeší pokročilá
modifikace \bftt{tf.idf} funkcí nazvaná BM25
(Best~Match)~\cite{Robertson:2009:PRF:1704809.1704810}.

U přibližného vyhledávání nastává problém s parametrem \bftt{idf}, protože
slova, která odpovídají překlepům, se vyskytují vzácně v porovnání s jejich
správnou verzí. Protože ale \bftt{idf} slouží ke zvýšení váhy u vzácných slov,
bude mít u přibližného vyhledávání nežádoucí efekt~\cite{elastic_fuzzy}.

\subsection{Ohodnocení pro přibližné vyhledávání}
Vyhledávač by měl umožnit uživatelům volbu preferenčního pořadí výsledků (podle
času, podle ceny, počtu zhlédnutí, apod.). Přesto by měla mít textová shoda
největší význam, jinak by mohl uživatel nabýt dojmu, že si buď vyhledávač
vymýšlí, má nízkou přesnost, nebo se mu někdo snaží něco vnutit (např.
upřednostněné produkty v eshopu). Statistické metody pro případ zde
navrhovaného vyhledávače jsou problematické, a proto se budeme zaměřovat na
alternativní ohodnocovací způsoby, které jsou závislé pouze na textové shodě.
Jiné způsoby by vyžadovaly náročnou sémantickou analýzu jednotlivých položek, a
proto je pro jednoduchost vynecháme.

\subsection{Blízkost}
Statistiky jako počet slov v textu ztrácí v krátkém textu význam. Např. film
nebude více \uv{zelený}, pokud se v jeho názvu bude vícekrát vyskytovat slovo
\bftt{zelený}. V krátkém textu by byly nadměrně hodnoceny delší texty, které
slovo obsahují vícekrát, protože většina textů v kolekci bude slovo obsahovat
jenom jednou.

Tradičně podporují implementace invertovaných indexů hledání s přihlédnutím k
blízkosti slov pomocí frázových dotazů. Už při slučování seznamů dojde k
odfiltrování všech záznamů, které nesplňují podmínku blízkosti fráze pro
nějakou stanovenou mez. Pro malý index není třeba filtrovat během slučování,
ale stačí pouze na konci seřadit výsledky podle nejmenší vzdálenosti mezi
slovy.

Ohodnocení blízkosti při seřazování výsledků, kdy známe pozice výskytů všech
slov dotazu uživatele, můžeme jednoduše sečíst velikosti mezer mezi
jednotlivými výskyty a zprůměrovat počtem slov. Čím kratší mezery, tím lépe
bude výsledek ohodnocený, tak jak očekáváme. Problém nastane, pokud výsledek
obsahuje více výskytů téhož slova, s tím že jedno patří do hledané fráze a
druhé je velmi vzdálené od této fráze. Např. při dotazu \bftt{dobrý den} by
mohl nalezený dokument obsahovat více výskytů druhého slova (\texttt{Krásný
\bftt{dobrý den} .... sobota je \bftt{den} volna}) a v tomto případě by druhý
výskyt znehodnotil přesnou shodu nalezenou dříve.

Velká vzdálenost tohoto slova od fráze, ke které nepřispívá zapříčiní nárůst
téhle naivní metriky. To naznačuje, že musíme uvažovat minimální vzdálenost
výskytů - tedy délku minimálního intervalu, který obsahuje všechna slova
dotazu.

\subsubsection{Nalezení minimálního intervalu} \label{sec:find_minimal_interval}
\paragraph{Algoritmus.} Vstupem algoritmu je seznam všech výskytů slov v
dokumentu a výstupem je interval s minimální délkou v tomto seznamu, který
obsahuje všechna unikátní slova alespoň jednou. Tento seznam je seřazen
vzestupně podle pozice výskytu a během algoritmu v něm postupujeme od
nejmenšího k největšímu, tedy zleva doprava, a udržuje si frekvenční tabulku
všech viděných slov.  V prvním kroku nalezneme první interval, který obsahuje
zástupce každého slova - ten označíme jako kandidát na minimální interval. Ve
druhém kroku rozšiřujeme tento interval slovy napravo od něj, dokud nenarazíme
na první slovo dosavadního nalezeného intervalu. Tento interval jistě obsahuje
zástupce každého slova, protože jsme zatím žádné neodebrali, ale obsahuje
některé vícekrát. Interval zmenšíme opět na minimální velikost tím, že zleva
odstraníme všechny výskyty tak, aby jej žádné ze slov neopustilo (postupně
sledujeme frekvenční tabulku pro každé slovo a zastavíme se, pokud odstraňovaný
prvek má hodnotu 1). Postupně projedeme celý seznam rozšiřováním intervalu
výskyty napravo a odebíráním výskytů zleva. Všechny nalezené kandidáty seřadíme
podle velikosti a vrátíme jako výstup poslední nalezený minimální interval.

\subsubsection{Jiné přístupy}
Index s rychlou podporou omezení blízkosti, ale za cenu velkého indexu
($\mathcal{O}(dn)$, kde $d$ představuje hranici blízkosti) navrhli Manber
a~Baeza-Yates v~\cite{MANBER1991133}. Jiný paměťově náročný způsob je
zaindexování párů všech slov do vzdálenosti $d$~\cite{Aref95}. Sadakane a Imai
navrhují dva algoritmy~\cite{Sadakane99textretrieval} s~podporou pro
invertovaný index, které vyžadují pouze index uchovávající si pozice bez
dalších změn. Prvním je algoritmus plane-sweep a~druhý je varianta prvního
s~použitím techniky rozděl a panuj. Algoritmus představený v~této sekci je
modifikací z~prvního z~nich.

\subsubsection{Zvolení vah}
Celkové ohodnocení shody se vypočte jako součet všech dílčích faktorů
ohodnocených různými váhami. U přibližného vyhledávání by důležitost shody měla
být v první řadě určena vzdáleností výsledku od dotazu podle editační
vzdálenosti. V druhé řadě by to měla být minimální vzdálenost mezi slovy ve
výsledku. Tohle pořadí pak může doupravit uživatelem zvolené pořadí, ale mělo
by mít menší váhu než čistě textová shoda.

\subsubsection{Tie-breaking} \label{sec:tie-breaking}
Jiný způsob výpočtu důležitosti používá implementace komerčního vyhledávače
Algolia~\cite{algolia_website,algolia_ranking}. Podle jejich popisu se určí
pořadí jednotlivých faktorů podle důležitosti a nižší faktory se uvažují pouze
pokud dojde ke shodě ve vyšších faktorech (tie-breaking). Např. pokud je
vzdálenost několika prvních výsledků dotazu od dotazu 0 - tedy přesná shoda,
pak teprve se přistoupí k ohodnocení podle předdefinovaného pořadí, blízkosti,
nebo jiného faktoru. Tie-breaking sice ulehčuje výpočetní náročnost i pro
složitější ohodnocovací funkce, ale neposkytuje flexibilitu v modifikacích vah,
a proto nebyl tie-breaking v této práci použit.

\subsection{Indexace}
Efektivní vytvoření indexu je hluboce prozkoumávaná oblast v literatuře
textového vyhledávání. Většina výzkumu se zaměřuje na rozsáhlé textové kolekce,
a proto počítá s tím, že se index i po kompresi nevejde do paměti. Pokud by se
celá datová sada vešla do paměti, pak je nejjednodušším způsobem vytvoření
invertovaného indexu pomocí hashovací tabulky. Při průchodu textem si v
hashovací tabulce zaznamenáváme pro každé slovo seznam dokumentů, ve kterých se
nachází. Tím, že procházíme dokumenty od prvního k poslednímu, budou tyto
seznamy rovnou seřazeny a posléze je můžeme použít přímo pro vyhledávání. Běžně
se používají hashovací tabulky s optimalizací vzhledem k zipfově zákonu, které
zvýhodňují frekventovaná slova při hledání v tabulce (např. techniky
move-at-back nebo move-to-front)~\cite{ZOBEL2001271}. Experimentální měření
technik pro konstrukci indexu čistě v paměti bylo provedeno
v~\cite{Buttcher05memorymanagement}.

V případě, že se data do paměti nevejdou, pak se používají algoritmy, které
jsou vytvářené na míru danému hardwaru pro persistentní uložení. Tradičně se
pro tuto úlohu používaly pevné disky, ale v poslední době se objevily techniky
přímo zaměřené na rychlejší ssd~\cite{Jung201525}.

\subsubsection{Konstrukce seřazením}
První z nejpopulárnějších metod pro vytvoření invertovaného indexu je
konstrukce blokovým seřazením~\citep[kap.~3]{Manning:2008:IIR:1394399}. Ta
sestává ze dvou fází.  V první dojde k vygenerování všech ntic výskytu
(viz~\S\,\ref{sec:inverted_index}) a jejich postupné seřazení. Druhou fází je
seřazení bloků ntic. Seřazení proběhne externě, tj. s použitím disku. Ntice
jsou rozděleny do bloků o velikosti, aby se vešly jednotlivě do paměti. Ty jsou
seřazeny běžným řadícím algoritmem. Krok externího seřazení vykonáme několika
slučovacími kroky mergesortu, které mohou zpracovávat jednotlivé bloky proudově
- tedy bez celého načtení do paměti.  Způsob je obdobný jako disjunktivní
slučování seznamů při vykonání dotazu, který byl popsán v
\S\,\ref{sec:disjunctive_merge}.

\subsubsection{Konstrukce o jednom průchodu}
Druhou populární metodou~\citep[kap.~3]{Manning:2008:IIR:1394399}, která je
experimentálně výkonější~\cite{Heinz:2003:ESI:873988.873992}, je v podstatě
stejná jako vytvoření indexu v paměti, jen v momentu, kdy dojde pro index
paměť, je dosavadní tabulka seřazena podle lexikografického pořadí slov a
zapsána na disk. Tímto způsobem se vytvoří a zapíší na disk jednotlivé
podindexy a v následujícím kroku se sloučí podobným způsobem jako u indexace
seřazením. Výhodou oproti předchozímu způsobu je, že identifikátory dokumentů
jsou ukládány v sekvenčním pořadí u každého slova, a díky kompresi umožní
uchovat v paměti více dat před tím, než bude blok zapsán na disk.

\subsubsection{Konstrukce hybridního indexu}
Rozložení do skupin u hybridního indexu vyžaduje dvoufázové indexování, protože
rozdělení do skupin potřebuje vědět statistiky, ze kterých se určí, jak mají
být sloučené skupiny velké. V~\cite{Bast:2011:FCH:1993036.1993040} autoři
popisují techniku inspirovanou technikami pro paralelní řazení, jak použít jen
několik vzorků, aby s vysokou pravděpodobností určili velikost skupin.

Hybridní index se od klasického liší v počtu invertovaných seznamů. Klasický má
jeden seznam, byť potenciálně velmi krátký, pro každé vyskytující se slovo v
kolekci. Počet slov se odvíjí od heapsova zákona - tedy roste přibližně s
odmocninou velikosti kolekce - a pro velké kolekce je následně velký počet slov
důvodem indexačních technik pracujících s externí pamětí. Kvůli
charakteristikám pevných disků (pomalé posunutí čtecí hlavice při nesekvenčním
přístupu) a problému mít otevřených tisíce souborů najednou spočívají klasické
indexační algoritmy v tom, že zapisují částečné indexy sekvenčně na konec
jednoho souboru, a poté (nebo ještě za běhu) je postupně sloučí do většího.
Hybridní index ale bude mít pouze omezené množství invertovaných seznamů,
jejichž velikosti lze navíc korigovat. Díky omezenému počtu invertovaných
hybridních skupin by nemělo být takovým problémem vytvořit pro každý
invertovaný seznam jeden soubor a vyhnout se slučovacímu kroku.
V~\cite{Bast:2011:FCH:1993036.1993040} využívají malého počtu seznamů a navíc
algoritmus optimalizují i pro efektivní přístupy do mezipaměti procesoru (L1
cache). Ve výsledku dosahují experimentálně vyššího výkonu než doposud
nejrychlejší metoda \uv{konstrukce o jednom průchodu} popsaná výše.


%\subsection{Datové struktury}
%Po uvážení běžně používaných a alternativních datových struktur pro index jsem
%zvolil klasický invertovaný index s hybridní materializací. Pro slovník jsem
%použil prefixovou trii, díky její podpoře pro efektivní prefixové i fuzzy
%vyhledání slov. Invertovaný index je hybridní, protože se díky jeho
%předsloučeným invertovaným seznamům hodí právě na prefixové a fuzzy dotazy,
%které jsou klíčovou vlastností mého návrhu.
%
%Vzdálenostní funkcí pro fuzzy vyhledávání ve slovníku jsem použil vzdálenostně
%adaptovanou Levenshteinovu metriku kvůli lepšímu výkonu za cenu o něcoméně
%přesných výsledků při překlepech na začátcích slov. Ve výsledku to není
%problém, protože většina odlišností je až na konci slov kvůli povaze
%morfologie.
%
%Uvážil jsem i alternativní datové struktury pro index. Waveletové stromy jsou
%vhodné pro velmi kompaktní uložení indexu a navíc mohou podporovat více pořadí
%bez nutnosti ukládat index vícekrát. (cite dualsorted index). Protože se
%soustředím na menší objemy dat, není kompaktnost až tak důležitá. Za druhé jsem
%nenašel způsob, jak v něm uložit hybridní index, který má pro mé účely velkou
%výhodu.

% V této kapitole budou popsány konkrétní detaily implementací porovnaných
% systémů. V první sekci bude rozebrána implementace prototypu navrhovaného
% systému a v pozdějších částech stručně popsány konfigurace v systémech
% ElasticSearch a Postgres.

\section{Navrhovaný systém}
Nový systém byl navrhován tak, aby v první řadě podporoval vyhledávací rozhraní
search-as-you-type a zároveň přibližné vyhledávání. Pro slovník je použita
datová struktura trie s podporou fuzzy vyhledávání samotných slov i prefixů.
Pro porovnání podobnosti slov může být použita i klasická editační vzdálenost i
dynamicky adaptovatelná, která struktury trie využívá (popsáno
v~\S\,\ref{sec:trie}). Schopnost vyhledávat v prefixech lze kromě
samodoplňování využít pro facetové filtrování (viz
~\S\,\ref{sec:faceted_search}), ale v této prototypovací fázi není facetové
filtrování uvažováno. Detailnější popis, jak prefixové vyhledávání tímto
způsobem použít je v~\cite{Bast_abstractwhen}.

\subsection{Invertovaný soubor}
Pro efektivní nalezení přibližných výskytů byla použita variace hybridního
indexu typu HYB (\S\,\ref{sec:hyb}), která oproti němu respektuje abecední
hranice jednotlivých sloučených seznamů. Důvodem je, že sloučené skupiny
invertovaných seznamů by měly být ideálně stejně dlouhé. Není to ale
jednoduché, protože se různá písmena objevují různě často. Např. obecně bude
existovat více slov začínající na \bftt{T} než těch, které začínají na
\bftt{X}. Kdyby se měla každá skupina shodovat alespoň v prvním písmenu, bylo
by to v rozporu s požadavkem na stejně dlouhé skupiny. Teoretickým řešením by
mohlo být předefinování abecedního pořadí dle frekvenční analýzy. Nejpočetnější
skupiny by mohly být rozděleny do více podskupin (např. \bftt{[TA-TD]},
\bftt{[TD-TM]}, \bftt{[TM-TZ]}), pro středně početné by existovala právě jedna
skupina a méně početné by byly sloučeny dohromady (např. \bftt{[U-X]},
\bftt{[X-Z]}). Kdyby se nepoužilo frekvenční pořadí, mohla by se totiž mezi méně
početnými skupinami vyskytovat vysoce početná skupina zabraňující sloučení těch
okolních.

Tím, že je použita vzdálenostně adaptovaná editační vzdálenost a většina slov
nalezených ve fuzzy slovníku bude mít podobný prefix, docházelo by k tomu, že
velká část seznamu by byla nevyužita, protože se liší v prvním znaku. Rozmezí
slov u jedné skupiny HYB může být např. \bftt{[J-M]} a dotaz by začínal na
\bftt{M}. Slova v seznamu začínající na \bftt{J}, \bftt{K} nebo \bftt{L} by v
tomhle případě byla nevyužita. Má úprava rozložení skupin oproti abecednímu
rozložení u struktury HYB se snaží zajistit, aby byly abecední hranice
respektovány co nejvíce. Výsledkem by měly být skupiny, jejichž hraniční slova
sdílejí co nejdelší prefixy.

\subsubsection{Rozdělení skupin respektující abecední hranice} \label{sec:hybrid_algorithm}
\paragraph{Algoritmus.} Zařazení slova do slučující skupiny lze provést při
prohledání trie obsahující celý slovník do hloubky. Začínáme na kořenovém uzlu
s tím, že uvažujeme prozatím jedinou skupinu, do které jsou zpočátku všechna
slova zařazena.  Pokud při rekurzivním průchodu součet délek invertovaných
seznamů poduzlů jednoho uzlu překročí stanovenou hranici, pak je dosavadní
přiřazená skupina uzlu nahrazena skupinou novou. Rekurzivní aplikací pro
všechny uzly se původně jedna skupina rozpadne na více skupin. Jediným
parametrem je hranice určující rozpadnutí skupiny, která může být staticky
daná, nebo být odvozená od dat. Zatím jsem ideální způsob, jak ji stanovit.

\subsection{Ohodnocování}
Prozatím je v prototypové verzi implementováno ohodnocení jednoduchým nalezením
minimálního intervalu se zástupci všech typů výskytu
(viz~\S\,\ref{sec:find_minimal_interval}). Do budoucna by mohlo být
implementované vícefázové ohodnocování, protože nalezení minimálního intervalu
je výpočetně poměrně náročné, pokud se má provést pro každý nalezený záznam.
Alternativně by problém s efektivitou mohl být vyřešení technikou tie-breaking
(\S\,\ref{sec:tie-breaking}), ale podle očekávání by celkový čas vykonání
dotazu měl být dominován slučováním invertovaných seznamů. Navíc primární
určení vyhledávače je pro datové sady s charakteristikou small-data, a tedy
optimalizování výkonu ohodnocování by mohlo být zbytečně předčasné. V první
řadě je důležitější flexibilita ohodnocovací funkce, kterou např. tie-breaking
silně omezuje. Experimentální měření náročnosti jednotlivých fází vykonání
dotazu je dále v \S\,\ref{sec:timings_query_parts}.


\subsection{Problém korelovaných slov} \label{sec:correlated_words}
Jednou příčinou pomalých dotazů ve fuzzy vyhledávači je výskyt více podobných
slov v dotazu. Jednoduchým případem pro ilustraci je, pokud dotaz obsahuje více
totožných slov. Jejich invertované seznamy, případně disjunkce více
invertovaných seznamů, budou totožné. Dotaz ale proběhne beze změn, a tedy
provede naivně jejich sloučení, přestože jsou stejné. Lze snadno vidět, že
sloučení dvou totožných seznamů bude odpovídat dvojnásobku času průchodu pouze
jedním seznamem.

O dvou podobných invertovaných seznamech můžeme hovořit jako o vysoce
korelovaných. Pokud slučujeme dva nízce korelované seznamy, pak jejich sloučení
bude relativně malá množina. Naopak vysoce korelované dotazy trpí tím, že
pozitivní zmenšující efekt konjunkce u nich neplatí a navíc zvyšuje výpočetní
náročnost lineárně s každým takovým slovem.  Jednoduchým případem pro vyřešení
tohoto problému je, pokud lze v dotazu identifikovat totožná slova a jednoduše
použít jen jedno z nich, protože průnik více stejných množin neovlivní
výsledek. Pouze se musí zajistit, že výsledky skutečně obsahují více výskytů
téhož slova, a to lze provést až v pozdější fázi po průchodem invertovanými
seznamy.  Těžším případem je, pokud se v dotazu vyskytují slova, která k sobě
mají podle editační vzdálenosti blízko.  Jejich invertované seznamy, případně
disjunkce více seznamů, budou sdílet značnou část slov. Kvůli tomu budou jejich
invertované seznamy vysoce korelované a budou způsobovat degradaci dotazu
popsanou výše. Nicméně tohle už nelze řešit pouhým přepsáním dotazu odstraněním
duplikovaných slov.

Dalším problémem korelovaných slov je najít každému slovu v dotazu odpovídající
slovo ve výsledku bez překrytí, tj. pro každé slovo v dotazu, přestože může být
duplikované, musí být nalezen výskyt ve výsledném dokumentu, který ale nesmí
připadat jinému slovu z dotazu. Např. naivní technika deduplikování totožných
slov by způsobila, že dotaz \bftt{the the} by nalezl všechna slova, která
obsahují pouze jeden výskyt slova \bftt{the}. Nás ale budou zajímat výsledky,
které obsahují alespoň dva výskyty. Proto jsme v dotazu druhé \bftt{the} přece
použili.

Řešení není úplně snadné, protože algoritmicky odpovídá nalezení maximálního
párování v bipartitním grafu. V prototypu této práce je použita pouze
jednoduchá heuristika, která zajistí, že neodfiltrovaný výsledek obsahuje
alespoň tolik výskytů, kolik má dotaz celkově slov. To zabrání případům, kdy
více podobných slov v dotazu zasáhne jediné vyskytující se slovo ve výsledku.
Např. dotaz \bftt{2015 - 2016} by nesprávně vrátil dokument \bftt{Grand Tour
2015}, protože obě slova dotazu zasáhnou jediný výskyt \bftt{2015}. Tím, že
nastolíme požadavek, že výsledek musí mít alespoň tolik výsledků (zde 1), kolik
je slov v dotazu (zde 2), bude těmto primitivním případům zabráněno.

\section{Podobné práce}
% COMPLETE SEARCH
% TASTIER
Navarro a spol. představili online vyhledávací
systém~\cite{Navarro:2003:MFA:638683.638685}, který pravidelně zpracovává nové
přicházející články z právní oblasti a vyhledává v nich jména právníků a názvy
firem dle předchozího nastavení. Vstupem je sada textových dokumentů a seznam
názvů k prohledání a výstupem je seznam všech přibližných výskytů těchto názvů
ve vstupních datech. Systém tedy provede všechny dotazy najednou. Systém je
typu online, jelikož před zpracováním neprovádí indexaci textu. Ta není
potřeba, protože systém provede jeden velký dotaz při získání nové sady a poté
už s ním systém nepracuje.

Podobnější této práci jsou v tomto smyslu offline systémy s podporou
prefixového nebo přibližného vyhledávání
Tastier~\cite{Ji:2009:EIF:1526709.1526760} a
CompleteSearch~\cite{Bast:2006:TLF:1148170.1148234}, které text zaindexují a pak
ho zpřístupňují uživatelům pro jejich zvolené dotazy.

Ji a spol. se~v~\cite{Ji:2009:EIF:1526709.1526760} zaměřují na ukládání
mezivýsledků inkrementálního vyhledávání a jak adaptovat datové struktury pro
podporu ukládací mezipaměti. Jejich řešení pro konjunkci disjunkcí spočívá v
kombinaci invertovaného a dopředného indexu. Nejprve se nalezne nejkratší z
disjunkcí, kterou se projde od začátku do konce. Každý nalezený potenciální
dokument se vyhledá v dopředném indexu a binárním vyhledáváním se zjistí
přítomnost slov z ostatních disjunkcí. V jejich práci poskytují výsledky
porovnání jejich metody s HYB indexem a materializovanými seznamy. Kombinace
posledních dvou je podobná přístupu zvoleném v této práci.

Systém CompleteSearch představený v~\cite{Bast:2006:TLF:1148170.1148234} byl
původně vytvářen pro efektivní prefixové vyhledávání díky HYB indexu. Autoři
ukázali, že tento prefixový systém lze použít na facetové
vyhledávání~\cite{Bast_abstractwhen} a~dokonce jako efektivní databázový systém
pro sémantické full-textové vyhledávání~\cite{Bast:2007:EES:1277741.1277856}.
Rozšíření systému CompleteSearch pro přibližné hledání bylo podrobněji popsáno
v~\cite{Bast:2013:EFS:2457465.2457470}. Pro implementaci fuzzy slovníku je
použita kombinace hashovací metody (viz~\ref{sec:hash_fuzzy}) pro krátká slova
(DeleteMatch) a~metody založené na permutovaném lexikonu (PermuteScan) pro
delší slova a trie. Ve výsledku autoři konstatují, že jejich navržené řešení se
přibližuje optimálnímu uspořádání, které lze dosáhnout u indexu operujícím
čistě v paměti RAM a jejich úzké hrdlo spočívá v získávání invertovaných
seznamů z disku. Přiznávají, že přibližné vyhledávání je (překvapivě) velmi
náročný problém pro rozsáhlé datové sady.

Z komerčních se prefixovému přibližnému vyhledávání a s důrazem na blízkost
věnují systémy Srch2~\cite{srch2_website} a Algolia~\cite{algolia_website}.
Z~open source systémů se přibližnému vyhledávání na základě editační
vzdálenosti částečně věnuje Lucene, ale podle některých dosahuje pro fuzzy
dotazy nepřijatelného
výkonu~\cite{Bast:2013:EFS:2457465.2457470,algolia_vs_elasticsearch}. Přibližné
vyhledávání na základě trigramů je celkem rozšířená technika, kterou podporuje
Lucene~\cite{lucene_website} i~Postgres~\cite{postgres_website}.


\chapter{Výsledky porovnání} \label{sec:results}
V této části budou nejprve shrnuty konfigurace a vlastnosti ostatních
implementovaných vyhledávacích systémů a datové soubory použité pro
experimentální analýzu.  První část analýzy je porovnání vlastností
implementovaného systému s ostatními. Druhá část obsahuje shrnutí výkonnostních
vlastností (rychlost, paměťová náročnost) systémů.

\section{Porovnávané systémy}
Kromě vlastní implementace byly pro systém ElasticSearch nakonfigurovány
dva systémy. V systému Postgres byl vytvořen jeden trigramový
vyhledávač. Všechny systémy byly porovnány na počítači s 16\,GB paměti a
procesorem Intel Core2 Quad Q8400 (2.66\,GHz).

\subsubsection{Vlastní implementace}
Protoyp nového systému vychází z výše popsaného návrhu vyhledávacího systému.
Implementace je prototyp napsaný v dynamicky typovaném Pythonu, a tedy jestě
nemůže z výkonostního důvodu sloužit k objektivnímu porovnání rychlosti a
jiných výkonnostních veličin. Protože jsou ale použity relativně malé datové
sady, neměla by být výkonnost překážkou. Prototyp slouží zejména jako
demonstrace implementovaných technik. Tou hlavní je výše popsaný modifikovaný
hybridní invertovaný index se slovníkem implementovaným jako trie. Ohodnocovací
funkce je čistě textově založená - nemá o datech žádné vlastní předpoklady nebo
přednostní pořadí. V první řadě upřednostňuje shodu dotazu s výsledkem podle
editační vzdálenosti - pro každé slovo zvlášť. Dále pozitivně hodnotí ty
výsledky, které obsahují shodu s minimální poziční vzdáleností. Přesné shody
výsledků se stejným ohodnocením jsou pak rozbity několika minoritními hodnotami
s nízkou váhou. Shoda, která je více na začátku záznamu má přednost a shoda,
která obsahuje více shodujících se slov má přednost - obdobně jako u modelu
tf.idf, pouze zde má velmi malou váhu a neuvažuje relativní důležitost slova ve
shodě (idf). Pro porovnání byly vytvořeny dvě varianty. První používá
Levenshteinovu editační vzdálenost (systém \bftt{edit}) a druhá dynamicky její
dynamicky adaptovatelnou variantu (systém \bftt{edit\_dyn}), viz
\S\,\ref{sec:trie}.

\subsubsection{ElasticSearch - základní řešení}
Jako základní řešení sloužící pro porovnání jsem nakonfiguroval Elasticsearch
tak, jak by se měl doporučeně nastavit pro full-textové vyhledávání ve větších
textových dokumentech. Text je stematizován zabudovaným stematizérem pro
češtinu. Cílem je ukázat, že tohle nastavení bude mít potíže s vícejazykovou
datovou sadou, jmény a s některými případy morfologie v češtině.

\subsubsection{ElasticSearch - trigramový vyhledávač} \label{sec:trigram_implementation}
Pro ukázku fuzzy vyhledávání pomocí n-gramů byla vytvořena další konfiguraci
pro Elasticsearch, která používá n-gramy o minimální a maximální velikosti 3.
Dále bude vystupovat pod názvem \bftt{trig\_ES}.

Pokud se v konfiguraci použije n-gramový filtr, pak není podporováno n-gramové
zvýrazňování výsledků. Musí se použít n-gramový tokenizér, který ale zahazuje
všechny n-gramy, které jsou kratší než minimální délka n-gramu. Tohle chování
nelze přenastavit a doporučeným řešením je právě vytvořit dodatečný index pro
zahozené ngramy. Pro zvýšení přesnosti byl vytvořen ještě třetí index, který
obsahuje slova v původním tvaru. Dotaz je proveden zároveň ve všech třech
indexech metodou \bftt{most\_fields}.

Konfigurace dotazu je navržena tak, aby došlo k zásahu, pokud dokument obsahuje
alespoň $70\%$ trigramů dotazu (odpovídá Jaccardově vzdálenosti $0.3$). Vyšší
hodnoty začaly rychle ztrácet výsledky a nižší hodnoty vracely příliš mnoho
nesouvisejících záznamů.

Zvýrazňovač pro n-gramy funguje, pouze pokud se použije tzv. \textit{Fast
Vector Highlighter}. Přestože je zvýrazňování nastaveno doporučeným způsobem
pro tento typ dotazu, dochází k chybám, když se n-gramy ve výsledku překrývají.
Je možné, že je chyba pouze ve verzi Elasticsearch, která byla použita. Jako
řešení byl v Pythonu vytvořen dodatečný zvýrazňovač, aby zde prezentované
výsledky byly vizuálně srovnatelné s ostatními systémy.

\subsubsection{Postgres - trigramový vyhledávač}
Databáze Postgres poskytuje více možností pro textové vyhledávání. Rozšíření
podporující fulltextové vyhledávání bylo vynecháno, protože jako zástupce pro
vyhledávání s využitím jazykové analýzy byla vytvořena flexibilnější
konfigurace v ElasticSearch (\bftt{cs\_ES}) s~lepší podporou češtiny.

Pro přibližné vyhledávání v Postgresu je doporučovaný trigramový plugin
\bftt{pg\_trgm}. Porovnávaný systém na něm založený (\bftt{trig\_PG}) ho
využívá v základním nastavení, pouze provádí odstranění diakritiky a při
indexování převede velká písmena na malá. Tento systém generuje trigramy
odlišným způsobem než trigramový systém v ElasticSearch. Např. pro slovo
\bftt{pes} vygeneruje ngramy \bftt{p}, \bftt{pe}, \bftt{pes}, \bftt{es},
zatímco ElasticSearch pouze jeden trigram v tomto případě shodující se s
původním slovem.



%\section{Implementované systémy}
%Pro demonstraci uvedených technik jsem vytvořil tři vyhledávací systémy. První
%z nich je má implementace hybridního indexu a zbylé dva jsou různé konfigurace
%pro Elasticsearch sloužící k porovnání. Všechny porovnávané systémy odstraňují
%diakritiku a převádějí velká písmena na malá při indexování i dotazování.
%Všechny konfigurace jsou v příloze \ref{appendix:search_config}.


\section{Použité datové soubory}
Cílem je porovnat několik vyhledávačů na datových sadách s relativně krátkými
dokumenty, které mají navíc různé záludnosti jako více jazyků nebo slova
obtížně zpracovatelná jazykovou analýzou (jména nebo odborné pojmy).

\subsubsection{ČSFD - filmy}
První datovou sadou (\bftt{csfd\_filmy}) jsou názvy filmů z Československé
filmové databáze (ČSFD - \url{http://csfd.cz}). Přestože je možné rozlišit jazyk
filmu, protože databáze tuto metainformaci obsahuje, byly názvy ve všech
jazycích sloučeny do jedné kolekce. V datové sadě se tedy film vyskytuje
vícekrát, pokud má více názvů.

Kolekce celkem obsahuje \textbf{398\,430} filmů. K nim jsou přidány názvy v
jiných jazycích, kterých je dohromady dalších \textbf{141\,722}.

\subsubsection{ČSFD - filmoví tvůrci}
Druhá datová sada (\bftt{csfd\_tvůrci}) jsou všichni tvůrci (herci, režiséři,
scénáristé, apod.) pocházejících rovněž z databáze ČSFD. Jsou uvažováni jen ti
tvůrci, kteří se podle dat podíleli na filmech z databáze \bftt{csfd\_filmy}.
Nejsou uvažováni tvůrci, kteří nejsou přiřazeni k žádnému filmu.

\subsubsection{Česká wikipedia - nadpisy článků}
Data z české wikipedie lze stáhnout z
\url{https://dumps.wikimedia.org/cswiki/}. Pro nadpisy byl použit soubor
\textbf{cswiki-20160111-all-titles.gz}, který obsahuje \textbf{907\,094}
titulků článků (\bftt{cswiki\_titulky}).


\mbox{}
\begin{tt}
\begin{table}[H]
\centering
\begin{tabulary}{\textwidth}{LRRRR}
\textbf{data} & \textbf{velikost} & \textbf{záznamů} & \textbf{slov} & \textbf{unikátních slov} \\
\hline
csfd\_filmy     & 11.2\,MB & 540\,152 & 1\,958\,147 & 231\,452 \\
csfd\_tvůrci    & 3.6\,MB  & 240\,659 & 523\,544    & 124\,780 \\
cswiki\_titulky & 17.1\,MB & 907\,094 & 2\,987\,750 & 339\,091 \\
\hline
\end{tabulary}
\caption{Statistiky datových sad}
\label{tab:data_stats}
\end{table}
\end{tt}

\section{Porovnání výsledků vyhledávání}
Všechny analyzované vyhledávače podle očekávání vrátí výsledek, pokud se v
jednotlivých slovech přesně shoduje s dotazem. Některé, zejména
\bftt{trigram\_ES}, vrací za cenu vyšší přesnosti více výsledků, aby se zvýšila
šance, že žádný relevantní výsledek nebude vynechán.  V následujících
výsledkových tabulkách je zobrazeno několik prvních výsledků vyhledávání pro
každý vyhledávač s uvedeným počtem celkových nalezených výsledků. Vedle každého
výsledku je uvedeno skóre, který daný vyhledávač přiřadil. Vyšší skóre zde
neznamená větší shodu, ale naopak označuje míru penalizace od perfektní shody.
Tedy menší skóre znamená přesnější shodu. Skóre nelze napříč vyhledávači
porovnávat (s výjimkou \bftt{edit} a \bftt{edit\_dyn}). Slouží jen pro
relativní porovnání shod v rámci jednoho vyhledávače.

\mbox{}\input{hits/forrest_gump.tex}\mbox{}

První příklad (tab.~\ref{tab:result:forrest_gump}) je dotaz, ve kterém hledáme
zahraniční film \bftt{Forrest Gump}, jehož jméno známe a umíme ho správně
napsat.

\section{Přesnost}
\subsubsection{Filmy}
Několik následujících příkladů demonstruje záměrně zkomolené nebo jinak
nepřesné dotazy, od kterých ale beztak očekáváme přesný zásah.
Vyhledávače by neměly vracet příliš mnoho výsledků, protože tyto dotazy
budou relativně dlouhé a specifické, které by v omezených datových
sadách měly odpovídat jen několika shodujícím se záznamům.

\mbox{}\input{hits/smrt_krasneho_srnce.tex}\mbox{}

Tab.~\ref{tab:result:smrt_krasneho_srnce} ukazuje schopnost vyhledávačů
vypořádat se s dotazem v jiném mluvnickém pádu a číslu, než je zamýšlený film.
K povšimnutí stojí rozdíl mezi vyhledávačem \bftt{edit} a jeho dynamickou
variantou \bftt{edit\_dyn}. Dynamická editovací vzdálenost má větší toleranci
pro odlišnosti na koncích slov, proto se dokázala vypořádat s editovací
vzdáleností $3$ u rozdílu mezi \bftt{krásného} a \bftt{krásných}.  \bftt{edit}
používá vzdálenost $3$ až od délky slova $9$, jinak by došlo k obrovskému
nárůstu podobných slov.

\bftt{cs\_ES} nenašel žádnou shodu, přestože by jeho český stematizér měl
tenhle případ zvládnout. Tím, že se jazyková analýza dotazu
(\bftt{smrt~krasnh~srnk}) neshoduje s analýzou výsledku
(\bftt{smrt~krasnych~srnk}), nedojde ke shodě. Vinou zde bude pouze
nedostatečný základní stematizér v ElasticSearch.

\mbox{}\input{hits/sindleruv_seznam.tex}\mbox{}

Protože dynamická editovací vzdálenost upřednostňuje odlišnosti na
koncích slov oproti těm na začátcích, nenajde \bftt{edit\_dyn} shodu
filmu \bftt{Schindlerův seznam} při zkomoleném dotazu \bftt{Šindlerův
seznam}, který odpovídá fonetické reprezentaci - \bftt{Schi} je
nahrazeno českým \bftt{Ši} (tab.~\ref{tab:result:sindleruv_seznam}).

\mbox{}\input{hits/veznice_showsank.tex}\mbox{}

Na jiný zkomolený dotaz \bftt{Věznice Showsank}
(tab.~\ref{tab:result:veznice_showsank}, který by měl nalézt \bftt{Vykoupení z
věznice Shawshank}, kladně reagují pouze vyhledávače \bftt{edit} a
\bftt{edit\_dyn} , protože cizí a specifický název \bftt{Shawshank} neprojde
jazykovou analýzou a trigramový systém selže, protože zasáhne pouze jeden
trigram \bftt{ank}.

Zvýrazňovač výsledků nebere v úvahu odstraněná písmena. Místo toho použije
délku dotazovaného slova nebo nejbližší mezeru, pokud je slovo delší. Kvůli
tomu není poslední písmeno slova \bftt{Shawshank} zvýrazněno, přičemž by mělo
být.

\mbox{}\input{hits/vezeni_showsank.tex}\mbox{}

Při zkomolení druhého slova už reaguje pouze \bftt{edit\_dyn}
(tab.~\ref{tab:result:vezeni_showsank}), protože ke zkomolení došlo na
konci slova.

\mbox{}\input{hits/prelet_kukacka.tex}\mbox{}

\begin{table}[H]
\begin{tt}
\horizlina

\bftt{edit\_dyn} [2 nalezeno]\vspace{5pt}

\begin{tabulary}{1.1\textwidth}{LL}
11.64 & \boldred{Přelet} nad \boldred{kukaččí}m hnízdem \\
11.87 & Byl jednou jeden film: \boldred{Přelet} nad \boldred{kukaččí}m hnízdem \\
\end{tabulary}
\horizlina

\end{tt}
\caption{Výsledek dotazu \bftt{Přeletěla kukajda}}
\label{tab:result:preletela_kukajda}
\end{table}

U dotazu \bftt{přelet~kukačka} v tab.~\ref{tab:result:prelet_kukacka} opět
selhal stematizér češtiny. Dotaz se převedl na \bftt{prelt~kukack}, zatímco
záznam \bftt{Přelet~nad~kukaččím~hnízdem} byl při indexování stematizován na
neshodující se \bftt{prelt~nad~kukaccim~hnizd}. \bftt{edit\_dyn} našel shodu i
pro velmi zkomolený dotaz (tab.~\ref{tab:result:preletela_kukajda}).

\mbox{}\input{hits/gottfather.tex}\mbox{}

Pokud chceme najít film s anglickým názvem \bftt{The Godfather} a nemůžeme si
vzpomenout, jak se to vlastně píše (př. \bftt{Gottfather}), vyhledávače
využívající editovací vzdálenost mohou napovědět. Trigramový vyhledávač
\bftt{trigram\_ES} nenašel shodu, kterou jsme hledali, ale několik textově
správných výsledků také našel.  (tab.~\ref{tab:result:gottfather}).

\mbox{}\input{hits/god_father.tex}\mbox{}

Obecně je u invertovaných indexů problém s oddělenými slovy. \bftt{edit},
\bftt{edit\_dyn} a \bftt{cs\_ES} indexují celá slova, zatímco
\bftt{trigram\_ES} indexuje trigramy. Příklad v
tab.~\ref{tab:result:god_father} je ukázkou, jak se vyhledávače zachovají, když
je cílovým dokumentem opět film \bftt{The Godfather}, ale dotaz je napsán jako
dvě oddělená slova \bftt{god} a \bftt{father}, ze kterých je kompozitní slovo
\bftt{godfather} složeno.

\bftt{trigram\_ES} byl schopen nalézt 1005 dokumentů s vysokou přesností k
našemu očekávání. Ostatní rovněž nalezly několik dokumenťů, ale až na pár
výjimek, které v textu obsahují kompozitní slovo oddělené pomlčkou, se k cíli
ani nepřibližíly.

\subsubsection{Jména}
Pro databázi jmen byla zvýšena tolerance překlepů v editační vzdálenosti pro
kratší slova, protože jména jsou obvykle krátká proti názvům filmů. Menší
datová sada vykompenzuje nadbytečný výpočetní výkon, který se použije pro vyšší
editační vzdálenost.

\mbox{}\input{hits/paul_mccartney.tex}\mbox{}

Od všech vyhledávačů opět požadujeme alespoň přesnou shodu
(tab.~\ref{tab:result:paul_mccartney}).

\mbox{}\input{hits/paul_mccountry.tex}\mbox{}

Pro ukázku fuzzy hledání v databázi jmen je dotazem zkomolené jméno
\bftt{Paul McCountry}, které by mělo nalézt \bftt{Paul McCartney}
(tab.~\ref{tab:result:paul_mccountry}).

\mbox{}\input{hits/sean_konery.tex}\mbox{}

Dynamická editační vzdálenost u \bftt{edit\_dyn} se obtížně vypořádává s
krátkými jmény jako \bftt{Sean} u \bftt{Sean Connery} a s fonetickým přepisem
přepisem příjmení jako \bftt{Konery} (tab.~\ref{tab:result:sean_konery}). Běžná
editační vzdálenost neupřednostňující změny na koncovkách se s fonetickým
případem vypořádá opět za cenu toho, že najde velké množství nesouvisejících
záznamů.

Žádný vyhledávač se neuplatnil u plně fonetického přepisu u dotazu \bftt{Šón
Konery}. Musela by být silně zvýšena tolerance editační vzdálenosti pro krátká
slova (\bftt{son} a \bftt{sean} se liší vzdáleností $2$). Nevýhodou by bylo velké
množství nevyhovujících záznamů.

\subsubsection{Názvy produktů}

\mbox{}\input{hits/intel_8086.tex}\mbox{}

Fuzzy vlastnost vyhledávače umožňuje explorativní vyhledávání podobných pojmů
těm, které by uživatel ani nemohl najít, narozdíl od běžných slov, v jiném
slovníku. Dostal by se k nim třeba přes seznam obdobných pojmů, které se
uvádějí v encyklopediích, nebo jaké nakupujícím představují eshopy (např.
\uv{sekce ostatní také koupili}). Příklad v tab.~\ref{tab:result:intel_8086}
ukazuje tuto situaci na produktu s technickým názvem, který má od stejné firmy
několik produktů s podobným nebo odvozeným názvem. Příklad v
tab.~\ref{tab:result:mitsubishi_ki} ukazuje, jak může fuzzy vyhledání u
\bftt{edit} a \bftt{edit\_dyn} pomoci, když je v databázi název firmy produktu
ve více jazycích, přestože se data tváří, že jsou pouze v jednom jazyce.
\bftt{trig\_ES} případ nezvládl, protože nelze jednoduše nakonfigurovat dotaz,
který by vyžadoval alespoň částečný zásah od každého slova dotazu.

\mbox{}\input{hits/mitsubishi_ki.tex}\mbox{}




\section{Proximita slov}
Rozdíl mezi klasickým upřednostňováním počtu shodujících se slov v dokumentu
(\bftt{tf}) a ohodnocením na základě maximální blízkosti slov (pomocí metody
minimálního intervalu, viz~\S\,\ref{sec:find_minimal_interval}) demonstrují dva
příklady v tab.~\ref{tab:result:the_live}~a~\ref{tab:result:the_live_tour}.
Zatímco první metoda preferuje delší dokumenty, protože mají větší šanci
obsahovat shodující se slova, druhá je k tomuhle imunní. Pro ni je ideální
taková shoda, kde jsou slova v dokumentu bezprostředně vedle sebe a navíc ve
stejném pořadí jako v dotazu.

Pro tento příklad byla záměrně použita slova, která se ve filmových datech
objevují frekventovaně díky tomu, že se mezi filmy řadí i záznamy hudebních
koncertů. U běžného dotazu nelze pozorovat, že by proximita měla zásadní dopad
na kvalitu výsledků právě kvůli krátkosti dokumentů. Smysl by pravděpodobně
měla při použití na delším textu, kde by se ale projevila její relativně vyšší
výpočetní náročnost.

%\mbox{}\input{hits/co_nam.tex}\mbox{}

\mbox{}\input{hits/the_live.tex}\mbox{}

\mbox{}\input{hits/the_live_tour.tex}\mbox{}


\section{Porovnání trigramových vyhledávačů}
Pro srovnání schopností trigramového indexu Postgresu (\bftt{trig\_PG})
a trigramové konfigurace pro ElasticSearch (\bftt{trig\_ES}) bylo
provedeno několik dotazů, které poukazují na slabost ostatních
vyhledávačů. Těmi jsou kompozitní slova.
Tab.~\ref{tab:result:trig_god_father} ukazuje, že oba systémy si podle
očekávání s kompozitními slovy poradí. \bftt{trig\_ES} má oproti běžnému
trigramovému indexu navíc nakonfigurovanou preferenci přesných shod, což
mu může dávat výhodu v přesnosti. Přestože pouze dva výsledky obsahují
přesnou shodu ve slovech \bftt{god} a \bftt{father}, zdá se, že
\bftt{trig\_ES} vrací celkově přesnější výsledky.
Tab.~\ref{tab:result:trig_auto_mobil} ukazuje podobný výsledek pro dotaz
\bftt{auto mobil}.

\mbox{}\input{hits/trig_god_father.tex}\mbox{}

\mbox{}\input{hits/trig_auto_mobil.tex}\mbox{}



\clearpage
\section{Rychlost vyhledávání} \label{sec:rychlost_vyhledavani}

%\mbox{}\input{figures/timings.tex}\mbox{}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost dotazu \bftt{[ms]}}
    \label{fig:timings}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost dotazu \bftt{log[y] [ms]}}
    \label{fig:timings_log}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Pro porovnání výkonu bylo náhodně zvoleno 24 dotazů pro vyhledání v
databázi \bftt{csfd\_filmy}. Obrázky \ref{fig:timings} a
\ref{fig:timings_log} porovnávají rychlost běhu dotazu mezi systémy
\bftt{edit\_dyn} interpretovaného v CPythonu, \bftt{edit\_dyn}
interpretovaného v PyPy, dvou konfigurací pro ElasticSearch
(\bftt{trig\_ES} a \bftt{cs\_ES}) a trigramového vyhledávače v Postgresu
(\bftt{trig\_PG}).  Časy jsou průměrem 100 běhů dotazu pro každý ze
systémů.

Podle očekávání je implementace \bftt{edit\_dyn} výkonově nejslabší kvůli
dynamicky typovanému Pythonu. Znatelný je rozdíl mezi jednotlivými interpretery
Pythonu. Zejména pro experimentální korelované dotazy \bftt{the of}, \bftt{the
they} a \bftt{the than then they thel them thun}, jimž odpovídá v
obr.~\ref{fig:timings_log} špička dosahující přes 4000\,ms. Tyto dotazy byly
záměrně zkonstruovány z nefrekventovanějších slov \bftt{the} a \bftt{of}, aby
ukázaly na nejhorší možný případ. Oproti CPythonu dokáže PyPy rozpoznat
opakující se kód průchodu invertovanými seznamy a efektivně ho vykonat, jak je
patrné z relativního rozdílu těchto dotazů oproti ostatním.

\bftt{trig\_ES} je podle očekávání pomalejší než základní \bftt{cs\_ES}, ale do
takové míry, kdy to pro malou datovou kolekci nevadí. Trigramový vyhledávač v
Postgresu \bftt{trig\_PG} dosahuje obecně horšího výkonu než jeho protějšek v
ElasticSearch \bftt{trig\_ES}.


\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_py_sorted.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro CPython \bftt{log[x]}}
    \label{fig:timings_py_sorted}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_pypy_sorted.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro PyPy \bftt{log[x]}}
    \label{fig:timings_pypy_sorted}
    \end{figure}
    \end{minipage}}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_py_sorted_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro CPython \bftt{log[x,y]}}
    \label{fig:timings_py_sorted_log}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/timings_pypy_sorted_log.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost fází \bftt{edit\_dyn} pro PyPy \bftt{log[x,y]}}
    \label{fig:timings_pypy_sorted_log}
    \end{figure}
    \end{minipage}}
\mbox{}\\

\subsection{Rozdělení náročnosti jednotlivých fází} \label{sec:timings_query_parts}
Poměr všech tří fází vykonání dotazu (1. vyhledání podobných slov ve slovníku
\bftt{trie}, 2. sloučení všech odpovídajících invertovaných seznamů
\bftt{slouceni}, 3. ohodnocení nalezených dokumentů \bftt{ohodnoceni}) na
obrázcích \ref{fig:timings_py_sorted}, \ref{fig:timings_pypy_sorted},
\ref{fig:timings_py_sorted_log} a~\ref{fig:timings_pypy_sorted_log} je odlišný
pro různé dotazy. Fáze \bftt{ohodnoceni} je přímo závislá na počtu nalezených
výsledků, protože se pro každý z nich aplikuje stejná ohodnocující funkce a
následné seřazení.

Ostatní fáze nejsou na počtu výsledků závislé. \bftt{slouceni} není tolik
závislé díky předsloučeným invertovaným seznamům hybridního uspořádání. Zvýšení
náročnosti je možné pozorovat pro dotazy s frekventovanými slovy \bftt{the} a
\bftt{of}, protože jejich invertované seznamy a disjunkce seznamů s podobnými
slovy jsou delší než u průměrného dotazu. Fáze prohledání všech slov v lexikonu
- fáze \bftt{trie} - se odvíjí pouze od charakteristiky dat. Tedy pokud
existuje více podobných slov, pak je vykonání delší. Přesto nedosahuje
velikých výkyvů.

Za zmínku stojí změna v pořadí fází \bftt{trie} a \bftt{slouceni} mezi
interpretery CPython a PyPy. PyPy si lehce poradí se sloučením invertovaných
seznamů a dominující fází se u něj stane ne tak snadno predikovatelné
prohledání ve slovníku.


%\subsubsection{Porovnání implementací disjunkce}
%\noindent\makebox[\textwidth][c]{%
%    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
%    \begin{figure}[H]
%    \includegraphics[width=\textwidth]{figures/timings_union_cpython.eps}
%    \captionsetup{justification=centering}
%    \caption{Rychlost sloučení pro CPython}
%    \label{fig:timings_union_cpython}
%    \end{figure}
%    \end{minipage}
%    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
%    \begin{figure}[H]
%    \includegraphics[width=\textwidth]{figures/timings_union_pypy3.eps}
%    \captionsetup{justification=centering}
%    \caption{Rychlost sloučení pro PyPy}
%    \label{fig:timings_union_pypy}
%    \end{figure}
%    \end{minipage}}
%\mbox{}\\

Pro srovnání byly naimplementovány čtyři různé druhy vykonání disjunkce
invertovaných seznamů, které následně figurují v konjunktivním sloučení.
Metoda \bftt{heap} je klasické sloučení za pomocí binární haldy, které se hodí,
pokud je invertovaných seznamů mnoho. Kvůli hybridizaci invertovaných seznamů
jich ale není tak mnoho, a proto by měly být minimálně stejně účinné
implementace, kde prioritní frontu tvoří pouhý lineární seznam - metoda
\bftt{list}. \bftt{list-prune} je obdobou, pouze s tím, že z prioritní fronty
odstraní invertované seznamy, které se dostaly do konce. \bftt{skip} je
experimentální implementace optimalizující přeskakování v invertovaných
seznamech oproti postupnému posunování po jednom dokumentu.

% V obr.~\ref{fig:timings_union_cpython}~a~\ref{fig:timings_union_pypy} jsou
% porovnání těchto implementací pro CPython a Pypy na dvaceti náhodně
% vygenerovaných dotazech. Obrázky jsou seřazeny vzestupně podle implementace
% \bftt{skip}. Žádná ze čtyř implementací nemá dle výsledků v obecném případě
% zásadní vliv na dobu vykonání dotazu. Každá je v některém případě víceméně tou
% nejméně vhodnou a naopak.

\newpage
\subsection{Rychlost sloučení}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_dense.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* y*]}}
    \label{fig:timings_prefix_union_x_y}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* yy*]}}
    \label{fig:timings_prefix_union_x_yy}
    \end{figure}
    \end{minipage}}

\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse2.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[x* yyyy*]}}
    \label{fig:timings_prefix_union_x_yyyy}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/prefix_union_sparse3.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[xxxx* yyyy*]}}
    \label{fig:timings_prefix_union_xxxx_yyyy}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Obrázky~\ref{fig:timings_prefix_union_x_y},~\ref{fig:timings_prefix_union_x_yy},~\ref{fig:timings_prefix_union_x_yyyy}
a~\ref{fig:timings_prefix_union_xxxx_yyyy} demonstrují rychlost vykonání
náhodných prefixových dotazů. U prefixových dotazů najde slovník velké množství
odpovídajících slov ke každému dotazovanému slovu. Dotaz typu \bftt{x* yyyy*}
v~obr.~\ref{fig:timings_prefix_union_x_yyyy} znamená dotaz o dvou prefixech.
\bftt{x} a~\bftt{yyyy} znázorňují šablony vygenerovaných slov. Tedy náhodně
vygenerovaný dotaz může vypadat například jako \bftt{s* bene*} nebo \bftt{m*
tolu*}. U dotazu \bftt{x* y*} očekáváme silný vliv disjunkce na dobu vykonání
sloučení, protože prefix pouze o jednom znaku odpovídá velkému množství slov. V
kontrastu dotaz typu \bftt{xxxx* yyyy*}  bude obsahovat menší množství seznamů
v disjunkci. Dotazy typu \bftt{x* yy*} a \bftt{x* yyyy*} slouží k porovnání
schopnost implementací přeskakovat dokumenty v kombinaci řídké (\bftt{yyyy*}) a
husté (\bftt{x*}) disjunkce. Podle původního očekávání měla dominovat
implementace \bftt{skip}, ale není tomu tak. Nejvhodnější implementací se
ukázala jednoduchost obyčejného seznamu v případech \bftt{list} a
\bftt{list-prune}.

\subsection{Výkon hybridních seznamů}
\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/hybrid_union_s_tebou.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[s* tebou*]}}
    \label{fig:hybrid_union_s_tebou}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/hybrid_union_s_t.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost sloučení pro PyPy - prefixový dotaz \bftt{[s* t*]}}
    \label{fig:hybrid_union_s_t}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Klíčovým srovnáním této práce je závislost rychlosti sloučení invertovaných
seznamů na velikosti seskupení invertovaných seznamů
(viz~\S\,\ref{sec:hybrid_algorithm}).  Pro porovnání byla datová sada
\bftt{csfd\_filmy} zaindexována pro více parametrů seskupení a pro každý takový
index byla změřena doba sloučení pro prefixové dotazy \bftt{s* tebou*}
(reprezentující typ dotazu \bftt{x* yyyyy*},
obr.~\ref{fig:hybrid_union_s_tebou}), a dotaz \bftt{s* t*} (reprezentující
\bftt{x* y*}, obr.~\ref{fig:hybrid_union_s_t}). Parametrem seskupení se rozumí
minimální velikost seskupeného invertovaného seznamu. Tedy klasický invertovaný
index bez hybridního sloučení odpovídá hodnotě 1. Naopak úplný dopředný index
má maximálně možně dlouhý invertovaný seznam o velikosti počtu všech slov v
datové sadě (v tomto případě je v databázi \bftt{csfd\_filmy} 1\,958\,147
výskytů slov). Lidsky řečeno, čím více vlevo v
obr.~\ref{fig:hybrid_union_s_tebou}~a~\ref{fig:hybrid_union_s_t}, tím je index
\uv{více invertovaný}. Naopak, čím více vpravo, tím index více připomíná
původní datovou kolekci. Křivky jsou v hodnotách 500\,000 a výše téměř totožné,
protože pro oba dotazy už jsou jejich invertované seznamy předsloučené a při
průchodu už dochází pouze k jejich filtrování.

Zajímavým pozorováním je absolutní nevhodnost klasického invertovaného seznamu
pro krátké prefixové dotazy. V takovém případě dochází k masivnímu sloučení
mnoha krátkých invertovaných seznamů. Podle očekávání zde uplatňuje
implementace \bftt{heap}, jejíž složitost nalezení následujícího dokumentu je
$\log n$ oproti ostatním, které procházejí disjunktivní seznam lineárně.



\subsection{Hledání ve slovníku}
\noindent\makebox[\textwidth][c]{%
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/trie_edit_cpython.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost trie (CPython)~\bftt{log[y]}}
    \label{fig:trie_edit_cpython}
    \end{figure}
    \end{minipage}
    \begin{minipage}[H]{.5\textwidth}\vspace{0pt}
    \begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/trie_edit_pypy3.eps}
    \captionsetup{justification=centering}
    \caption{Rychlost trie (PyPy)~\bftt{log[y]}}
    \label{fig:trie_edit_pypy}
    \end{figure}
    \end{minipage}}
\mbox{}\\

Jedním z důvodů, proč je dynamicky prořezávaná editovací vzdálenost výhodná je
její časová náročnost, protože se, narozdíl od běžné, vyhýbá drahému průchodu
podstromy u počátečních znaků.
Obr.~\ref{fig:trie_edit_pypy}~a~\ref{fig:trie_edit_cpython} znázorňují
závislost tří variant editovací vzdálenosti při průchodu prefixovým stromem a
nalezením všech podobných slov. \bftt{edit} je klasická Levenshteinova
vzdálenost, \bftt{edit-dyn} je dynamicky prořezávaná varianta a
\bftt{edit-dyn-prefix} je předchozí varianta, která vyhledává prefixově.
Vertikální čáry znázorňují zvýšení práhu editovací vzdálenosti. Ta začíná na
nule - tedy nejkratší slova se hledají přesně, a na délkách 2, 4, 6, 8 se
zvedne o jeden bod (slova o délce 3 se hledají s práhem 1, o délce 5 s práhem
2, ...). Dlouhé vykonání u krátkých slov u \bftt{edit-dyn-prefix} je způsobeno
vyčerpávajícím průchodem všemi slovy, které odpovídají prefixu. Dynamické
prořezávání je výhodné, protože umožňuje bez větší ztráty vyšší toleranci
editační vzdálenosti s rostoucí délkou slova.

Narozdíl od sloučení invertovaných seznamů není u vyhledání ve slovníku patrná
výhoda PyPy oproti CPythonu. Dokonce je u \bftt{edit-dyn} a
\bftt{edit-dyn-prefix} CPython výkonnější, což naznačuje, že JIT kompilace PyPy
v tomto případě nepřinesla žádný lepší výkon. Implementace slovníku v jazyce s
překladem do nativního kódu by měla být znatelně rychlejší.



\newpage
\subsection{Velikost indexu}
ElasticSearch nabízí ve svém programovatelném rozhraní bod se statistikami,
které nám řeknou i velikost místa zabraného indexem. Alternativně lze velikost
indexu zjistit sečtením velikostí všech uložených souborů. Obě metody vracejí
zhruba stejné výsledky ($\pm 4\%$).

\begin{Verbatim}
# Součet souborů na disku
du -shc //elasticsearch/nodes/*/indices/<index>/*/index/*

# Využití statistik skrz API
http '<uri>/<index>/_stats' | jq .indices.<index>.primaries.store.size_in_bytes
\end{Verbatim}

\begin{tt}
\begin{table}[H]
\centering
\begin{tabulary}{\textwidth}{LLRR}
\textbf{databáze} & \textbf{implementace} & \textbf{velikost indexu} \\
\hline
\multirow{3}{*}{\texttt{csfd\_filmy}} & trigram\_ES & 86\,MB \\
                                      & cs\_ES      & 37\,MB \\
                                      & edit        & 42\,MB \\
\hline
\multirow{3}{*}{\texttt{csfd\_tvůrci}} & trigram\_ES & 31\,MB \\
                                       & cs\_ES & 14\,MB \\
                                       & edit & 15\,MB \\
\hline
\multirow{3}{*}{\texttt{cswiki\_titulky}} & trigram\_ES & 135\,MB \\
                                          & cs\_ES & 63\,MB \\
                                          & edit & 63\,MB \\
\hline
\end{tabulary}
\caption{Velikosti indexů}
\label{tab:index_size}
\end{table}
\end{tt}

V~tab.~\ref{tab:index_size} je vidět, že indexy pro vyhledávače \bftt{edit} a
\bftt{edit\_dyn} jsou totožné. Liší se pouze formou hledání podobných slov ve
slovníku. Nutné je podotknout, že prototyp navrhovaného systému reprezentovaný
těmito dvěma vyhledávači uchováva data naivně v textové reprezentaci bez
jakékoliv komprese. Překvapivě se pohybuje ve stejné kategorii jako index
vyhledávače \bftt{cs\_ES}, přestože by měly být jeho hodnoty kvůli absence
efektivního binárního formátu o poznání větší.  Trigramový index je podle
očekávaní větší než běžný index. Jednak z důvodu, že trigramy se překrývají, a
proto se indexuje větší množství dat, a jednak protože konkrétní implementace
\bftt{trigram\_ES} sestává dohromady z více indexů (trigramový, celá slova a
ngramy s n kratším než 3.  viz~\S\,\ref{sec:trigram_implementation}).

\chapter{Závěr}

\section{Další postup v implementaci}
Častečná implementace zde navrhovaného systému zatím není vhodná pro produkční
nasazení zejména z výkonnostních důvodů. V této části bude představeno několik
možností, jakými cestami by se mohla dosavadní implemtance tohoto sytému dále
vyvíjet. Vyhledávač by mohl být přepsán ve výkonějším jazyce než Python a
vyvíjen jako samostatný systém, nebo by mohl být integrován do existujících
databázových nebo vyhledávacích systémů.

\subsubsection{Integrace do existujících systémů}
Systém byl navržen tak, aby byl ideálně jazykově nezávislý, čehož dosahuje díky
možnosti přibližného vyhledávání. To ho činí vhodným kandidátem pro integraci
do open source databázových systémů, které nejsou výhradně zaměřeny na
zpracování textu. Díky jednoduchosti nasazení bez nutnosti žádné složité
textové analýzy před indexováním by mohl být tento systém implementován
jako plugin podobně jako trigramový plugin do systému Postgres, který zde
byl použit pro porovnání.

Integrace do specializovaných vyhledávačů by paradoxně mohla být obtížnější,
protože jsou silně specializované na některé specifické datové struktury.
Vytvoření slovníku s podporou fuzzy hledání a implementace hybridních
invertovaných seznamů - oproti klasickému invertovanému indexu - by mohla
znamenat významný zásah do celkové architektury a prakticky znamenala vytvoření
zcela nového systému.

\subsubsection{Částečná integrace}
Alternativně by mohl být nový systém částečně integrován do existujících
databázových systémů tak, že by v nich byly uloženy pouze invertované seznamy,
které tvoří většinu indexu. Slovník a část věnující se ohodnocování výsledků by
byla implementována externě. Tenhle přístup by vyřešil problém distribuovaného
indexu při použití více počítačů najednou.

Sloupcové databáze v principu uchovávají data v komprimované formě stejné jakou
používají invertované indexy pro své seznamy. Podle některých dochází k
postupnému slučování oborů zabývajícími se databázemi a textovým vyhledáváním
právě díky obdobné architektuře používaných systémů. Invertované seznamy mohou
být ve sloupcové databázi uloženy v sekvenčním pořadí. Vytvoření jednoho
sloupce pro jeden invertovaný seznam není prakticky proveditelné, protože počet
slov typické textové kolekce významně překračuje povolené počty sloupců v
současných systémech. Kvůli zipfově zákonu je většina invertovaných seznamů
velmi krátká a tím, že sloupcové databáze uchovávají data v blocích o nějaké
minimální velikosti, došlo by k velkému plýtvání~\cite{Bjorklund_aconfluence}.

Hybridní index by mohl vyřešit problém velkého počtu sloupců, protože už z
principu omezuje jejich počet tím, že je slučuje dohromady. Kombinace
hybridních invertovaných indexů a sloupcových databází nabízí cestu pro další
výzkum s řadou praktických aplikací.

\subsubsection{Samostatný systém}
Systém by mohl být dále vyvíjen jako samostatný systém. Pro implementaci by to
znamenalo vytvoření efektivního procesu indexování, uchovánání dat v
perzistentním úložišti, distribuci indexu na více počítačů, podporu dalších
netextových datových typů a vyřešení flexibility při vytváření různých typů
dotazu.

\section{Otevřené problémy}
Zde prezentovaný systém cílí na řešení základních otázek okolo prefixového,
přibližného vyhledávání. Nový systém sice nabízí efektivnější odpovědi na
pokročilé dotazy, ale s nimi přichází další sada problémů. Těmi se při použití
klasického invertovaného indexu často ani nemusíme zabývat, protože se pro
některé pokročilé dotazy ani nepoužívají.

% static: no locks and latches, compact data structures, cache efficient
% 2 level data structure + bulk updates
% columnar storage
% document or term based
% share nothing
\subsubsection{Příliš krátký vstupní řetězec}
Obtížná situace u prefixového vyhledávání je pokud dotaz obsahuje pouze jedno
slovo, které navíc obsahuje pouze první znak (např. dotaz \bftt{s*}). Výhoda
invertovaného indexu je, že dokáže efektivně zúžit počet výsledné množiny.
Prefixový dotaz tohoto typu prování minimální prořezávání Nalezení všech
výsledků je náročné i v optimálním případě, kdy je výpočetní náročnost určena
pouze velikostí nalezené množiny.

V případě, že použijeme prefixovou schopnost vyhledávače pro vyhledávání typu
search-as-you-type, pak v každém případě uživatel pocítí počáteční zpoždění,
jakmile začne vkládat první znak svého dotazu. U dalších znaků pak dojde k
výraznému zrychlení.

Řešením je nevracet celou množinu, ale problém nějak omezit. Např.
implementovaný prototyp nepovoluje prefixové dotazy, které jsou definované
pouze počátečním písmenem. Místo nich provede přepsání prefixového dotazu na
běžný (např. \bftt{s*} $\rightarrow$ \bftt{s}). Ve většině případech není změna
uživatelsky vnímatelná, protože datové sady obsahují spoustu jednopísmenných
slov. V případech vzácněji se vyskytujících písmen může tento způsob
pokulhávat.

Pokud bychom chtěli, aby systém systém fungoval zaručeně bez velkých zpoždění,
mohli bychom implementovat slučování invertovaných systémů typu Top-K (viz
\S\,\ref{sec:ohodnoceni_vysledku}) a jednoduše omezit počet vrácených výsledků
na prvních $k$ nejvýše ohodnocených.

\subsubsection{Korelovaná slova}
V \S\,\ref{sec:correlated_words} byl naznačen problém, který poukazuje na možný
útok proti vyhledávači tím, že ho zahltí spoustou pomalých dotazů. Nejhorší
případ pomalých dotazů lze vytvořit nějakého slova s malými změnami, jejichž
disjunkce invertovaných seznamů se z velké části překrývají. Tím, že jsou téměř
totožné, nedojde k efektivnímu konjunktivnímu filtrování, a tím naroste
náročnost vykonání dotazu. Protože jsou při slučování seznamů použity pouze
jednoduché booleovské operátory AND, OR a případně NOT, mohli bychom využít
jejich vlastností a dotaz podle nich přepsat. Např. invertované seznamy dvou
korelovaných slov \bftt{them} a \bftt{they} budou odpovídat sdílené části
seznamu $X$ a částem, které nesdílí - respektive $A$ a $B$. Klasické vykonání
konjunkce disjunkcí by mělo tvar \ref{eq:puvodni}:
\begin{align}
\bftt{(X OR A) AND (X OR B)} \label{eq:puvodni} \\
\bftt{(X AND X) OR (X AND A) OR (X AND B) OR (X AND AB)} \label{eq:dlouhy} \\
\bftt{X OR (A AND B)} \label{eq:kratky}
\end{align}

Pomocí booleovského kalkulu ho můžeme změnit na \ref{eq:dlouhy}, nebo rovnou do
minimální formy \ref{eq:kratky}. Poslední forma ukazuje, jak zabránit pomalým
dotazům, pokud bychom potřebovali zjistit ty dokumenty, ve kterých se slova
\bftt{them} a \bftt{they} nalézají. Jenže vyhledávač není pouze booleovský, ale
zaznamenává si navíc pozice výskytů pro ohodnocování a zvýrazňování, proto není
jednoduché přepsání dotazu takhle triviální. Přesto tato myšlenka může být
odrážecím můstkem, jak tenhle problém v budoucnu vyřešit.

\subsubsection{Kompozitní slova}
Tím, že index uvažuje text jako posloupnost slov, dělají mu potíže složená
slova, která vznikla ať už přirozeně složením dvou kratších slov nebo kvůli
překlepu. Např. páry dotazů (\bftt{i phone}, \bftt{iphone}) nebo
(\bftt{godfather}, \bftt{god father}) vyhodnotí vyhledávač naprosto odlišně,
přestože intuitivně cítíme, že se jedná o víceméně tentýž dotaz. Ngramové
vyhledávače neuvažují čistě jako slova a díky tomu dokážou rozpoznat i mezeru
jako překlep. Obohacením vyhledávače o ngramový vejdlejší index by mohlo pomoci
s těmito případy.

Jiná možnost je rozpoznat složená slova ještě před vykonáním dotazu a rozložit
je. Každé slovo je možné předem uvažovat jako složené a systém pro přepisování
dotazu by se snažil najít rozdělení na podslova, které maximalizuje frekvenci
vzniklých slov. Např. \bftt{godfather} můžeme rozdělit na \bftt{g odfather},
\bftt{go dfather}, \bftt{god father}, \bftt{godf ather}, \bftt{godfa ther},
\ldots s tím, že ne všechna rozdělení vytváří slova, která se nachází ve
slovníku, a proto je může rovnou zahodit. V opačném směru - tedy kdy dokument
obsahuje kompozitní slovo (\bftt{godfather}), ale uživatel zadal rozložené na
podslova (\bftt{god father}) - je možné uvažovat všechny páry sousedících slov
a zkusit stejným způsobem najít, jestli jejich sloučení produkuje smysluplné
slovo.

\subsubsection{Ukládání inkrementálních mezivýsledků}
Tento problém byl krátce naznačen v \S\,\ref{sec:inkrementalni_vyhledavani}.
Vyhledávač s podporou search-as-you-type dotazů může využít výsledku hledání
kratšího prefixu pro výpočet výsledků delšího prefixu. Pro příklad mějme dotaz
\bftt{já jsem doma} a předpokládejme, že ho uživatel vkládá postupně po
slovech. Před tím, než uživatel zadá celý dotaz, dostane se do momentu, kdy mu
vyhledávač vrátí výsledky prefixu \bftt{já jsem}. Vyhledávač může postupného
zadávání využít a uložit si mezivýsledky tohoto prefixu do krátkodobé
mezipaměti. Jakmile uživatel zadá poslední slovo, stačí použít mezivýsledek,
najít poslední slovo \bftt{doma} a provést sloučení invertovaných seznamů.
Tuhle techniku využívají fuzzy vyhledávací systémy
CompleteSearch~\cite{Bast:2006:TLF:1148170.1148234}
a~Tastier~\cite{Ji:2009:EIF:1526709.1526760}.

%\subsubsection{Zvýrazňovač pro editační vzdálenost}


%\subsubsection{Indexování}
%Implementace je zaměřená především na ty části, které jsou klíčové pro
%demonstraci návrhu. Indexovací fáze používá jednoduchý \bftt{sort based}
%algoritmus a nevyužívá techniky více souborů. Do budoucna by bylo užitečné
%zjistit, jaké jsou nejvhodnější indexovací metody pro hybridní index oproti
%klasickému invertovanému indexu.
%
%\subsubsection{Porovnání výkonu}
%Algoritmy pro slučování invertovaných seznamů jsou obzvlášť citlivé na správnou
%implementaci a především na zvolený programovací jazyk. Zatímco Python je
%ideální prototypovací jazyk, je asi tím nejhorším, v čem by měl být produkční
%vyhledávací systém vytvořen. To se sice časem může změnit díky výkonově
%orientovanému interpreteru PyPy. Přesto by měla být pro adekvátní porovnání
%implementace provedena v jazyce s podporou překladu do nativního strojového
%kódu.
%
%Se zpomalujícím Mooreovým zákonem se objevují různé formy paralelismu, které by
%měly sloužit jako náhrada za klesající tempo růstu. Pro pečlivě optimalizovaný
%kód, které databázové a vyhledávací systémy často využívají, se častěji a
%častěji využívají bitově-paralelní instrukce moderních procesorů. I z tohoto
%důvodu by měl být zvolen takový jazyk, ve kterém není velkou překážkou tyto
%moderní schopnosti procesorů využívat. (cite completesearch C++).


\section{Shrnutí}
Záměrem této práce je poukázat na alternativní techniky v oblasti vyhledávání a
demonstrovat možný vyhledávací systém, který je implementuje. Tento systém by
měl být vhodný pro potřeby jednotlivců i firem, které vyžadují od vyhledávacího
systému robustnost proti malým textovým změnám v dotazu. Toho dosahuje
zaměřením se na techniky pro prefixové a přibližné vyhledávání. Uplatnění by
měl nalézt v menších datových sadách, než na které je dnes kladen důraz,
protože díky omezené velikosti dat je možné přebytečný výpočetní výkon použít k
těmto náročnějším technikám.

% Některé z popisovaných technik byly dlouho opomíjené, protože jejich
% implementace není jednoduchá, nebo dlouho nebyla výpočetně výhodná. Pokrok ve
% výpočetním výkonu, nových počítačových architekturách, vyšších kapacitách
% pamětí a obecně pokrok v hardwaru pro ně ale poskytuje novou příležitost.

Navržený vyhledávací systém byl implementován jako prototyp obsahující popsané
techniky a porovnán s několika dalšími vyhledávači, které byly vytvořeny jako
různé konfigurace v některých populárních open source vyhledávacích systémech
tak, aby odpovídaly podobným vlastnostem.

Implementovaný prototyp byl vytvořen tak, aby byl schopný v první řadě
demonstrovat popsané techniky a byl dostatečně flexibilní pro potřeby
experimentování. Výkonově není reprezentativní kvůli implementaci v zvoleném
dynamicky typovaném programovacím jazyce Python, ale byla použita stejná
architektura, kterou by bylo možné v další fázi pouze přepsat do výkonnějšího
jazyka. Bylo provedeno relativní porovnání výkonu mezi určitými typy dotazů s
vyšší mírou tolerance pro běh dotazu, které ukázalo že pro běžný dotaz by
vyhledávač měl splňovat kritéria pro vyhledávání v reálném čase.

Analýza nejhorších případů ukázala na typ dotazů, které by mohly být zneužity
proti vyhledávači. Pro tento případ byl nastíněn způsob, jak ho řešit, přestože
by mohl zkomplikoval původně jednoduchou architekturu.

V porovnání s alternativami založených na ngramové podobnosti není navržený
systém robustní v případech oddělených slov nebo naopak kompozitních slov.
Možným dalším rozšířením by byla kombinace navržené architektury s ngramy.

%\medskip
% prevent page breaks in bibitems
\interlinepenalty=10000

\bibliographystyle{myplainnat}
\renewcommand{\refname}{Reference}
\renewcommand{\bibname}{Reference}
\setlength{\bibsep}{0pt}
\bibliography{ref}

%\appendix
%\chapter{Konfigurace porovnaných systémů v ElasticSearch}\label{appendix:search_config}
%\section{ElasticSearch}
%\subsection{trigram\_ES}
%\subsubsection{Analyzér}
%Pro ngramové zaindexování lze použít buď ngramový tokenizér
%\bftt{my\_ngram\_tokenizer}, nebo ngramový filtr \bftt{my\_ngram\_filter}.
%Ngramový tokenizér podporuje zvýrazňování, které se mi s ngramovým filtrem
%nepovedlo nastavit ani po vyčerpávajícím hledání. Ve výpisu konfigurace jsou
%uvedeny oba způsoby nastavení. \bftt{my\_short\_words\_filter} vybírá slova
%zahozená tokenizérem \bftt{my\_ngram\_tokenizer}, proto je v něm nastavena
%maximální délka menší než délka trigramu - 2.
%
%\input{appendix/es_settings/trigram_analyzer.tex}
%
%\subsubsection{Mapování}
%Aby fungoval zvýrazňovač pro ngramové indexy, musí být použit \bftt{Fast
%Vector Highlighter}, který funguje, pokud se pro pole nastaví hodnota
%\bftt{with\_position\_offsets} u parametru \bftt{term\_vector}. Nicméně
%nakonec byl použit externí zvýrazňovač, proto by mohlo být ukládání této
%informace vypnuto. Doporučuje se ji použít pro dlouhé texty, což není
%případ použitých datových sad v této práci.
%
%\input{appendix/es_settings/trigram_mapping.tex}
%
%\subsubsection{Dotaz pro vyhledávání}
%Dotaz je navržen, aby došlo k zásahu, pokud dokument obsahuje alespoň
%$70\%$ trigramů dotazu. Vyšší hodnoty začaly rychle ztrácet výsledky a
%nižší hodnoty vracely příliš šumu.
%
%Dotaz se snaží shodu najít napříč třemi indexy. \bftt{shorts} jsou
%ngramy kratší než $3$, protože je ElasticSearch zahazuje. \bftt{title}
%obsahuje všechny tokeny dokumentu (titulku filmu) a \bftt{trigram} je
%trigramový index. Manipulace s váhami jednotlivých polí nemá velký vliv
%na přesnost výsledku.
%
%Zvýrazňování je nastaveno doporučenou formou pro tento typ dotazu, ale
%přesto vynechává překrývající se dotazy (viz TODO). Zvýrazňování je
%provedeno externím přiloženým programem \bftt{trihigh.py}.
%
%\input{appendix/es_settings/trigram_search.tex}


\backmatter


\end{document}
